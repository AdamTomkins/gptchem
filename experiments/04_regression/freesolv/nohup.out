2023-01-27 13:33:18.811396: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │   1e-05 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │   0     │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   1     │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/1.49k [00:00<?, ?it/s]Upload progress: 100%|██████████| 1.49k/1.49k [00:00<00:00, 1.04Mit/s]
2023-01-27 13:33:20.479 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674822800,
  "events": [
    {
      "created_at": 1674822800,
      "level": "info",
      "message": "Created fine-tune: ft-ljPXFRcfI7B0bcIeqQXAvfe8",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-ljPXFRcfI7B0bcIeqQXAvfe8",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 1492,
      "created_at": 1674822800,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_133318/train.jsonl",
      "id": "file-AqsbLk0P9IU3RYawsmveiGef",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674822800,
  "validation_files": []
}
2023-01-27 13:33:20.728 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:35:31.497 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:37:31.948 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:39:32.452 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:41:33.463 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:43:33.900 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:45:38.825 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:47:39.259 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:49:39.852 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:51:40.397 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:53:45.609 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:55:46.048 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 13:57:50.749 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-27 13:59:51.227 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-27 13:59:51.231 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_133318', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_133318/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-27-12-58-23', 'ft_id': 'ft-ljPXFRcfI7B0bcIeqQXAvfe8', 'date': '20230127_135951', 'train_file_id': 'file-AqsbLk0P9IU3RYawsmveiGef', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_133318/train.jsonl: file-AqsbLk0P9IU3RYawsmveiGef
Ran train size 10 and got MAE 3.62308, GPR baseline 2.9868300057651695
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/3.07k [00:00<?, ?it/s]Upload progress: 100%|██████████| 3.07k/3.07k [00:00<00:00, 7.15Mit/s]
2023-01-27 14:00:05.492 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674824405,
  "events": [
    {
      "created_at": 1674824405,
      "level": "info",
      "message": "Created fine-tune: ft-ahcDDnoSk4N9kn0iCEe5u0CX",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-ahcDDnoSk4N9kn0iCEe5u0CX",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 3068,
      "created_at": 1674824405,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_140003/train.jsonl",
      "id": "file-kMsjM627KN3rH72ohTVhDU8h",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674824405,
  "validation_files": []
}
2023-01-27 14:00:05.891 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:02:06.333 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:04:06.840 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:06:09.910 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:08:10.348 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:10:11.709 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:12:12.172 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:14:12.774 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:16:13.625 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:18:20.468 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-27 14:20:21.065 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-27 14:20:21.067 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_140003', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_140003/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-27-13-18-57', 'ft_id': 'ft-ahcDDnoSk4N9kn0iCEe5u0CX', 'date': '20230127_142021', 'train_file_id': 'file-kMsjM627KN3rH72ohTVhDU8h', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_140003/train.jsonl: file-kMsjM627KN3rH72ohTVhDU8h
Ran train size 20 and got MAE 3.41184, GPR baseline 2.9867359928276653
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  0.06602 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 16.3862  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.00042 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/7.27k [00:00<?, ?it/s]Upload progress: 100%|██████████| 7.27k/7.27k [00:00<00:00, 17.4Mit/s]
2023-01-27 14:20:31.718 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674825631,
  "events": [
    {
      "created_at": 1674825631,
      "level": "info",
      "message": "Created fine-tune: ft-ABR95xxVOjQcKpwvJrvqbQW4",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-ABR95xxVOjQcKpwvJrvqbQW4",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 7270,
      "created_at": 1674825627,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_142026/train.jsonl",
      "id": "file-zFvJzKbpBf2tv8QFOOPX3xK5",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674825631,
  "validation_files": []
}
2023-01-27 14:20:32.010 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:22:32.983 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:24:34.875 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:26:35.791 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:28:36.381 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:30:39.659 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:32:40.227 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:34:40.875 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:36:41.567 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:38:42.253 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:42:04.828 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-27 14:42:04.830 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_142026', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_142026/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-27-13-41-48', 'ft_id': 'ft-ABR95xxVOjQcKpwvJrvqbQW4', 'date': '20230127_144204', 'train_file_id': 'file-zFvJzKbpBf2tv8QFOOPX3xK5', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_142026/train.jsonl: file-zFvJzKbpBf2tv8QFOOPX3xK5
Ran train size 50 and got MAE 2.48908, GPR baseline 1.3758910871938506
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ 0.31485 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 8.12075 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │ 0.00071 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/14.6k [00:00<?, ?it/s]Upload progress: 100%|██████████| 14.6k/14.6k [00:00<00:00, 26.3Mit/s]
2023-01-27 14:42:25.042 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674826944,
  "events": [
    {
      "created_at": 1674826944,
      "level": "info",
      "message": "Created fine-tune: ft-1wZ8oDsyHih5Tb9tMOda3vGw",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-1wZ8oDsyHih5Tb9tMOda3vGw",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 14582,
      "created_at": 1674826940,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_144219/train.jsonl",
      "id": "file-1t7645kqpAeMS4x8VgRAcJev",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674826944,
  "validation_files": []
}
2023-01-27 14:42:25.233 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:44:26.659 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:46:27.120 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:48:28.047 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:53:01.206 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:55:01.704 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 14:57:02.394 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-27 14:59:02.974 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-27 14:59:02.976 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_144219', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_144219/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-27-13-58-45', 'ft_id': 'ft-1wZ8oDsyHih5Tb9tMOda3vGw', 'date': '20230127_145902', 'train_file_id': 'file-1t7645kqpAeMS4x8VgRAcJev', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_144219/train.jsonl: file-1t7645kqpAeMS4x8VgRAcJev
Ran train size 100 and got MAE 1.7728799999999998, GPR baseline 0.9619113059134867
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ 1.13192 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 6.80561 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │ 0.00182 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/28.8k [00:00<?, ?it/s]Upload progress: 100%|██████████| 28.8k/28.8k [00:00<00:00, 57.1Mit/s]
2023-01-27 14:59:14.376 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674827954,
  "events": [
    {
      "created_at": 1674827954,
      "level": "info",
      "message": "Created fine-tune: ft-Sf2E3qMSHOpBgry9YUxB3ttL",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-Sf2E3qMSHOpBgry9YUxB3ttL",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 28832,
      "created_at": 1674827952,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_145911/train.jsonl",
      "id": "file-DkiKuOjrHYsoBxNGfvcDGxZc",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674827954,
  "validation_files": []
}
2023-01-27 14:59:15.062 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:01:15.623 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:03:16.207 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:05:20.830 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:07:21.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:09:23.028 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:11:23.630 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:13:24.193 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:15:26.021 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:17:26.607 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:19:27.195 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:21:27.790 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:23:28.427 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:25:28.981 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:27:32.075 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:29:32.571 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:31:33.317 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:33:34.578 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:35:35.115 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:37:36.375 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:39:38.056 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:41:38.792 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:43:39.518 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:45:40.226 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:47:41.397 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:49:42.701 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:51:45.928 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-27 15:53:46.643 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-27 15:55:49.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-27 15:57:49.700 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-27 15:57:49.702 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_145911', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_145911/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-27-14-57-26', 'ft_id': 'ft-Sf2E3qMSHOpBgry9YUxB3ttL', 'date': '20230127_155749', 'train_file_id': 'file-DkiKuOjrHYsoBxNGfvcDGxZc', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230127_145911/train.jsonl: file-DkiKuOjrHYsoBxNGfvcDGxZc
Ran train size 200 and got MAE 1.4508800000000002, GPR baseline 0.8403198077853156
Traceback (most recent call last):
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/run_experiments.py", line 84, in <module>
    train_test_model(representation, num_train_points, seed + 3234564)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/run_experiments.py", line 30, in train_test_model
    train_data, test_data = train_test_split(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/model_selection/_split.py", line 2562, in train_test_split
    n_train, n_test = _validate_shuffle_split(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/model_selection/_split.py", line 2181, in _validate_shuffle_split
    raise ValueError(
ValueError: test_size=-401 should be either positive and smaller than the number of samples 599 or a float in the (0, 1) range
