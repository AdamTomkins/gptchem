2023-01-22 15:10:52.686474: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │   1e-05 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │   0     │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   1     │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/1.49k [00:00<?, ?it/s]Upload progress: 100%|██████████| 1.49k/1.49k [00:00<00:00, 450kit/s]
2023-01-22 15:10:54.642 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674396654,
  "events": [
    {
      "created_at": 1674396654,
      "level": "info",
      "message": "Created fine-tune: ft-1npIKYBpFhfyK39L6ZsWJUKL",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-1npIKYBpFhfyK39L6ZsWJUKL",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 1492,
      "created_at": 1674396654,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_151052/train.jsonl",
      "id": "file-c1Q9WHoxrgl4JmQKxrKPlcz5",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674396654,
  "validation_files": []
}
2023-01-22 15:10:55.166 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:12:56.475 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:14:57.117 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:16:58.768 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:18:59.758 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 15:18:59.764 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_151052', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_151052/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-14-17-43', 'ft_id': 'ft-1npIKYBpFhfyK39L6ZsWJUKL', 'date': '20230122_151859', 'train_file_id': 'file-c1Q9WHoxrgl4JmQKxrKPlcz5', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_151052/train.jsonl: file-c1Q9WHoxrgl4JmQKxrKPlcz5
Ran train size 10 and got MAE 3.6188, GPR baseline 2.9868300057651695
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/3.07k [00:00<?, ?it/s]Upload progress: 100%|██████████| 3.07k/3.07k [00:00<00:00, 3.51Mit/s]
2023-01-22 15:19:10.409 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674397150,
  "events": [
    {
      "created_at": 1674397150,
      "level": "info",
      "message": "Created fine-tune: ft-2EWX0n91glkmG2ckm6q05Ybk",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-2EWX0n91glkmG2ckm6q05Ybk",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 3068,
      "created_at": 1674397149,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_151908/train.jsonl",
      "id": "file-bk4S49aAigMCj329w7O5xurI",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674397150,
  "validation_files": []
}
2023-01-22 15:19:10.827 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:21:11.561 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:23:12.803 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:25:13.450 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:27:14.020 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:29:14.704 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:31:15.907 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:33:17.910 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:35:19.031 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 15:37:19.607 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 15:37:19.608 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_151908', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_151908/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-14-35-16', 'ft_id': 'ft-2EWX0n91glkmG2ckm6q05Ybk', 'date': '20230122_153719', 'train_file_id': 'file-bk4S49aAigMCj329w7O5xurI', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_151908/train.jsonl: file-bk4S49aAigMCj329w7O5xurI
Ran train size 20 and got MAE 3.1877600000000004, GPR baseline 2.9867359928276653
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  0.06602 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 16.3862  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.00042 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/7.27k [00:00<?, ?it/s]Upload progress: 100%|██████████| 7.27k/7.27k [00:00<00:00, 12.9Mit/s]
2023-01-22 15:37:29.801 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674398249,
  "events": [
    {
      "created_at": 1674398249,
      "level": "info",
      "message": "Created fine-tune: ft-d5FgBRSyD9guDQtGZPlaZooe",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-d5FgBRSyD9guDQtGZPlaZooe",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 7270,
      "created_at": 1674398249,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_153728/train.jsonl",
      "id": "file-jE2eYK8DnMxfcRq2yNjDYbm3",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674398249,
  "validation_files": []
}
2023-01-22 15:37:30.094 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:39:32.378 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:41:33.297 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:43:34.009 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:45:35.198 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:47:36.389 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:49:36.942 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:51:38.191 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 15:53:39.058 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 15:53:39.068 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_153728', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_153728/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-14-52-20', 'ft_id': 'ft-d5FgBRSyD9guDQtGZPlaZooe', 'date': '20230122_155339', 'train_file_id': 'file-jE2eYK8DnMxfcRq2yNjDYbm3', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_153728/train.jsonl: file-jE2eYK8DnMxfcRq2yNjDYbm3
Ran train size 50 and got MAE 2.62348, GPR baseline 1.3758910871938506
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ 0.31485 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 8.12075 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │ 0.00071 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/14.6k [00:00<?, ?it/s]Upload progress: 100%|██████████| 14.6k/14.6k [00:00<00:00, 26.2Mit/s]
2023-01-22 15:53:50.375 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674399230,
  "events": [
    {
      "created_at": 1674399230,
      "level": "info",
      "message": "Created fine-tune: ft-oha2TyIs9g7r14ODZzRBiEUZ",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-oha2TyIs9g7r14ODZzRBiEUZ",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 14582,
      "created_at": 1674399229,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_155348/train.jsonl",
      "id": "file-KyksLvFqyl3ASdyS8HOzAtts",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674399230,
  "validation_files": []
}
2023-01-22 15:53:50.594 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:55:52.377 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:57:53.056 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:59:53.870 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:01:54.517 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:03:57.678 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 16:06:00.047 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 16:06:00.050 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_155348', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_155348/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-15-05-51', 'ft_id': 'ft-oha2TyIs9g7r14ODZzRBiEUZ', 'date': '20230122_160600', 'train_file_id': 'file-KyksLvFqyl3ASdyS8HOzAtts', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/out/20230122_155348/train.jsonl: file-KyksLvFqyl3ASdyS8HOzAtts
Traceback (most recent call last):
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/run_experiments.py", line 84, in <module>
    train_test_model(representation, num_train_points, seed + 3234564)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/freesolv/run_experiments.py", line 58, in train_test_model
    completions = querier(test_formatted)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/querier.py", line 123, in __call__
    return self.query(df, temperature, logprobs)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/querier.py", line 98, in query
    completions_ = openai.Completion.create(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_resources/completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_requestor.py", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_requestor.py", line 599, in _interpret_response
    self._interpret_response_line(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_requestor.py", line 655, in _interpret_response_line
    raise self.handle_error_response(
openai.error.APIError: internal error {
    "error": {
        "message": "internal error",
        "type": "invalid_request_error",
        "param": null,
        "code": null
    }
}
 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Sun, 22 Jan 2023 15:06:09 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'Vary': 'Origin', 'X-Request-Id': 'b3b3dc53803170b6873d283b4b059375', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains'}
