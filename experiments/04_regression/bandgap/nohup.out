nohup: ignoring input
2023-01-31 09:00:17.244125: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-31 09:00:18.004934: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-01-31 09:00:18.005032: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-01-31 09:00:18.005047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-31 09:00:36.657589: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2023-01-31 09:00:36.657625: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2023-01-31 09:00:36.657649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kevin-OptiPlex-9020): /proc/driver/nvidia/version does not exist
2023-01-31 09:00:36.657945: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From /home/kevin/anaconda3/envs/gptchem/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/3.99k [00:00<?, ?it/s]Upload progress: 100%|██████████| 3.99k/3.99k [00:00<00:00, 3.08Mit/s]
2023-01-31 09:00:40.416 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675152040,
  "events": [
    {
      "created_at": 1675152040,
      "level": "info",
      "message": "Created fine-tune: ft-ZsDHfV7KyitH68MwOOYFg0iJ",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-ZsDHfV7KyitH68MwOOYFg0iJ",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 3994,
      "created_at": 1675152040,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_090038/train.jsonl",
      "id": "file-3xzGUdCcDFWczG7yuxNpcLCP",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675152040,
  "validation_files": []
}
2023-01-31 09:00:40.593 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:02:41.122 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:04:41.592 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:06:42.145 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:08:42.608 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:10:43.157 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:12:43.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:14:44.231 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:16:44.725 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:18:45.293 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:20:45.807 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:22:46.294 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:24:46.855 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:26:47.292 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:28:47.859 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:30:48.415 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:32:48.909 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:34:49.401 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:36:49.953 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:38:50.487 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:40:51.044 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:42:51.533 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:44:52.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:46:52.626 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:48:53.083 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:50:53.566 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:52:54.009 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:54:54.542 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:56:54.995 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:58:55.470 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:00:56.023 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:02:56.506 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:04:57.027 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:06:57.500 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:08:58.050 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:10:58.511 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:12:59.030 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:14:59.481 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:16:59.933 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:19:00.414 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:21:00.873 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:23:01.394 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 10:23:01.412 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_090038', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_090038/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-09-22-27', 'ft_id': 'ft-ZsDHfV7KyitH68MwOOYFg0iJ', 'date': '20230131_102301', 'train_file_id': 'file-3xzGUdCcDFWczG7yuxNpcLCP', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_090038/train.jsonl: file-3xzGUdCcDFWczG7yuxNpcLCP
Ran train size 10 and got MAE nan, GPR baseline 0.6494814537788131
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.93116 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  2.96219 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.60704 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/8.63k [00:00<?, ?it/s]Upload progress: 100%|██████████| 8.63k/8.63k [00:00<00:00, 16.1Mit/s]
2023-01-31 10:23:28.104 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675157007,
  "events": [
    {
      "created_at": 1675157008,
      "level": "info",
      "message": "Created fine-tune: ft-wmS0RHG88sDlpu91Lt2qiZCg",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-wmS0RHG88sDlpu91Lt2qiZCg",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 8630,
      "created_at": 1675157007,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_102326/train.jsonl",
      "id": "file-fIkSmPGXthbF97QuW4C97lxR",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675157008,
  "validation_files": []
}
2023-01-31 10:23:28.290 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:25:28.828 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:27:29.280 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:29:29.818 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:31:30.363 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:33:30.897 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:35:31.432 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:37:31.971 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:39:32.521 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:41:32.993 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:43:33.542 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:45:34.097 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:47:34.587 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:49:35.032 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:51:35.601 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:53:36.125 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:55:36.666 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:57:37.202 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:59:37.731 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:01:38.170 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:03:38.679 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:05:39.165 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:07:39.653 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:09:40.095 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:11:40.534 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:13:40.986 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:15:41.537 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:17:42.073 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:19:42.559 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:21:43.101 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:23:43.586 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:25:44.107 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:27:44.634 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:29:45.093 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:31:45.615 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:33:46.060 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:35:46.597 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:37:47.159 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:39:47.670 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:41:48.178 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:43:48.684 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:45:49.161 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:47:49.647 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:49:50.183 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:51:50.727 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:53:51.198 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 11:53:51.198 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_102326', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_102326/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-10-53-49', 'ft_id': 'ft-wmS0RHG88sDlpu91Lt2qiZCg', 'date': '20230131_115351', 'train_file_id': 'file-fIkSmPGXthbF97QuW4C97lxR', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_102326/train.jsonl: file-fIkSmPGXthbF97QuW4C97lxR
Ran train size 20 and got MAE 0.7043199999999997, GPR baseline 0.613513522684917
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.35378 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  9.5244  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.48374 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/21.5k [00:00<?, ?it/s]Upload progress: 100%|██████████| 21.5k/21.5k [00:00<00:00, 38.4Mit/s]
2023-01-31 11:54:15.379 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675162455,
  "events": [
    {
      "created_at": 1675162455,
      "level": "info",
      "message": "Created fine-tune: ft-Py2rj0E7z4xN4z6L2MAe7NIl",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-Py2rj0E7z4xN4z6L2MAe7NIl",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 21544,
      "created_at": 1675162455,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_115414/train.jsonl",
      "id": "file-w3wf6Qd4YPbOjmyd7ZJclXNv",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675162455,
  "validation_files": []
}
2023-01-31 11:54:15.549 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:56:16.060 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:58:16.528 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:00:17.062 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:02:17.543 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:04:18.011 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:06:18.468 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:08:19.018 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:10:19.542 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:12:20.025 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:14:20.550 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:16:21.089 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:18:21.637 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:20:22.171 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:22:22.723 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:24:23.237 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:26:23.773 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:28:24.327 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:30:24.857 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:32:25.365 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:34:25.924 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:36:26.472 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:38:26.959 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:40:27.506 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:42:28.015 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:44:28.560 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:46:29.036 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:48:29.570 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:50:30.133 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:52:30.650 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:54:31.156 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:56:31.663 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:58:32.130 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:00:32.631 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:02:33.098 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:04:33.634 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:06:34.104 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:08:34.643 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:10:35.144 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:12:35.586 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:14:36.150 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:16:36.666 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:18:37.111 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:20:37.547 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:22:38.109 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:24:38.641 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:26:39.173 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:28:39.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:30:40.114 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:32:40.650 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:34:41.205 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:36:41.678 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:38:42.126 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:40:42.659 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:42:43.201 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:44:43.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:46:44.288 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:48:44.830 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:50:45.273 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:52:45.815 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:54:46.323 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:56:46.848 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 13:58:47.364 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 13:58:47.365 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_115414', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_115414/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-12-57-24', 'ft_id': 'ft-Py2rj0E7z4xN4z6L2MAe7NIl', 'date': '20230131_135847', 'train_file_id': 'file-w3wf6Qd4YPbOjmyd7ZJclXNv', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_115414/train.jsonl: file-w3wf6Qd4YPbOjmyd7ZJclXNv
Ran train size 50 and got MAE 0.6529999999999999, GPR baseline 0.6010078609589963
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.06015 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 47.2285  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.11942 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/43.0k [00:00<?, ?it/s]Upload progress: 100%|██████████| 43.0k/43.0k [00:00<00:00, 41.8Mit/s]
2023-01-31 13:59:12.012 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675169951,
  "events": [
    {
      "created_at": 1675169951,
      "level": "info",
      "message": "Created fine-tune: ft-goIaQq8miLXea2p78BIFw0OM",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-goIaQq8miLXea2p78BIFw0OM",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 42984,
      "created_at": 1675169951,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_135910/train.jsonl",
      "id": "file-nSpESGr5DPwh1ycVGWwCTdLm",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675169951,
  "validation_files": []
}
2023-01-31 13:59:12.183 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:01:12.722 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:03:13.264 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:05:13.811 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:07:14.369 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:09:14.874 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:11:15.396 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:13:15.906 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:15:16.435 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:17:16.987 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:19:17.526 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:21:18.052 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:23:18.593 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:25:19.153 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:27:19.619 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:29:20.082 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:31:20.667 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:33:21.140 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:35:21.688 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:37:22.193 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:39:22.747 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:41:23.274 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:43:23.797 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:45:24.339 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:47:24.889 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:49:25.383 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:51:25.945 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:53:26.464 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:55:27.002 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:57:27.524 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:59:28.060 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:01:28.548 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:03:29.037 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:05:29.492 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:07:29.946 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:09:30.478 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:11:31.005 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:13:31.516 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:15:32.073 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:17:32.580 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:19:33.048 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:21:33.596 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:23:34.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:25:34.633 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:27:35.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:29:35.639 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:31:36.184 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:33:36.724 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:35:37.283 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:37:37.797 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:39:38.344 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:41:38.843 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:43:39.389 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:45:39.916 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:47:40.472 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 15:49:40.937 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 15:51:41.503 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 15:51:41.504 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_135910', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_135910/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-14-50-00', 'ft_id': 'ft-goIaQq8miLXea2p78BIFw0OM', 'date': '20230131_155141', 'train_file_id': 'file-nSpESGr5DPwh1ycVGWwCTdLm', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_135910/train.jsonl: file-nSpESGr5DPwh1ycVGWwCTdLm
Ran train size 100 and got MAE 0.653, GPR baseline 0.5007550384885692
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  2.42499 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 53.834   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.03253 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/86.6k [00:00<?, ?it/s]Upload progress: 100%|██████████| 86.6k/86.6k [00:00<00:00, 150Mit/s]
2023-01-31 15:52:06.308 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675176726,
  "events": [
    {
      "created_at": 1675176726,
      "level": "info",
      "message": "Created fine-tune: ft-FDRbVLXzS4kcN8PVR5qkDnX8",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-FDRbVLXzS4kcN8PVR5qkDnX8",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 86564,
      "created_at": 1675176725,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_155204/train.jsonl",
      "id": "file-XQWfbZXYMUSpi4BodhlbgM4B",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675176726,
  "validation_files": []
}
2023-01-31 15:52:06.487 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:54:06.978 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:56:07.519 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:58:07.978 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:00:08.561 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:02:09.043 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:04:09.514 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:06:10.092 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:08:10.596 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:10:11.141 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:12:11.646 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:14:12.147 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:16:12.734 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:18:13.237 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:20:13.772 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:22:14.251 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:24:14.809 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:26:15.287 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:28:15.811 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:30:16.323 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:32:16.896 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:34:17.418 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:36:17.956 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 16:38:18.482 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 16:40:19.060 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 16:40:19.061 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_155204', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_155204/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-15-39-38', 'ft_id': 'ft-FDRbVLXzS4kcN8PVR5qkDnX8', 'date': '20230131_164019', 'train_file_id': 'file-XQWfbZXYMUSpi4BodhlbgM4B', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_155204/train.jsonl: file-XQWfbZXYMUSpi4BodhlbgM4B
Ran train size 200 and got MAE 0.60096, GPR baseline 0.5026799620858016
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  7.52693 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 44.5921  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.11016 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/442k [00:00<?, ?it/s]Upload progress: 100%|██████████| 442k/442k [00:00<00:00, 734Mit/s]
2023-01-31 16:41:05.522 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675179665,
  "events": [
    {
      "created_at": 1675179665,
      "level": "info",
      "message": "Created fine-tune: ft-NXfZI3ByjJAkIgmBHH69Rd0Y",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-NXfZI3ByjJAkIgmBHH69Rd0Y",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 442248,
      "created_at": 1675179665,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_164103/train.jsonl",
      "id": "file-MDfsVzEd0dMZiUsflxTbaa3e",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675179665,
  "validation_files": []
}
2023-01-31 16:41:05.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:43:06.228 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:45:06.757 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:47:07.308 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:49:07.799 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:51:08.328 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:53:08.869 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:55:09.396 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 16:57:09.922 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 16:59:10.419 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:01:10.940 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:03:11.454 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:05:11.991 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:07:12.503 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:09:13.000 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:11:13.449 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:13:13.963 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:15:14.518 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:17:15.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:19:15.643 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 17:19:15.643 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_164103', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_164103/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-16-18-52', 'ft_id': 'ft-NXfZI3ByjJAkIgmBHH69Rd0Y', 'date': '20230131_171915', 'train_file_id': 'file-MDfsVzEd0dMZiUsflxTbaa3e', 'valid_file_id': None}
2023-01-31 17:19:55.571906: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 200000000 exceeds 10% of free system memory.
2023-01-31 17:19:56.035632: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 200000000 exceeds 10% of free system memory.
2023-01-31 17:19:56.488522: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 200000000 exceeds 10% of free system memory.
2023-01-31 17:19:56.982941: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 200000000 exceeds 10% of free system memory.
2023-01-31 17:19:57.160220: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 200000000 exceeds 10% of free system memory.
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_164103/train.jsonl: file-MDfsVzEd0dMZiUsflxTbaa3e
Ran train size 1000 and got MAE 0.46568000000000004, GPR baseline 0.429676680692937
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  9.79402 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 26.6843  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.2096  │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/2.21M [00:00<?, ?it/s]Upload progress: 100%|██████████| 2.21M/2.21M [00:00<00:00, 3.57Git/s]
2023-01-31 17:27:42.638 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675182462,
  "events": [
    {
      "created_at": 1675182462,
      "level": "info",
      "message": "Created fine-tune: ft-9wM5QeYoJLJVOaS86TQbqV1o",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-9wM5QeYoJLJVOaS86TQbqV1o",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 2207226,
      "created_at": 1675182462,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_172737/train.jsonl",
      "id": "file-VluVk9mwyP23MWi0xoIzNMbX",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675182462,
  "validation_files": []
}
2023-01-31 17:27:42.810 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:29:43.302 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:31:43.811 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:33:44.324 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:35:44.850 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:37:45.394 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:39:45.832 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:41:46.336 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:43:46.880 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:45:47.430 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:47:47.893 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:49:48.435 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:51:49.020 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:53:49.477 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:56:07.589 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:58:08.143 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:00:08.671 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:02:09.202 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:04:09.727 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:06:10.271 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:08:10.834 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:10:11.342 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:12:11.851 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:14:12.400 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:16:12.933 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:18:13.405 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:20:13.885 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:22:14.415 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:24:14.951 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:26:15.462 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:28:15.962 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:30:16.457 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:32:16.923 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:34:17.469 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:36:17.989 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:38:18.434 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:40:18.965 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:42:19.463 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:44:19.949 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:46:20.510 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:48:21.026 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:50:21.538 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:52:22.069 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:54:22.604 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:56:23.160 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:58:23.615 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:00:24.116 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:02:24.626 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:04:25.177 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:06:25.629 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:08:26.106 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:10:26.648 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:12:27.188 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:14:27.660 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:16:28.211 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:18:28.791 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:20:29.329 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:22:29.909 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:24:30.459 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:26:30.991 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:28:31.441 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:30:31.993 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:32:32.532 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:34:33.077 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:36:33.605 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:38:34.124 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:40:34.662 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:42:35.188 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:44:35.730 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:46:36.194 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:48:36.665 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:50:37.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:52:37.620 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:54:38.152 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:56:38.704 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:58:39.255 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:00:39.805 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:02:40.368 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:04:40.842 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:06:41.394 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:08:41.915 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:10:42.380 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:12:42.899 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:14:43.444 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:16:43.881 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:18:44.339 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:20:44.871 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:22:45.395 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:24:45.914 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:26:46.471 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:28:46.935 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:30:47.485 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:32:48.019 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:34:48.540 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:36:49.011 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:38:49.481 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:40:49.938 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:42:50.450 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:44:50.946 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 20:46:51.441 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 20:48:51.969 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 20:50:52.512 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 20:52:53.014 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 20:54:53.515 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 20:56:54.027 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 20:58:54.539 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 21:00:55.051 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 21:02:55.600 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 21:04:56.079 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 21:06:56.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 21:08:57.141 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 21:10:57.604 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 21:12:58.048 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 21:14:58.513 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 21:14:58.513 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_172737', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_172737/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-20-13-44', 'ft_id': 'ft-9wM5QeYoJLJVOaS86TQbqV1o', 'date': '20230131_211458', 'train_file_id': 'file-VluVk9mwyP23MWi0xoIzNMbX', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_172737/train.jsonl: file-VluVk9mwyP23MWi0xoIzNMbX
Ran train size 5000 and got MAE 0.35936, GPR baseline 0.3554377721560356
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/10.4k [00:00<?, ?it/s]Upload progress: 100%|██████████| 10.4k/10.4k [00:00<00:00, 23.4Mit/s]
2023-01-31 21:15:21.783 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675196121,
  "events": [
    {
      "created_at": 1675196121,
      "level": "info",
      "message": "Created fine-tune: ft-KocMIHbobqIcar5DdMklIejQ",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-KocMIHbobqIcar5DdMklIejQ",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 10362,
      "created_at": 1675196121,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_211520/train.jsonl",
      "id": "file-RUbvgUYBj71Y74HK65l5IYP5",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675196121,
  "validation_files": []
}
2023-01-31 21:15:21.957 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:17:22.469 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:19:22.941 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:21:23.465 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:23:23.970 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:25:24.519 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:27:25.059 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:29:25.515 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:31:26.048 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:33:26.582 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:35:27.108 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:37:27.600 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:39:28.121 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:41:28.591 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:43:29.102 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:45:29.595 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:47:30.117 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:49:30.665 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:51:31.133 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:53:31.634 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:55:32.120 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:57:32.654 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:59:33.162 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:01:33.678 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:03:34.174 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:05:34.697 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:07:35.185 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:09:35.688 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:11:36.223 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:13:36.753 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:15:37.293 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:17:37.786 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:19:38.302 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:21:38.829 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 22:21:38.830 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_211520', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_211520/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-21-20-51', 'ft_id': 'ft-KocMIHbobqIcar5DdMklIejQ', 'date': '20230131_222138', 'train_file_id': 'file-RUbvgUYBj71Y74HK65l5IYP5', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_211520/train.jsonl: file-RUbvgUYBj71Y74HK65l5IYP5
Ran train size 10 and got MAE 0.7045999999999999, GPR baseline 0.6018326625429775
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.93116 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  2.96219 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.60704 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/24.7k [00:00<?, ?it/s]Upload progress: 100%|██████████| 24.7k/24.7k [00:00<00:00, 43.1Mit/s]
2023-01-31 22:22:03.390 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675200123,
  "events": [
    {
      "created_at": 1675200123,
      "level": "info",
      "message": "Created fine-tune: ft-Jz9tzKnBJkS1P4AYgYpdft7w",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-Jz9tzKnBJkS1P4AYgYpdft7w",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 24676,
      "created_at": 1675200123,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_222202/train.jsonl",
      "id": "file-f6HKMbBsiWVBoOpwqd44Sr6r",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675200123,
  "validation_files": []
}
2023-01-31 22:22:03.554 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:24:04.030 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:26:04.482 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:28:04.962 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:30:05.495 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:32:33.030 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:34:33.568 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:36:34.064 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:38:34.593 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:40:35.123 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:42:35.627 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:44:36.150 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:46:36.644 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:48:37.179 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:50:37.636 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:52:38.159 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:54:38.668 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:56:39.121 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:58:39.652 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:00:40.185 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:02:40.713 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:04:41.248 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:06:41.777 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:08:42.348 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:10:42.882 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:12:43.413 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:14:43.999 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:16:44.550 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:18:45.099 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:20:45.646 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:22:46.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:24:46.717 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:26:47.259 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:28:47.810 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:30:48.360 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:32:48.885 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:34:49.321 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:36:49.796 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:38:50.297 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:40:50.729 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:42:51.233 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:44:51.758 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:46:52.203 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:48:52.694 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:50:53.190 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:52:53.713 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:54:54.261 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:56:54.757 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:58:55.275 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:00:55.706 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:02:56.181 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:04:56.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:06:57.168 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:08:57.677 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:10:58.217 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:12:58.779 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:14:59.316 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:16:59.836 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 00:16:59.836 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_222202', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_222202/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-23-16-23', 'ft_id': 'ft-Jz9tzKnBJkS1P4AYgYpdft7w', 'date': '20230201_001659', 'train_file_id': 'file-f6HKMbBsiWVBoOpwqd44Sr6r', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230131_222202/train.jsonl: file-f6HKMbBsiWVBoOpwqd44Sr6r
Ran train size 20 and got MAE 0.77832, GPR baseline 0.613513522684917
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.35378 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  9.5244  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.48374 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/62.2k [00:00<?, ?it/s]Upload progress: 100%|██████████| 62.2k/62.2k [00:00<00:00, 117Mit/s]
2023-02-01 00:17:25.628 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675207045,
  "events": [
    {
      "created_at": 1675207045,
      "level": "info",
      "message": "Created fine-tune: ft-d4JletWBYBVjI2xvWfnxg9jD",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-d4JletWBYBVjI2xvWfnxg9jD",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 62246,
      "created_at": 1675207045,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_001723/train.jsonl",
      "id": "file-Ov4gL0ohAquKrq7MRo2b3h1A",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675207045,
  "validation_files": []
}
2023-02-01 00:17:25.793 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:19:26.320 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:21:26.775 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:23:27.334 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:25:27.835 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:27:28.374 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:29:28.816 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:31:29.504 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:33:30.004 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:35:30.530 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:37:31.080 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:39:31.586 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:41:32.075 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:43:32.628 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:45:33.163 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:47:33.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:49:34.080 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:51:34.606 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:53:35.164 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:55:35.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:57:36.149 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:59:36.695 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:01:37.191 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:03:37.701 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:05:38.203 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:07:38.671 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:09:39.113 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:11:39.580 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:13:40.127 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:15:40.645 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:17:41.143 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:19:41.663 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:21:42.140 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:23:42.584 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:25:43.030 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:27:43.537 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:29:44.093 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:31:44.579 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:33:45.076 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:35:45.523 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:37:46.053 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:39:46.579 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:41:47.092 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:43:47.629 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:45:48.152 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:47:48.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:49:49.234 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:51:49.752 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:53:50.288 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:55:50.823 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:57:51.348 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:59:51.831 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:01:52.364 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:03:52.802 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:05:53.328 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:07:53.858 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:09:54.376 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:11:54.806 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:13:55.333 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:15:55.840 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:17:56.321 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:19:56.875 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:21:57.403 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:23:57.954 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:25:58.479 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:27:59.000 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:29:59.510 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:32:00.042 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:34:00.606 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:36:01.162 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:38:01.666 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:40:02.123 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:42:02.670 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:44:03.136 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:46:03.567 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 02:48:04.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 02:48:04.084 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_001723', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_001723/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-01-46-57', 'ft_id': 'ft-d4JletWBYBVjI2xvWfnxg9jD', 'date': '20230201_024804', 'train_file_id': 'file-Ov4gL0ohAquKrq7MRo2b3h1A', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_001723/train.jsonl: file-Ov4gL0ohAquKrq7MRo2b3h1A
Ran train size 50 and got MAE 0.65256, GPR baseline 0.6010078609589963
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.06015 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 47.2285  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.11942 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/123k [00:00<?, ?it/s]Upload progress: 100%|██████████| 123k/123k [00:00<00:00, 262Mit/s]
2023-02-01 02:48:28.435 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675216108,
  "events": [
    {
      "created_at": 1675216108,
      "level": "info",
      "message": "Created fine-tune: ft-9O2xILmCjvZqzsll7QtLbQkj",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-9O2xILmCjvZqzsll7QtLbQkj",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 123392,
      "created_at": 1675216108,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_024826/train.jsonl",
      "id": "file-7NHzj12ST7ywaMZKvZqhT6bh",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675216108,
  "validation_files": []
}
2023-02-01 02:48:28.611 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:50:29.091 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:52:29.640 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:54:30.133 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:56:30.669 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:58:31.229 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:00:31.730 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:02:32.215 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:04:32.745 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:06:33.273 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:08:33.796 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:10:34.344 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:12:34.863 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:14:35.384 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:16:35.889 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:18:36.399 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:20:36.925 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:22:37.395 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:24:37.856 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:26:38.402 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:28:38.951 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:30:39.425 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:32:39.949 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:34:40.473 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:36:40.992 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:38:41.532 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:40:42.044 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:42:42.582 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:44:43.026 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:46:43.523 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:48:44.074 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:50:44.587 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:52:45.136 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:54:45.615 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:56:46.146 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:58:46.695 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:00:47.211 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 04:02:47.692 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 04:02:47.692 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_024826', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_024826/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-03-02-18', 'ft_id': 'ft-9O2xILmCjvZqzsll7QtLbQkj', 'date': '20230201_040247', 'train_file_id': 'file-7NHzj12ST7ywaMZKvZqhT6bh', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_024826/train.jsonl: file-7NHzj12ST7ywaMZKvZqhT6bh
Ran train size 100 and got MAE 0.61236, GPR baseline 0.5007550384885692
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  2.42499 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 53.834   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.03253 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/248k [00:00<?, ?it/s]Upload progress: 100%|██████████| 248k/248k [00:00<00:00, 430Mit/s]
2023-02-01 04:03:13.430 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675220593,
  "events": [
    {
      "created_at": 1675220593,
      "level": "info",
      "message": "Created fine-tune: ft-uTFIO837tJdKVEFqqrXBpJMQ",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-uTFIO837tJdKVEFqqrXBpJMQ",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 247912,
      "created_at": 1675220593,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_040311/train.jsonl",
      "id": "file-Dmu1SEWfY69uoSBsbbrD8MWQ",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675220593,
  "validation_files": []
}
2023-02-01 04:03:13.603 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:05:14.109 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:07:14.650 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:09:15.167 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:11:15.653 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:13:16.168 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:15:16.700 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:17:17.232 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:19:17.733 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:21:18.265 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:23:18.773 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:25:19.305 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:27:19.761 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:29:20.323 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:31:20.845 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:33:21.318 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:35:21.830 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:37:22.325 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:39:22.853 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:41:23.384 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:43:23.881 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:45:24.621 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:47:25.160 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:49:25.679 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:51:26.199 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:53:26.744 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:55:27.293 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:57:27.838 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:59:28.310 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:01:28.827 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:03:29.296 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:05:29.832 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:07:30.354 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:09:30.896 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:11:31.435 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:13:31.970 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:15:32.495 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:17:32.986 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:19:33.480 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:21:33.977 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:23:34.432 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:25:34.968 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:27:35.509 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:29:36.001 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:31:36.487 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:33:37.029 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:35:37.568 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:37:38.103 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:39:38.590 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:41:39.029 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:43:39.547 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:45:40.067 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:47:40.585 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:49:41.031 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:51:41.576 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:53:42.102 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:55:42.633 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:57:43.143 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:59:43.593 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:01:44.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:03:44.625 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:05:45.090 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:07:45.567 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 06:09:46.076 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 06:11:46.605 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 06:11:46.605 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_040311', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_040311/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-05-10-22', 'ft_id': 'ft-uTFIO837tJdKVEFqqrXBpJMQ', 'date': '20230201_061146', 'train_file_id': 'file-Dmu1SEWfY69uoSBsbbrD8MWQ', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_040311/train.jsonl: file-Dmu1SEWfY69uoSBsbbrD8MWQ
Ran train size 200 and got MAE 0.62428, GPR baseline 0.5026799620858016
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  7.52693 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 44.5921  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.11016 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/1.27M [00:00<?, ?it/s]Upload progress: 100%|██████████| 1.27M/1.27M [00:00<00:00, 2.18Git/s]
2023-02-01 06:12:27.309 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675228347,
  "events": [
    {
      "created_at": 1675228347,
      "level": "info",
      "message": "Created fine-tune: ft-pZ2fCByMQ1Ns3ufp20Ln5qFx",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-pZ2fCByMQ1Ns3ufp20Ln5qFx",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 1269516,
      "created_at": 1675228346,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_061224/train.jsonl",
      "id": "file-YQqJ8tcQNIYF8bYKrxb8gEon",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675228347,
  "validation_files": []
}
2023-02-01 06:12:27.480 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:14:28.009 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:16:28.520 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:18:29.021 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:20:29.561 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:22:30.051 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:24:30.578 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:26:31.026 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:28:31.490 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:30:32.018 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:32:32.538 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:34:33.078 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:36:33.640 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:38:34.194 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:40:34.745 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:42:35.284 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:44:35.817 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:46:36.297 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:48:36.818 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:50:37.360 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:52:37.904 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:54:38.439 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:56:38.979 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:58:39.497 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:00:39.974 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:02:40.510 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:04:41.050 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:06:41.579 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:08:42.100 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:10:42.587 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:12:43.015 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:14:43.563 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:16:44.001 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:18:44.542 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:20:45.062 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:22:45.597 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:24:46.138 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:26:46.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:28:47.110 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:30:47.577 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:32:48.105 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 07:34:48.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 07:34:48.610 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_061224', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_061224/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-06-33-52', 'ft_id': 'ft-pZ2fCByMQ1Ns3ufp20Ln5qFx', 'date': '20230201_073448', 'train_file_id': 'file-YQqJ8tcQNIYF8bYKrxb8gEon', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_061224/train.jsonl: file-YQqJ8tcQNIYF8bYKrxb8gEon
Ran train size 1000 and got MAE 0.53216, GPR baseline 0.429676680692937
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  9.79402 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 26.6843  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.2096  │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/6.34M [00:00<?, ?it/s]Upload progress: 100%|██████████| 6.34M/6.34M [00:00<00:00, 11.0Git/s]
2023-02-01 07:42:41.305 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675233761,
  "events": [
    {
      "created_at": 1675233761,
      "level": "info",
      "message": "Created fine-tune: ft-I1Ei6iqft0d4FeMToK6AOhEi",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-I1Ei6iqft0d4FeMToK6AOhEi",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 6339796,
      "created_at": 1675233760,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_074228/train.jsonl",
      "id": "file-uuwsgmBQcWLju08ryUmCH4AH",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675233761,
  "validation_files": []
}
2023-02-01 07:42:41.471 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:44:41.994 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:46:42.497 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:48:43.034 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:50:43.563 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:52:44.025 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:54:44.553 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:56:45.052 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:58:45.575 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:00:46.117 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:02:46.625 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:04:47.155 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:06:47.683 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:08:48.224 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:10:48.730 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:12:49.266 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:14:49.717 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:16:50.275 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:18:50.798 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:20:51.340 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:22:51.850 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:24:52.296 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:26:52.754 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:28:53.281 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:30:53.799 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:32:54.357 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:34:54.887 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:36:55.433 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:38:55.995 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:40:56.554 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:42:57.001 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:44:57.539 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:46:58.044 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:48:58.495 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:50:59.002 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:52:59.502 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:55:00.005 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:57:00.485 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:59:01.020 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:01:01.554 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:03:02.063 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:05:02.517 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:07:03.033 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:09:03.556 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:11:04.075 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:13:04.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:15:05.120 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:17:05.601 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:19:06.046 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:21:06.515 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:23:06.944 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:25:07.507 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:27:08.051 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:29:08.590 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:31:09.116 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:33:09.605 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:35:10.123 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:37:10.611 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:39:11.166 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:41:11.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:43:12.249 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:45:12.958 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:47:13.399 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:49:13.947 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:51:14.486 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:53:14.956 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:55:15.734 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:57:16.258 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:59:16.780 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:01:17.223 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:03:17.770 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:05:18.317 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:07:18.817 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:09:19.342 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:11:19.944 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:13:20.436 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:15:20.961 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:17:21.497 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:19:21.989 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:21:22.531 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:23:23.123 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:25:25.378 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:27:25.901 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:29:26.446 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:31:26.974 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:33:27.483 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:35:27.953 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:37:28.479 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:39:28.947 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:41:29.482 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:43:30.022 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:45:30.563 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:47:31.070 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:49:31.544 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:51:31.978 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:53:32.458 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:55:33.002 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:57:33.537 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:59:34.071 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:01:34.524 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:03:34.982 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:05:35.460 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:07:35.953 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:09:36.496 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:11:36.996 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:13:37.463 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:15:39.869 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:17:40.411 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:19:40.948 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:21:41.409 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:23:41.929 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:25:42.450 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:27:42.970 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:29:43.493 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:31:43.971 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:33:44.460 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:35:44.933 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:37:45.456 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:39:45.981 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:41:46.508 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:43:47.015 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:45:47.545 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:47:48.050 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:49:48.575 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:51:49.105 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:53:49.578 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 11:55:50.113 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 11:57:50.595 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 11:59:51.139 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:01:51.688 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:03:52.217 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:05:52.752 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:07:53.288 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:09:53.848 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:11:54.424 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:13:54.966 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:15:55.464 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:17:56.001 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:19:56.549 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:21:57.103 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:23:57.645 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:25:58.105 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:27:58.595 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 12:29:59.109 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 12:29:59.109 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_074228', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_074228/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-11-28-24', 'ft_id': 'ft-I1Ei6iqft0d4FeMToK6AOhEi', 'date': '20230201_122959', 'train_file_id': 'file-uuwsgmBQcWLju08ryUmCH4AH', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_074228/train.jsonl: file-uuwsgmBQcWLju08ryUmCH4AH
Ran train size 5000 and got MAE 0.37712, GPR baseline 0.3554377721560356
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/3.98k [00:00<?, ?it/s]Upload progress: 100%|██████████| 3.98k/3.98k [00:00<00:00, 8.01Mit/s]
2023-02-01 12:30:28.771 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675251028,
  "events": [
    {
      "created_at": 1675251028,
      "level": "info",
      "message": "Created fine-tune: ft-CBoyjgt7AGovuNry8WGt3i8n",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-CBoyjgt7AGovuNry8WGt3i8n",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 3976,
      "created_at": 1675251028,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_123027/train.jsonl",
      "id": "file-LPzGNY4dBafgeHOVHSkQmXtF",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675251028,
  "validation_files": []
}
2023-02-01 12:30:28.944 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:32:29.519 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:34:30.036 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:36:30.538 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:38:31.061 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:40:31.586 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:42:32.101 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:44:32.633 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:46:33.121 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:48:33.669 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:50:34.148 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:52:34.611 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:54:35.173 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:56:35.715 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:58:36.231 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:00:36.776 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:02:37.309 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:04:37.842 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:06:38.368 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:08:38.866 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:10:39.363 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:12:39.886 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:14:40.446 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:16:40.942 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:18:41.439 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:20:41.983 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:22:42.509 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:24:43.079 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:26:43.613 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:28:44.147 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:30:44.691 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:32:45.233 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:34:45.770 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:36:46.289 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:38:46.835 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:40:47.386 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:42:47.852 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:44:48.351 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:46:48.913 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:48:49.439 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:50:49.905 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:52:50.468 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:54:51.016 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:56:51.540 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:58:52.048 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:00:52.622 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:02:53.108 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:04:53.623 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:06:54.160 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:08:54.660 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:10:55.193 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:12:55.788 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:14:56.326 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:16:56.863 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:18:57.465 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 14:18:57.466 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_123027', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_123027/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-13-18-26', 'ft_id': 'ft-CBoyjgt7AGovuNry8WGt3i8n', 'date': '20230201_141857', 'train_file_id': 'file-LPzGNY4dBafgeHOVHSkQmXtF', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_123027/train.jsonl: file-LPzGNY4dBafgeHOVHSkQmXtF
Ran train size 10 and got MAE 0.6762400000000001, GPR baseline 0.6018326625429775
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.93116 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  2.96219 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.60704 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/8.77k [00:00<?, ?it/s]Upload progress: 100%|██████████| 8.77k/8.77k [00:00<00:00, 16.9Mit/s]
2023-02-01 14:19:20.958 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675257560,
  "events": [
    {
      "created_at": 1675257560,
      "level": "info",
      "message": "Created fine-tune: ft-jONvXJWtrUpVelJzGbTaxCRP",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-jONvXJWtrUpVelJzGbTaxCRP",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 8766,
      "created_at": 1675257560,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/bandgap/out/20230201_141919/train.jsonl",
      "id": "file-QDVL1y6S31ekewF7IVojicY3",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675257560,
  "validation_files": []
}
2023-02-01 14:19:21.133 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:21:21.622 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:23:22.192 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:25:23.290 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:27:23.821 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:29:24.311 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:31:24.839 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:33:25.372 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:35:25.923 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:37:26.445 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:39:26.979 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:41:27.508 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:43:28.059 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:45:33.835 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:47:34.388 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:49:34.944 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:51:35.479 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:53:35.994 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:55:39.928 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:57:40.457 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:59:40.997 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:01:41.459 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:03:41.987 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:05:42.428 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:07:42.950 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:09:43.479 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:11:44.006 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:13:44.530 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:15:45.786 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:17:46.320 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:19:46.847 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:21:47.363 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:23:47.797 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:25:49.787 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:27:50.228 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:29:50.723 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:31:51.247 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:33:51.786 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:35:52.346 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:37:52.810 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:39:53.339 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:41:53.875 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:43:54.400 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:45:54.883 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:47:55.402 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:49:55.904 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:51:56.364 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:53:56.899 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:55:57.342 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:57:57.861 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:59:58.374 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:01:58.907 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:03:59.449 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:05:59.972 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:08:00.508 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:10:01.030 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:12:01.556 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:14:02.105 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:16:02.563 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:18:03.098 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:20:03.637 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:22:04.167 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:24:04.709 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:26:05.230 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:28:05.760 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:30:06.293 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:32:06.792 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:34:07.294 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:36:07.790 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:38:08.267 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:40:08.789 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:42:09.225 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:44:09.716 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:46:10.238 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:48:10.767 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:50:11.303 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:52:11.742 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:54:12.275 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:56:12.799 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:58:13.332 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:00:13.843 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:02:14.381 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:04:14.883 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:06:15.431 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:08:15.964 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:10:16.493 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:12:17.033 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:14:17.473 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:16:18.007 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:18:18.552 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:20:19.067 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:22:19.535 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:24:20.077 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:26:20.612 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:28:21.098 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:30:21.597 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:32:22.131 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:34:22.637 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:36:23.160 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:38:23.677 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:40:24.214 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:42:24.734 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:44:25.235 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:46:25.667 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:48:26.186 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:50:26.666 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:52:27.214 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:54:27.730 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:56:28.262 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:58:28.771 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 18:00:29.316 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
