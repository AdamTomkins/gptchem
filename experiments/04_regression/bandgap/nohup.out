2023-01-22 15:10:43.621949: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/3.69k [00:00<?, ?it/s]Upload progress: 100%|██████████| 3.69k/3.69k [00:00<00:00, 1.56Mit/s]
2023-01-22 15:10:45.887 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674396645,
  "events": [
    {
      "created_at": 1674396645,
      "level": "info",
      "message": "Created fine-tune: ft-qWmLnfDzVqdtj7zHfy9E7e4V",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-qWmLnfDzVqdtj7zHfy9E7e4V",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 3692,
      "created_at": 1674396644,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_151043/train.jsonl",
      "id": "file-hXD8vmruI1gsXeHEuJJD5bv5",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674396645,
  "validation_files": []
}
2023-01-22 15:10:46.254 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:12:47.039 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:14:47.807 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:16:48.674 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 15:16:48.697 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_151043', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_151043/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-14-15-47', 'ft_id': 'ft-qWmLnfDzVqdtj7zHfy9E7e4V', 'date': '20230122_151648', 'train_file_id': 'file-hXD8vmruI1gsXeHEuJJD5bv5', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_151043/train.jsonl: file-hXD8vmruI1gsXeHEuJJD5bv5
Ran train size 10 and got MAE 0.6918800000000002, GPR baseline 0.6018326625429775
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.93116 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  2.96219 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.60704 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/8.63k [00:00<?, ?it/s]Upload progress: 100%|██████████| 8.63k/8.63k [00:00<00:00, 15.8Mit/s]
2023-01-22 15:17:19.838 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674397039,
  "events": [
    {
      "created_at": 1674397039,
      "level": "info",
      "message": "Created fine-tune: ft-wgnoleP5XKpdt3ZBKYi8yy9J",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-wgnoleP5XKpdt3ZBKYi8yy9J",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 8630,
      "created_at": 1674397039,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_151718/train.jsonl",
      "id": "file-2tlWe44XgaeFxuBnIYjirmDP",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674397039,
  "validation_files": []
}
2023-01-22 15:17:20.240 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:19:20.741 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:21:21.329 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:23:22.409 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:25:23.274 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:27:23.983 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:29:25.734 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:31:26.423 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:33:26.981 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 15:33:26.982 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_151718', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_151718/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-14-33-05', 'ft_id': 'ft-wgnoleP5XKpdt3ZBKYi8yy9J', 'date': '20230122_153326', 'train_file_id': 'file-2tlWe44XgaeFxuBnIYjirmDP', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_151718/train.jsonl: file-2tlWe44XgaeFxuBnIYjirmDP
Ran train size 20 and got MAE 0.7073999999999999, GPR baseline 0.6135135226849172
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.35378 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  9.5244  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.48374 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/21.5k [00:00<?, ?it/s]Upload progress: 100%|██████████| 21.5k/21.5k [00:00<00:00, 26.9Mit/s]
2023-01-22 15:34:00.938 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674398040,
  "events": [
    {
      "created_at": 1674398040,
      "level": "info",
      "message": "Created fine-tune: ft-da5gtqXP0IyODfgSNmNrDwOv",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-da5gtqXP0IyODfgSNmNrDwOv",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 21544,
      "created_at": 1674398040,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_153359/train.jsonl",
      "id": "file-ztgLzOK86OkxvLtKtU0K36kl",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674398040,
  "validation_files": []
}
2023-01-22 15:34:01.291 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:36:02.038 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:38:03.244 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:40:04.522 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:42:05.409 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:44:10.189 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:46:11.242 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 15:48:12.036 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 15:48:12.038 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_153359', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_153359/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-14-47-22', 'ft_id': 'ft-da5gtqXP0IyODfgSNmNrDwOv', 'date': '20230122_154812', 'train_file_id': 'file-ztgLzOK86OkxvLtKtU0K36kl', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_153359/train.jsonl: file-ztgLzOK86OkxvLtKtU0K36kl
Ran train size 50 and got MAE 0.6413999999999999, GPR baseline 0.6010078609589953
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.06015 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 47.2285  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.11942 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/43.0k [00:00<?, ?it/s]Upload progress: 100%|██████████| 43.0k/43.0k [00:00<00:00, 14.4Mit/s]
2023-01-22 15:48:45.788 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674398925,
  "events": [
    {
      "created_at": 1674398925,
      "level": "info",
      "message": "Created fine-tune: ft-q5aOrZnVRITsGKHNWf0ZEqqG",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-q5aOrZnVRITsGKHNWf0ZEqqG",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 42984,
      "created_at": 1674398925,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_154843/train.jsonl",
      "id": "file-qIhyiP5p8wISaRFZMOpX2A2Z",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674398925,
  "validation_files": []
}
2023-01-22 15:48:46.073 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:50:46.703 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:52:47.639 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:54:48.408 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:56:49.205 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 15:58:50.844 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:00:52.689 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:02:53.269 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:04:53.986 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:06:54.772 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:08:55.421 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:10:56.265 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:12:56.857 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:14:57.443 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 16:16:58.138 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 16:16:58.142 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_154843', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_154843/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-15-16-20', 'ft_id': 'ft-q5aOrZnVRITsGKHNWf0ZEqqG', 'date': '20230122_161658', 'train_file_id': 'file-qIhyiP5p8wISaRFZMOpX2A2Z', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_154843/train.jsonl: file-qIhyiP5p8wISaRFZMOpX2A2Z
Ran train size 100 and got MAE 0.6257999999999999, GPR baseline 0.5007550384885529
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  2.42499 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 53.834   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.03253 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/86.6k [00:00<?, ?it/s]Upload progress: 100%|██████████| 86.6k/86.6k [00:00<00:00, 145Mit/s]
2023-01-22 16:17:33.239 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674400652,
  "events": [
    {
      "created_at": 1674400652,
      "level": "info",
      "message": "Created fine-tune: ft-RA3eUYI1mf6aGUs0v9mGyJ13",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-RA3eUYI1mf6aGUs0v9mGyJ13",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 86564,
      "created_at": 1674400652,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_161729/train.jsonl",
      "id": "file-xdxZ1JQ1eTLlCp6hdkAgY8fP",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674400652,
  "validation_files": []
}
2023-01-22 16:17:33.683 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:19:35.847 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:21:36.836 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:23:37.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:25:38.297 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:27:39.100 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:29:39.804 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:31:40.814 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:33:41.323 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:35:42.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:37:43.592 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:39:44.211 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 16:41:44.905 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 16:43:45.889 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 16:43:45.894 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_161729', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_161729/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-15-42-51', 'ft_id': 'ft-RA3eUYI1mf6aGUs0v9mGyJ13', 'date': '20230122_164345', 'train_file_id': 'file-xdxZ1JQ1eTLlCp6hdkAgY8fP', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_161729/train.jsonl: file-xdxZ1JQ1eTLlCp6hdkAgY8fP
Ran train size 200 and got MAE 0.5862, GPR baseline 0.5026799620857773
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  7.52693 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 44.5921  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.11016 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/442k [00:00<?, ?it/s]Upload progress: 100%|██████████| 442k/442k [00:00<00:00, 778Mit/s]
2023-01-22 16:44:23.967 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674402263,
  "events": [
    {
      "created_at": 1674402263,
      "level": "info",
      "message": "Created fine-tune: ft-TGb1HnKmreydBi8cH6K6a0Lm",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-TGb1HnKmreydBi8cH6K6a0Lm",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 442248,
      "created_at": 1674402262,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_164420/train.jsonl",
      "id": "file-GwSul1089BJa2lDm5E5jzjbx",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674402263,
  "validation_files": []
}
2023-01-22 16:44:24.622 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:46:25.469 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:48:27.122 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:50:28.162 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:52:28.706 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:54:30.284 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:56:30.729 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 16:58:32.285 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:00:33.024 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:02:33.728 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:04:35.383 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:06:36.457 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:08:37.036 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:10:42.620 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:12:46.811 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:14:48.436 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:16:49.243 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:18:50.248 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:20:51.118 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:22:52.094 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:24:54.086 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 17:24:54.090 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_164420', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_164420/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-16-22-54', 'ft_id': 'ft-TGb1HnKmreydBi8cH6K6a0Lm', 'date': '20230122_172454', 'train_file_id': 'file-GwSul1089BJa2lDm5E5jzjbx', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_164420/train.jsonl: file-GwSul1089BJa2lDm5E5jzjbx
Ran train size 1000 and got MAE 0.49863999999999997, GPR baseline 0.42967668069302545
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  9.79402 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 26.6843  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.2096  │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/2.21M [00:00<?, ?it/s]Upload progress: 100%|██████████| 2.21M/2.21M [00:00<00:00, 4.49Git/s]
2023-01-22 17:29:18.474 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674404958,
  "events": [
    {
      "created_at": 1674404958,
      "level": "info",
      "message": "Created fine-tune: ft-W0SkxN4Y6ZUbb4MadBBtRKXS",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-W0SkxN4Y6ZUbb4MadBBtRKXS",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 2207226,
      "created_at": 1674404957,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_172913/train.jsonl",
      "id": "file-zBX3AMVaKED0FSkzv2PqcBsd",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674404958,
  "validation_files": []
}
2023-01-22 17:29:18.993 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 17:31:19.482 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 17:33:21.269 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 17:35:25.449 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 17:37:25.977 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:39:26.731 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:41:29.474 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:43:29.958 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:45:32.324 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:47:32.942 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:49:34.261 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:51:36.668 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:53:37.520 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:55:38.563 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:57:39.624 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 17:59:41.753 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 18:01:42.400 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 18:03:42.951 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 18:05:43.755 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 18:07:44.359 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 18:07:44.362 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_172913', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_172913/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-17-06-18', 'ft_id': 'ft-W0SkxN4Y6ZUbb4MadBBtRKXS', 'date': '20230122_180744', 'train_file_id': 'file-zBX3AMVaKED0FSkzv2PqcBsd', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_172913/train.jsonl: file-zBX3AMVaKED0FSkzv2PqcBsd
Ran train size 5000 and got MAE 0.35172000000000003, GPR baseline 0.3554377721559475
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/10.4k [00:00<?, ?it/s]Upload progress: 100%|██████████| 10.4k/10.4k [00:00<00:00, 23.3Mit/s]
2023-01-22 18:08:12.439 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674407292,
  "events": [
    {
      "created_at": 1674407292,
      "level": "info",
      "message": "Created fine-tune: ft-S4OI19lUYowa5z9oKoWspHN5",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-S4OI19lUYowa5z9oKoWspHN5",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 10362,
      "created_at": 1674407291,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_180810/train.jsonl",
      "id": "file-zCXTKQsTLHSItduSWaRvLmDB",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674407292,
  "validation_files": []
}
2023-01-22 18:08:12.633 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:10:16.578 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:12:18.889 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:14:41.336 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 18:14:41.337 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_180810', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_180810/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-17-14-27', 'ft_id': 'ft-S4OI19lUYowa5z9oKoWspHN5', 'date': '20230122_181441', 'train_file_id': 'file-zCXTKQsTLHSItduSWaRvLmDB', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_180810/train.jsonl: file-zCXTKQsTLHSItduSWaRvLmDB
Ran train size 10 and got MAE 0.7436799999999999, GPR baseline 0.6018326625429775
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.93116 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  2.96219 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.60704 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/24.7k [00:00<?, ?it/s]Upload progress: 100%|██████████| 24.7k/24.7k [00:00<00:00, 43.4Mit/s]
2023-01-22 18:15:15.063 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674407714,
  "events": [
    {
      "created_at": 1674407714,
      "level": "info",
      "message": "Created fine-tune: ft-ySKrVYzmsFnAmUFEyBJCAOEm",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-ySKrVYzmsFnAmUFEyBJCAOEm",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 24676,
      "created_at": 1674407712,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_181511/train.jsonl",
      "id": "file-T1A5bKlffRplnxgNOePzZkOW",
      "object": "file",
      "purpose": "fine-tune",
      "status": "processed",
      "status_details": null
    }
  ],
  "updated_at": 1674407714,
  "validation_files": []
}
2023-01-22 18:15:16.183 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:17:16.931 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:19:18.177 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:21:19.040 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:23:19.675 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:25:20.889 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 18:27:22.804 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 18:27:22.808 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_181511', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_181511/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-17-26-03', 'ft_id': 'ft-ySKrVYzmsFnAmUFEyBJCAOEm', 'date': '20230122_182722', 'train_file_id': 'file-T1A5bKlffRplnxgNOePzZkOW', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_181511/train.jsonl: file-T1A5bKlffRplnxgNOePzZkOW
Ran train size 20 and got MAE 0.84184, GPR baseline 0.6135135226849172
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.35378 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  9.5244  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.48374 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/62.2k [00:00<?, ?it/s]Upload progress: 100%|██████████| 62.2k/62.2k [00:00<00:00, 11.0Mit/s]
2023-01-22 18:27:58.373 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674408478,
  "events": [
    {
      "created_at": 1674408478,
      "level": "info",
      "message": "Created fine-tune: ft-gNIwpYPsvCOV7upNZKc3llmL",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-gNIwpYPsvCOV7upNZKc3llmL",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 62246,
      "created_at": 1674408477,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_182755/train.jsonl",
      "id": "file-YQNt3rPxIXNcUnjnfGUyJDOV",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674408478,
  "validation_files": []
}
2023-01-22 18:27:58.766 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:30:01.581 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:32:02.219 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:34:02.760 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:36:04.274 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:38:05.191 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:40:06.104 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:42:06.999 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:44:07.809 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:46:10.521 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:48:11.792 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:50:12.772 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:52:13.555 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:54:15.750 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:56:16.716 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 18:58:17.426 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 19:00:18.115 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 19:08:05.553 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 19:08:05.557 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_182755', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_182755/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-18-00-06', 'ft_id': 'ft-gNIwpYPsvCOV7upNZKc3llmL', 'date': '20230122_190805', 'train_file_id': 'file-YQNt3rPxIXNcUnjnfGUyJDOV', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_182755/train.jsonl: file-YQNt3rPxIXNcUnjnfGUyJDOV
Ran train size 50 and got MAE 0.7314, GPR baseline 0.6010078609589953
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.06015 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 47.2285  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.11942 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/123k [00:00<?, ?it/s]Upload progress: 100%|██████████| 123k/123k [00:00<00:00, 112Mit/s]
2023-01-22 19:08:40.859 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674410920,
  "events": [
    {
      "created_at": 1674410920,
      "level": "info",
      "message": "Created fine-tune: ft-lrQlYHDP5t8oA5zm91M6ddvn",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-lrQlYHDP5t8oA5zm91M6ddvn",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 123392,
      "created_at": 1674410919,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_190838/train.jsonl",
      "id": "file-6OcdbKjT9M9r8wLHiBrwrV3X",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674410920,
  "validation_files": []
}
2023-01-22 19:08:41.262 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 19:10:41.819 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 19:12:42.377 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-22 19:14:42.966 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-22 19:14:42.972 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_190838', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_190838/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-22-18-14-16', 'ft_id': 'ft-lrQlYHDP5t8oA5zm91M6ddvn', 'date': '20230122_191442', 'train_file_id': 'file-6OcdbKjT9M9r8wLHiBrwrV3X', 'valid_file_id': None}
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_190838/train.jsonl: file-6OcdbKjT9M9r8wLHiBrwrV3X
Ran train size 100 and got MAE 0.67208, GPR baseline 0.5007550384885529
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  2.42499 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 53.834   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.03253 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/248k [00:00<?, ?it/s]Upload progress: 100%|██████████| 248k/248k [00:00<00:00, 181Mit/s]
2023-01-22 19:15:15.818 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {
  "created_at": 1674411315,
  "events": [
    {
      "created_at": 1674411315,
      "level": "info",
      "message": "Created fine-tune: ft-QCEqYOH60lOOh8iaYnyNwZ3n",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-QCEqYOH60lOOh8iaYnyNwZ3n",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 247912,
      "created_at": 1674411315,
      "filename": "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_191513/train.jsonl",
      "id": "file-9ctrgpkHsHr10Tny0SgJdPKG",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1674411315,
  "validation_files": []
}
2023-01-22 19:15:16.046 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 19:17:16.746 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 19:19:19.422 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 19:21:21.953 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 19:23:22.678 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-22 19:26:48.232 | ERROR    | gptchem.tuner:tune:194 - Fine tuning failed.
Traceback (most recent call last):

  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
           │          └ <function create_connection at 0x1387cf820>
           └ <module 'urllib3.util.connection' from '/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/...
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/util/connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               │      │           │     │     │       │      └ <SocketKind.SOCK_STREAM: 1>
               │      │           │     │     │       └ <module 'socket' from '/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/socket.py'>
               │      │           │     │     └ <AddressFamily.AF_UNSPEC: 0>
               │      │           │     └ 443
               │      │           └ 'api.openai.com'
               │      └ <function getaddrinfo at 0x104b1a0d0>
               └ <module 'socket' from '/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/socket.py'>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/socket.py", line 954, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               │       │           │     │     │       │     │      └ 0
               │       │           │     │     │       │     └ 0
               │       │           │     │     │       └ <SocketKind.SOCK_STREAM: 1>
               │       │           │     │     └ <AddressFamily.AF_UNSPEC: 0>
               │       │           │     └ 443
               │       │           └ 'api.openai.com'
               │       └ <built-in function getaddrinfo>
               └ <module '_socket' from '/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/lib-dynload/_socket.cpython-39-darwin....

socket.gaierror: [Errno 8] nodename nor servname provided, or not known


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
                       │    └ <function HTTPConnectionPool._make_request at 0x1388a3280>
                       └ <urllib3.connectionpool.HTTPSConnectionPool object at 0x29f79f580>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._validate_conn(conn)
    │    │              └ <urllib3.connection.HTTPSConnection object at 0x29f0abc10>
    │    └ <function HTTPSConnectionPool._validate_conn at 0x1388a3790>
    └ <urllib3.connectionpool.HTTPSConnectionPool object at 0x29f79f580>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/connectionpool.py", line 1042, in _validate_conn
    conn.connect()
    │    └ <function HTTPSConnection.connect at 0x13887ce50>
    └ <urllib3.connection.HTTPSConnection object at 0x29f0abc10>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/connection.py", line 358, in connect
    self.sock = conn = self._new_conn()
    │    │             │    └ <function HTTPConnection._new_conn at 0x13887c8b0>
    │    │             └ <urllib3.connection.HTTPSConnection object at 0x29f0abc10>
    │    └ None
    └ <urllib3.connection.HTTPSConnection object at 0x29f0abc10>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/connection.py", line 186, in _new_conn
    raise NewConnectionError(
          └ <class 'urllib3.exceptions.NewConnectionError'>

urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x29f0abc10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/requests/adapters.py", line 489, in send
    resp = conn.urlopen(
           │    └ <function HTTPConnectionPool.urlopen at 0x1388a34c0>
           └ <urllib3.connectionpool.HTTPSConnectionPool object at 0x29f79f580>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/connectionpool.py", line 815, in urlopen
    return self.urlopen(
           │    └ <function HTTPConnectionPool.urlopen at 0x1388a34c0>
           └ <urllib3.connectionpool.HTTPSConnectionPool object at 0x29f79f580>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/connectionpool.py", line 815, in urlopen
    return self.urlopen(
           │    └ <function HTTPConnectionPool.urlopen at 0x1388a34c0>
           └ <urllib3.connectionpool.HTTPSConnectionPool object at 0x29f79f580>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    retries = retries.increment(
              │       └ <function Retry.increment at 0x1387eff70>
              └ Retry(total=0, connect=None, read=None, redirect=None, status=None)
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/urllib3/util/retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
          │             │      │    │        │             └ 'unknown'
          │             │      │    │        └ <class 'urllib3.exceptions.ResponseError'>
          │             │      │    └ NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x29f0abc10>: Failed to establish a new connection: [Errno ...
          │             │      └ '/v1/fine-tunes/ft-QCEqYOH60lOOh8iaYnyNwZ3n'
          │             └ <urllib3.connectionpool.HTTPSConnectionPool object at 0x29f79f580>
          └ <class 'urllib3.exceptions.MaxRetryError'>

urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/fine-tunes/ft-QCEqYOH60lOOh8iaYnyNwZ3n (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x29f0abc10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_requestor.py", line 498, in request_raw
    result = _thread_context.session.request(
             └ <_thread._local object at 0x13f435a40>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/requests/sessions.py", line 587, in request
    resp = self.send(prep, **send_kwargs)
           │    │    │       └ {'timeout': 600, 'allow_redirects': True, 'proxies': OrderedDict(), 'stream': False, 'verify': True, 'cert': None}
           │    │    └ <PreparedRequest [GET]>
           │    └ <function Session.send at 0x138b9e280>
           └ <requests.sessions.Session object at 0x13f906bb0>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/requests/sessions.py", line 701, in send
    r = adapter.send(request, **kwargs)
        │       │    │          └ {'timeout': 600, 'proxies': OrderedDict(), 'stream': False, 'verify': True, 'cert': None}
        │       │    └ <PreparedRequest [GET]>
        │       └ <function HTTPAdapter.send at 0x138b9b700>
        └ <requests.adapters.HTTPAdapter object at 0x29f79f850>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/requests/adapters.py", line 565, in send
    raise ConnectionError(e, request=request)
          │                          └ <PreparedRequest [GET]>
          └ <class 'requests.exceptions.ConnectionError'>

requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/fine-tunes/ft-QCEqYOH60lOOh8iaYnyNwZ3n (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x29f0abc10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))


The above exception was the direct cause of the following exception:


Traceback (most recent call last):

  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/run_experiments.py", line 84, in <module>
    train_test_model(representation, num_train_points, seed + 443565)
    │                │               │                 └ 0
    │                │               └ 200
    │                └ 'SELFIES'
    └ <function train_test_model at 0x1048e30d0>

  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/run_experiments.py", line 56, in train_test_model
    tune_res = tuner(train_formatted)
               │     └                                                 prompt  ...                                     representation
               │       0    What is t...
               └ gptchem.tuner.Tuner(base_model='ada', batch_size=None, n_epochs=8, learning_rate_multiplier=0.02, run_name=None)

  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/tuner.py", line 208, in __call__
    return self.tune(train_df, validation_df)
           │    │    │         └ None
           │    │    └                                                 prompt  ...                                     representation
           │    │      0    What is t...
           │    └ <function Tuner.tune at 0x13f908d30>
           └ gptchem.tuner.Tuner(base_model='ada', batch_size=None, n_epochs=8, learning_rate_multiplier=0.02, run_name=None)

> File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/tuner.py", line 189, in tune
    modelname = get_ft_model_name(ft_id, self._sleep)
                │                 │      │    └ 120
                │                 │      └ gptchem.tuner.Tuner(base_model='ada', batch_size=None, n_epochs=8, learning_rate_multiplier=0.02, run_name=None)
                │                 └ 'ft-QCEqYOH60lOOh8iaYnyNwZ3n'
                └ <function get_ft_model_name at 0x13f908790>

  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/tuner.py", line 27, in get_ft_model_name
    ft = FineTune.retrieve(id=ft_id)
         │        │           └ 'ft-QCEqYOH60lOOh8iaYnyNwZ3n'
         │        └ <classmethod object at 0x13f42b6a0>
         └ <class 'openai.api_resources.fine_tune.FineTune'>

  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_resources/abstract/api_resource.py", line 20, in retrieve
    instance.refresh(request_id=request_id, request_timeout=request_timeout)
    │        │                  │                           └ None
    │        │                  └ None
    │        └ <function APIResource.refresh at 0x13f8708b0>
    └ <FineTune id=ft-QCEqYOH60lOOh8iaYnyNwZ3n at 0x15a000f90> JSON: {
        "id": "ft-QCEqYOH60lOOh8iaYnyNwZ3n"
      }
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_resources/abstract/api_resource.py", line 32, in refresh
    self.request(
    │    └ <function OpenAIObject.request at 0x13f870310>
    └ <FineTune id=ft-QCEqYOH60lOOh8iaYnyNwZ3n at 0x15a000f90> JSON: {
        "id": "ft-QCEqYOH60lOOh8iaYnyNwZ3n"
      }
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/openai_object.py", line 179, in request
    response, stream, api_key = requestor.request(
              │                 │         └ <function APIRequestor.request at 0x13f869430>
              │                 └ <openai.api_requestor.APIRequestor object at 0x29d5e3a00>
              └ False
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_requestor.py", line 216, in request
    result = self.request_raw(
             │    └ <function APIRequestor.request_raw at 0x13f869790>
             └ <openai.api_requestor.APIRequestor object at 0x29d5e3a00>
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/api_requestor.py", line 510, in request_raw
    raise error.APIConnectionError("Error communicating with OpenAI: {}".format(e)) from e
          │     └ <class 'openai.error.APIConnectionError'>
          └ <module 'openai.error' from '/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/openai/error.py'>

openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/fine-tunes/ft-QCEqYOH60lOOh8iaYnyNwZ3n (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x29f0abc10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))
Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/out/20230122_191513/train.jsonl: file-9ctrgpkHsHr10Tny0SgJdPKG
Traceback (most recent call last):
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/run_experiments.py", line 84, in <module>
    train_test_model(representation, num_train_points, seed + 443565)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/bandgap/run_experiments.py", line 56, in train_test_model
    tune_res = tuner(train_formatted)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/tuner.py", line 208, in __call__
    return self.tune(train_df, validation_df)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/tuner.py", line 196, in tune
    self._modelname = modelname
UnboundLocalError: local variable 'modelname' referenced before assignment
