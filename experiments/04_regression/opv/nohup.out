nohup: ignoring input
2023-01-31 01:24:26.070166: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-31 01:24:27.224938: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-01-31 01:24:27.225056: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-01-31 01:24:27.225069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-31 01:24:33.416091: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2023-01-31 01:24:33.416129: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2023-01-31 01:24:33.416163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kevin-OptiPlex-9020): /proc/driver/nvidia/version does not exist
2023-01-31 01:24:33.416411: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From /home/kevin/anaconda3/envs/gptchem/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.
Instructions for updating:
Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ 1e-05   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 0       │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │ 0.99998 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/3.97k [00:00<?, ?it/s]Upload progress: 100%|██████████| 3.97k/3.97k [00:00<00:00, 3.96Mit/s]
2023-01-31 01:24:37.198 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675124677,
  "events": [
    {
      "created_at": 1675124677,
      "level": "info",
      "message": "Created fine-tune: ft-5X19eMGNhkGrWfsB7rSngdyh",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-5X19eMGNhkGrWfsB7rSngdyh",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 3974,
      "created_at": 1675124676,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_012435/train.jsonl",
      "id": "file-88JMHKMAplvYFlYkJhw26Ahz",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675124677,
  "validation_files": []
}
2023-01-31 01:24:37.378 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:26:37.909 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:28:38.453 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:30:39.019 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:32:39.556 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:34:40.135 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:36:40.668 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:38:41.212 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:40:41.756 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:42:42.305 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:44:42.866 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:46:43.422 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:48:43.986 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:50:44.528 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:52:45.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:54:45.624 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:56:46.161 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 01:58:46.724 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:00:47.263 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:02:47.810 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:04:48.358 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:06:48.898 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:08:49.426 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:10:49.956 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:12:50.517 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:14:51.060 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:16:51.620 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:18:52.184 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:20:52.720 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:22:53.304 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:24:53.844 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:26:54.392 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:28:54.952 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:30:55.537 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:32:56.124 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:34:56.684 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:36:57.241 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:38:57.824 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:40:58.381 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:42:58.960 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:44:59.508 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:47:00.076 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:49:00.616 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:51:01.172 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:53:01.717 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:55:02.261 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:57:02.806 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 02:59:03.371 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:01:03.940 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 03:01:03.941 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_012435', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_012435/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-02-00-11', 'ft_id': 'ft-5X19eMGNhkGrWfsB7rSngdyh', 'date': '20230131_030103', 'train_file_id': 'file-88JMHKMAplvYFlYkJhw26Ahz', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_012435/train.jsonl: file-88JMHKMAplvYFlYkJhw26Ahz
Ran train size 10 and got MAE 2.78864, GPR baseline 2.091499877331547
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/18.9k [00:00<?, ?it/s]Upload progress: 100%|██████████| 18.9k/18.9k [00:00<00:00, 30.8Mit/s]
2023-01-31 03:01:12.720 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675130472,
  "events": [
    {
      "created_at": 1675130472,
      "level": "info",
      "message": "Created fine-tune: ft-mpnmHBbMydRDIxKTXOIQtjW1",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-mpnmHBbMydRDIxKTXOIQtjW1",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 18934,
      "created_at": 1675130472,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_030111/train.jsonl",
      "id": "file-FlPKJy5fCiFlAB4o9ej23lIr",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675130472,
  "validation_files": []
}
2023-01-31 03:01:12.900 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:03:13.468 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:05:14.024 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:07:14.568 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:09:15.121 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:11:15.663 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:13:16.195 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:15:16.744 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:17:17.302 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:19:17.840 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:21:18.384 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:23:18.915 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:25:19.476 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:27:20.017 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:29:20.588 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:31:21.151 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:33:21.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:35:22.269 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:37:22.806 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:39:23.336 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:41:23.895 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:43:24.433 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:45:24.988 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:47:25.556 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:49:26.114 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:51:26.656 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:53:27.195 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:55:27.760 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:57:28.312 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 03:59:28.872 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 04:01:29.444 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 04:01:29.445 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_030111', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_030111/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-03-00-33', 'ft_id': 'ft-mpnmHBbMydRDIxKTXOIQtjW1', 'date': '20230131_040129', 'train_file_id': 'file-FlPKJy5fCiFlAB4o9ej23lIr', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_030111/train.jsonl: file-FlPKJy5fCiFlAB4o9ej23lIr
Ran train size 50 and got MAE 2.67356, GPR baseline 1.9859199984252607
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/37.5k [00:00<?, ?it/s]Upload progress: 100%|██████████| 37.5k/37.5k [00:00<00:00, 57.3Mit/s]
2023-01-31 04:01:38.876 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675134098,
  "events": [
    {
      "created_at": 1675134098,
      "level": "info",
      "message": "Created fine-tune: ft-tCSBS4exLUmqtMnpFH7utx1n",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-tCSBS4exLUmqtMnpFH7utx1n",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 37522,
      "created_at": 1675134098,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_040137/train.jsonl",
      "id": "file-cZSwncZLInbKbqsPE8iA9r6n",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675134098,
  "validation_files": []
}
2023-01-31 04:01:39.056 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:03:39.612 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:05:40.176 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:07:40.724 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:09:41.292 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:11:41.852 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:13:42.404 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:15:42.984 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 04:17:43.543 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 04:17:43.543 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_040137', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_040137/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-03-16-04', 'ft_id': 'ft-tCSBS4exLUmqtMnpFH7utx1n', 'date': '20230131_041743', 'train_file_id': 'file-cZSwncZLInbKbqsPE8iA9r6n', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_040137/train.jsonl: file-cZSwncZLInbKbqsPE8iA9r6n
Ran train size 100 and got MAE 2.6028800000000003, GPR baseline 1.910187555252833
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.52274 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 76.7416  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.63396 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/74.9k [00:00<?, ?it/s]Upload progress: 100%|██████████| 74.9k/74.9k [00:00<00:00, 89.6Mit/s]
2023-01-31 04:17:53.756 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675135073,
  "events": [
    {
      "created_at": 1675135073,
      "level": "info",
      "message": "Created fine-tune: ft-mD9zoQVVYGqte39luE1sbr9W",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-mD9zoQVVYGqte39luE1sbr9W",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 74908,
      "created_at": 1675135073,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_041751/train.jsonl",
      "id": "file-vag07HVKlNDpYMaUOmHRsivU",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675135073,
  "validation_files": []
}
2023-01-31 04:17:53.932 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:19:54.487 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:21:55.025 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:23:55.594 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:25:56.155 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:27:56.716 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:29:57.255 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 04:31:57.811 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 04:33:58.383 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 04:33:58.383 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_041751', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_041751/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-03-33-37', 'ft_id': 'ft-mD9zoQVVYGqte39luE1sbr9W', 'date': '20230131_043358', 'train_file_id': 'file-vag07HVKlNDpYMaUOmHRsivU', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_041751/train.jsonl: file-vag07HVKlNDpYMaUOmHRsivU
Ran train size 200 and got MAE 2.12892, GPR baseline 1.746607858633924
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │     value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  -3.05847 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 138.088   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   0.593   │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════╛
Upload progress:   0%|          | 0.00/187k [00:00<?, ?it/s]Upload progress: 100%|██████████| 187k/187k [00:00<00:00, 230Mit/s]
2023-01-31 04:34:12.660 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675136052,
  "events": [
    {
      "created_at": 1675136052,
      "level": "info",
      "message": "Created fine-tune: ft-L84QEJ9ubL9B2wnT7tPbl8TP",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-L84QEJ9ubL9B2wnT7tPbl8TP",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 186768,
      "created_at": 1675136052,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_043410/train.jsonl",
      "id": "file-80d9urwJerddLEMNka0CnZn7",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675136052,
  "validation_files": []
}
2023-01-31 04:34:12.855 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:36:13.416 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:38:13.954 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:40:14.508 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:42:15.047 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:44:15.588 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:46:16.116 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:48:16.662 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:50:17.206 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:52:17.793 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:54:18.353 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:56:18.893 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 04:58:19.448 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:00:20.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:02:20.628 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:04:21.160 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:06:21.708 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 05:08:22.268 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 05:10:22.805 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 05:12:23.271 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 05:14:23.797 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 05:16:24.360 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 05:16:24.361 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_043410', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_043410/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-04-15-51', 'ft_id': 'ft-L84QEJ9ubL9B2wnT7tPbl8TP', 'date': '20230131_051624', 'train_file_id': 'file-80d9urwJerddLEMNka0CnZn7', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_043410/train.jsonl: file-80d9urwJerddLEMNka0CnZn7
Ran train size 500 and got MAE 1.97412, GPR baseline 1.7283103030403382
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ 1e-05   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 0       │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │ 0.99998 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/14.6k [00:00<?, ?it/s]Upload progress: 100%|██████████| 14.6k/14.6k [00:00<00:00, 16.7Mit/s]
2023-01-31 05:16:32.039 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675138591,
  "events": [
    {
      "created_at": 1675138591,
      "level": "info",
      "message": "Created fine-tune: ft-QOZnq0JDXU9B7vNhtOgaCs48",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-QOZnq0JDXU9B7vNhtOgaCs48",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 14598,
      "created_at": 1675138591,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_051630/train.jsonl",
      "id": "file-d3TAmoDOzUsv9pOnKj1iMkQ1",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675138591,
  "validation_files": []
}
2023-01-31 05:16:32.221 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:18:32.782 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:20:33.297 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:22:33.880 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:24:34.356 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:26:34.876 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:28:35.356 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:30:35.903 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:32:36.398 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:34:36.905 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:36:37.344 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:38:37.828 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:40:38.396 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 05:40:38.396 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_051630', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_051630/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-04-39-39', 'ft_id': 'ft-QOZnq0JDXU9B7vNhtOgaCs48', 'date': '20230131_054038', 'train_file_id': 'file-d3TAmoDOzUsv9pOnKj1iMkQ1', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_051630/train.jsonl: file-d3TAmoDOzUsv9pOnKj1iMkQ1
Ran train size 10 and got MAE nan, GPR baseline 2.091499877331547
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/68.7k [00:00<?, ?it/s]Upload progress: 100%|██████████| 68.7k/68.7k [00:00<00:00, 138Mit/s]
2023-01-31 05:40:47.609 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675140047,
  "events": [
    {
      "created_at": 1675140047,
      "level": "info",
      "message": "Created fine-tune: ft-kqc41IFnCIO15KVNMH08ZCEA",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-kqc41IFnCIO15KVNMH08ZCEA",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 68656,
      "created_at": 1675140047,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_054045/train.jsonl",
      "id": "file-RsltFSTXiK9SqLLI5Fijgz8a",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675140047,
  "validation_files": []
}
2023-01-31 05:40:47.799 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:42:48.322 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:44:48.807 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:46:49.355 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 05:46:49.355 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_054045', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_054045/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-04-46-42', 'ft_id': 'ft-kqc41IFnCIO15KVNMH08ZCEA', 'date': '20230131_054649', 'train_file_id': 'file-RsltFSTXiK9SqLLI5Fijgz8a', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_054045/train.jsonl: file-RsltFSTXiK9SqLLI5Fijgz8a
Ran train size 50 and got MAE 2.31692, GPR baseline 1.9859199984252607
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/135k [00:00<?, ?it/s]Upload progress: 100%|██████████| 135k/135k [00:00<00:00, 276Mit/s]
2023-01-31 05:46:58.066 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675140417,
  "events": [
    {
      "created_at": 1675140417,
      "level": "info",
      "message": "Created fine-tune: ft-l7uaLVpWC9JHYM7l9VYvmqY1",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-l7uaLVpWC9JHYM7l9VYvmqY1",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 135430,
      "created_at": 1675140417,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_054656/train.jsonl",
      "id": "file-rxtoPOCKTsmfcBIgJgYYt732",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675140417,
  "validation_files": []
}
2023-01-31 05:46:58.245 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:48:58.710 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:50:59.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:52:59.711 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:55:00.244 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 05:57:00.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 05:59:01.192 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 05:59:01.192 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_054656', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_054656/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-04-57-28', 'ft_id': 'ft-l7uaLVpWC9JHYM7l9VYvmqY1', 'date': '20230131_055901', 'train_file_id': 'file-rxtoPOCKTsmfcBIgJgYYt732', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_054656/train.jsonl: file-rxtoPOCKTsmfcBIgJgYYt732
Ran train size 100 and got MAE 2.2896, GPR baseline 1.910187555252833
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.52274 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 76.7416  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.63396 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/270k [00:00<?, ?it/s]Upload progress: 100%|██████████| 270k/270k [00:00<00:00, 571Mit/s]
2023-01-31 05:59:11.309 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675141151,
  "events": [
    {
      "created_at": 1675141151,
      "level": "info",
      "message": "Created fine-tune: ft-6Ga1iEuExF4Oe6BQVEJU0chb",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-6Ga1iEuExF4Oe6BQVEJU0chb",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 269612,
      "created_at": 1675141150,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_055909/train.jsonl",
      "id": "file-GRZrk8T54pql2t15YODVsZNe",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675141151,
  "validation_files": []
}
2023-01-31 05:59:11.480 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:01:12.011 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:03:12.568 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:05:13.103 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:07:13.645 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 06:09:14.165 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 06:11:14.691 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 06:11:14.691 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_055909', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_055909/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-05-10-27', 'ft_id': 'ft-6Ga1iEuExF4Oe6BQVEJU0chb', 'date': '20230131_061114', 'train_file_id': 'file-GRZrk8T54pql2t15YODVsZNe', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_055909/train.jsonl: file-GRZrk8T54pql2t15YODVsZNe
Ran train size 200 and got MAE 2.26092, GPR baseline 1.746607858633924
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │     value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  -3.05847 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 138.088   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   0.593   │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════╛
Upload progress:   0%|          | 0.00/671k [00:00<?, ?it/s]Upload progress: 100%|██████████| 671k/671k [00:00<00:00, 1.33Git/s]
2023-01-31 06:11:29.250 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675141889,
  "events": [
    {
      "created_at": 1675141889,
      "level": "info",
      "message": "Created fine-tune: ft-kQhnIuhUASzxns5UFaYvmSH5",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-kQhnIuhUASzxns5UFaYvmSH5",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 670722,
      "created_at": 1675141888,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_061124/train.jsonl",
      "id": "file-tz6artJQHx2kgLAS8kUgt1fh",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675141889,
  "validation_files": []
}
2023-01-31 06:11:29.425 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:13:29.965 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:15:30.531 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:17:31.002 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:19:31.528 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:21:31.961 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:23:32.489 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 06:25:33.006 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 06:27:33.453 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 06:29:33.967 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 06:31:34.461 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 06:33:35.010 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 06:33:35.011 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_061124', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_061124/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-05-32-06', 'ft_id': 'ft-kQhnIuhUASzxns5UFaYvmSH5', 'date': '20230131_063335', 'train_file_id': 'file-tz6artJQHx2kgLAS8kUgt1fh', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_061124/train.jsonl: file-tz6artJQHx2kgLAS8kUgt1fh
Ran train size 500 and got MAE 2.22116, GPR baseline 1.7283103030403382
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ 1e-05   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 0       │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │ 0.99998 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/7.47k [00:00<?, ?it/s]Upload progress: 100%|██████████| 7.47k/7.47k [00:00<00:00, 11.8Mit/s]
2023-01-31 06:33:42.818 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675143222,
  "events": [
    {
      "created_at": 1675143222,
      "level": "info",
      "message": "Created fine-tune: ft-CCMVX0v8cTl5WAz2eCgmftFd",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-CCMVX0v8cTl5WAz2eCgmftFd",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 7468,
      "created_at": 1675143222,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_063341/train.jsonl",
      "id": "file-fX3wCXgsDJMprQjuy5aztF9s",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675143222,
  "validation_files": []
}
2023-01-31 06:33:42.996 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:35:43.545 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:37:44.005 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:39:44.495 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:41:44.994 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:43:45.514 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:45:45.981 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 06:47:46.534 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 06:47:46.534 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_063341', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_063341/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-05-46-24', 'ft_id': 'ft-CCMVX0v8cTl5WAz2eCgmftFd', 'date': '20230131_064746', 'train_file_id': 'file-fX3wCXgsDJMprQjuy5aztF9s', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_063341/train.jsonl: file-fX3wCXgsDJMprQjuy5aztF9s
Ran train size 10 and got MAE 3.0194399999999995, GPR baseline 2.091499877331547
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/36.2k [00:00<?, ?it/s]Upload progress: 100%|██████████| 36.2k/36.2k [00:00<00:00, 75.1Mit/s]
2023-01-31 06:47:54.366 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675144074,
  "events": [
    {
      "created_at": 1675144074,
      "level": "info",
      "message": "Created fine-tune: ft-vPGfeXUwFXfKUE4KPc8uVQoj",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-vPGfeXUwFXfKUE4KPc8uVQoj",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 36198,
      "created_at": 1675144074,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_064752/train.jsonl",
      "id": "file-jiAYYY8RNExJlnMqbQBFPwmP",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675144074,
  "validation_files": []
}
2023-01-31 06:47:54.531 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:49:55.038 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:51:55.607 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:53:56.111 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 06:55:56.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 06:57:57.212 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 06:57:57.212 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_064752', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_064752/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-05-55-58', 'ft_id': 'ft-vPGfeXUwFXfKUE4KPc8uVQoj', 'date': '20230131_065757', 'train_file_id': 'file-jiAYYY8RNExJlnMqbQBFPwmP', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_064752/train.jsonl: file-jiAYYY8RNExJlnMqbQBFPwmP
Ran train size 50 and got MAE 2.47204, GPR baseline 1.9859199984252607
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/72.4k [00:00<?, ?it/s]Upload progress: 100%|██████████| 72.4k/72.4k [00:00<00:00, 157Mit/s]
2023-01-31 06:58:05.184 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675144684,
  "events": [
    {
      "created_at": 1675144684,
      "level": "info",
      "message": "Created fine-tune: ft-IZKl7qbOJmr46X5VMamAUN1y",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-IZKl7qbOJmr46X5VMamAUN1y",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 72380,
      "created_at": 1675144684,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_065803/train.jsonl",
      "id": "file-YnNhSesqdWWpzMr6zecteoDz",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675144684,
  "validation_files": []
}
2023-01-31 06:58:05.358 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:00:05.882 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:02:06.366 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:04:06.873 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:06:07.361 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 07:06:07.361 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_065803', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_065803/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-06-06-05', 'ft_id': 'ft-IZKl7qbOJmr46X5VMamAUN1y', 'date': '20230131_070607', 'train_file_id': 'file-YnNhSesqdWWpzMr6zecteoDz', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_065803/train.jsonl: file-YnNhSesqdWWpzMr6zecteoDz
Ran train size 100 and got MAE 2.29296, GPR baseline 1.910187555252833
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.52274 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 76.7416  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.63396 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/145k [00:00<?, ?it/s]Upload progress: 100%|██████████| 145k/145k [00:00<00:00, 280Mit/s]
2023-01-31 07:06:16.287 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675145176,
  "events": [
    {
      "created_at": 1675145176,
      "level": "info",
      "message": "Created fine-tune: ft-HZksZLv1PXqO1c3VgmGUgQcl",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-HZksZLv1PXqO1c3VgmGUgQcl",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 144586,
      "created_at": 1675145175,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_070614/train.jsonl",
      "id": "file-5jsLT6NE1IROjNTeJqjGXg78",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675145176,
  "validation_files": []
}
2023-01-31 07:06:16.462 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:08:16.979 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:10:17.495 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:12:17.926 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:14:18.377 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:16:18.913 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:18:19.424 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:20:19.889 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 07:20:19.890 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_070614', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_070614/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-06-18-48', 'ft_id': 'ft-HZksZLv1PXqO1c3VgmGUgQcl', 'date': '20230131_072019', 'train_file_id': 'file-5jsLT6NE1IROjNTeJqjGXg78', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_070614/train.jsonl: file-5jsLT6NE1IROjNTeJqjGXg78
Ran train size 200 and got MAE 2.2542799999999996, GPR baseline 1.746607858633924
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │     value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  -3.05847 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 138.088   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   0.593   │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════╛
Upload progress:   0%|          | 0.00/362k [00:00<?, ?it/s]Upload progress: 100%|██████████| 362k/362k [00:00<00:00, 757Mit/s]
2023-01-31 07:20:31.438 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675146031,
  "events": [
    {
      "created_at": 1675146031,
      "level": "info",
      "message": "Created fine-tune: ft-GUtK0A1CBBOQzjJJCDxRFYG4",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-GUtK0A1CBBOQzjJJCDxRFYG4",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 361680,
      "created_at": 1675146031,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_072029/train.jsonl",
      "id": "file-acmxfN1k72FEOeGG4A7NsBL7",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675146031,
  "validation_files": []
}
2023-01-31 07:20:31.619 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:22:32.150 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:24:32.669 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:26:33.161 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:28:33.704 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:30:34.254 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:32:34.683 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:34:35.227 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:36:35.728 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:38:36.226 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:40:36.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:42:37.246 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:44:37.767 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:46:38.329 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:48:38.881 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:50:39.339 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:52:39.853 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 07:54:40.428 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 07:54:40.429 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_072029', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_072029/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-06-54-20', 'ft_id': 'ft-GUtK0A1CBBOQzjJJCDxRFYG4', 'date': '20230131_075440', 'train_file_id': 'file-acmxfN1k72FEOeGG4A7NsBL7', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_072029/train.jsonl: file-acmxfN1k72FEOeGG4A7NsBL7
Ran train size 500 and got MAE 2.1153999999999997, GPR baseline 1.7283103030403382
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -1e-05   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  0       │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  1.00003 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/3.12k [00:00<?, ?it/s]Upload progress: 100%|██████████| 3.12k/3.12k [00:00<00:00, 3.67Mit/s]
2023-01-31 07:54:47.902 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675148087,
  "events": [
    {
      "created_at": 1675148087,
      "level": "info",
      "message": "Created fine-tune: ft-SDFRZxdMvkwkMDxhaRAjOMXY",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-SDFRZxdMvkwkMDxhaRAjOMXY",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 3124,
      "created_at": 1675148087,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_075446/train.jsonl",
      "id": "file-AqSVZpgnNiWyn7O0ujHHr22p",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675148087,
  "validation_files": []
}
2023-01-31 07:54:48.080 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:56:48.629 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 07:58:49.129 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:00:49.686 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:02:50.251 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:04:50.725 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:06:51.188 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:08:51.737 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:10:52.307 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 08:10:52.307 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_075446', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_075446/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-07-10-20', 'ft_id': 'ft-SDFRZxdMvkwkMDxhaRAjOMXY', 'date': '20230131_081052', 'train_file_id': 'file-AqSVZpgnNiWyn7O0ujHHr22p', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_075446/train.jsonl: file-AqSVZpgnNiWyn7O0ujHHr22p
Ran train size 10 and got MAE 2.32444, GPR baseline 2.1152533738828447
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/18.4k [00:00<?, ?it/s]Upload progress: 100%|██████████| 18.4k/18.4k [00:00<00:00, 37.2Mit/s]
2023-01-31 08:10:59.761 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675149059,
  "events": [
    {
      "created_at": 1675149059,
      "level": "info",
      "message": "Created fine-tune: ft-lvhUvObBUKlxlCJWMna1Dxi0",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-lvhUvObBUKlxlCJWMna1Dxi0",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 18418,
      "created_at": 1675149059,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_081058/train.jsonl",
      "id": "file-LWyz4g872q9jHBSqFyIvRobR",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675149059,
  "validation_files": []
}
2023-01-31 08:10:59.936 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:13:00.952 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:15:01.412 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 08:17:01.923 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 08:17:01.924 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_081058', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_081058/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-07-16-09', 'ft_id': 'ft-lvhUvObBUKlxlCJWMna1Dxi0', 'date': '20230131_081701', 'train_file_id': 'file-LWyz4g872q9jHBSqFyIvRobR', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_081058/train.jsonl: file-LWyz4g872q9jHBSqFyIvRobR
Ran train size 50 and got MAE 2.10468, GPR baseline 1.9669827946305245
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.41449 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  0.59011 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.92288 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/36.3k [00:00<?, ?it/s]Upload progress: 100%|██████████| 36.3k/36.3k [00:00<00:00, 76.0Mit/s]
2023-01-31 08:17:09.657 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675149429,
  "events": [
    {
      "created_at": 1675149429,
      "level": "info",
      "message": "Created fine-tune: ft-zTMpzlYoEXq1qWjWkdV0Oudx",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-zTMpzlYoEXq1qWjWkdV0Oudx",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 36276,
      "created_at": 1675149429,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_081708/train.jsonl",
      "id": "file-KVwBAo3neLcCJvZEN0y4voUT",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675149429,
  "validation_files": []
}
2023-01-31 08:17:09.829 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:19:10.372 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:21:10.919 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:23:11.403 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 08:25:11.919 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 08:27:12.484 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 08:27:12.484 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_081708', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_081708/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-07-25-30', 'ft_id': 'ft-zTMpzlYoEXq1qWjWkdV0Oudx', 'date': '20230131_082712', 'train_file_id': 'file-KVwBAo3neLcCJvZEN0y4voUT', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_081708/train.jsonl: file-KVwBAo3neLcCJvZEN0y4voUT
Ran train size 100 and got MAE 2.1707599999999996, GPR baseline 1.9898550848837868
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.33711 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 40.3554  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.74177 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/73.2k [00:00<?, ?it/s]Upload progress: 100%|██████████| 73.2k/73.2k [00:00<00:00, 163Mit/s]
2023-01-31 08:27:21.348 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675150041,
  "events": [
    {
      "created_at": 1675150041,
      "level": "info",
      "message": "Created fine-tune: ft-gMdIIU6k48WmgIV21TxYZVQT",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-gMdIIU6k48WmgIV21TxYZVQT",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 73154,
      "created_at": 1675150041,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_082719/train.jsonl",
      "id": "file-TKJAKg5j2ExUmCXEVH3RBdOh",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675150041,
  "validation_files": []
}
2023-01-31 08:27:21.521 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:29:22.072 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:31:22.633 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:33:23.096 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:35:23.607 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 08:37:24.064 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 08:39:24.607 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 08:39:24.607 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_082719', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_082719/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-07-38-55', 'ft_id': 'ft-gMdIIU6k48WmgIV21TxYZVQT', 'date': '20230131_083924', 'train_file_id': 'file-TKJAKg5j2ExUmCXEVH3RBdOh', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_082719/train.jsonl: file-TKJAKg5j2ExUmCXEVH3RBdOh
Ran train size 200 and got MAE 2.18988, GPR baseline 1.8006358875598976
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │     value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  -1.55906 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 223.893   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   0.49773 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════╛
Upload progress:   0%|          | 0.00/186k [00:00<?, ?it/s]Upload progress: 100%|██████████| 186k/186k [00:00<00:00, 367Mit/s]
2023-01-31 08:39:36.012 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675150775,
  "events": [
    {
      "created_at": 1675150775,
      "level": "info",
      "message": "Created fine-tune: ft-HyvbiIDFXHR90byKxIgEidSW",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-HyvbiIDFXHR90byKxIgEidSW",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 185502,
      "created_at": 1675150775,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_083933/train.jsonl",
      "id": "file-ea8c7CLPyCjNG6PxK7Gibxtw",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675150775,
  "validation_files": []
}
2023-01-31 08:39:36.181 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:41:36.737 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:43:37.248 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:45:38.888 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:47:39.415 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:49:39.935 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:51:40.438 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:53:40.956 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:55:41.488 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:57:42.028 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 08:59:42.561 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:01:43.081 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 09:03:43.609 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 09:05:44.160 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 09:07:44.623 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 09:09:45.162 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 09:11:45.662 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 09:11:45.673 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_083933', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_083933/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-08-10-42', 'ft_id': 'ft-HyvbiIDFXHR90byKxIgEidSW', 'date': '20230131_091145', 'train_file_id': 'file-ea8c7CLPyCjNG6PxK7Gibxtw', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_083933/train.jsonl: file-ea8c7CLPyCjNG6PxK7Gibxtw
Ran train size 500 and got MAE 1.9177199999999999, GPR baseline 1.5955436161904135
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -1e-05   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  0       │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  1.00003 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/10.7k [00:00<?, ?it/s]Upload progress: 100%|██████████| 10.7k/10.7k [00:00<00:00, 10.6Mit/s]
2023-01-31 09:11:54.308 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675152714,
  "events": [
    {
      "created_at": 1675152714,
      "level": "info",
      "message": "Created fine-tune: ft-DJ5k6sDc5STh1AtzIGHXsPFy",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-DJ5k6sDc5STh1AtzIGHXsPFy",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 10658,
      "created_at": 1675152713,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_091152/train.jsonl",
      "id": "file-F4342vJZkuQbPHxIvNSLgcIR",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675152714,
  "validation_files": []
}
2023-01-31 09:11:54.490 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:13:54.957 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:15:55.472 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:17:55.983 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:19:56.498 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:21:56.955 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:23:57.480 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:25:57.990 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:27:58.509 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:29:59.034 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:31:59.603 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:34:00.151 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:36:00.634 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:38:01.179 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:40:01.625 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:42:02.156 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:44:02.709 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:46:03.224 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:48:03.686 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:50:04.188 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:52:04.728 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:54:05.222 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:56:05.708 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 09:58:06.210 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:00:06.767 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:02:07.280 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:04:07.823 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:06:08.293 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:08:08.786 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:10:09.209 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:12:09.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:14:10.201 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:16:10.750 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:18:11.274 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:20:11.826 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:22:12.390 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:24:12.900 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:26:13.432 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:28:13.978 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:30:14.435 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:32:14.958 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:34:15.481 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:36:16.039 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:38:16.577 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:40:17.125 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:42:17.710 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:44:18.269 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:46:18.765 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:48:19.279 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 10:48:19.279 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_091152', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_091152/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-09-47-43', 'ft_id': 'ft-DJ5k6sDc5STh1AtzIGHXsPFy', 'date': '20230131_104819', 'train_file_id': 'file-F4342vJZkuQbPHxIvNSLgcIR', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_091152/train.jsonl: file-F4342vJZkuQbPHxIvNSLgcIR
Ran train size 10 and got MAE nan, GPR baseline 2.1152533738828447
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/67.1k [00:00<?, ?it/s]Upload progress: 100%|██████████| 67.1k/67.1k [00:00<00:00, 109Mit/s]
2023-01-31 10:48:27.740 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675158507,
  "events": [
    {
      "created_at": 1675158507,
      "level": "info",
      "message": "Created fine-tune: ft-Uxyan9aM8uSlcjwdNQNkGJ9l",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-Uxyan9aM8uSlcjwdNQNkGJ9l",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 67076,
      "created_at": 1675158507,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_104825/train.jsonl",
      "id": "file-0sf6FnlsKW6wGMJz6nuksLvh",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675158507,
  "validation_files": []
}
2023-01-31 10:48:27.912 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:50:28.416 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:52:28.891 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:54:29.417 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:56:29.978 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 10:58:30.491 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:00:30.977 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:02:31.474 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:04:31.937 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:06:32.409 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:08:32.966 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:10:33.476 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:12:34.037 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:14:34.547 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:16:35.071 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:18:35.590 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:20:36.106 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:22:36.631 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:24:37.142 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:26:37.692 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:28:38.187 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:30:38.732 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:32:39.201 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:34:39.735 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:36:40.266 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:38:40.809 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:40:41.342 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:42:41.861 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:44:42.437 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:46:42.951 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:48:43.429 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:50:43.957 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:52:44.507 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:54:44.937 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:56:45.476 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 11:58:45.933 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:00:46.405 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:02:46.887 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:04:47.394 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:06:47.883 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:08:48.377 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 12:10:48.928 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 12:10:48.929 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_104825', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_104825/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-11-09-05', 'ft_id': 'ft-Uxyan9aM8uSlcjwdNQNkGJ9l', 'date': '20230131_121048', 'train_file_id': 'file-0sf6FnlsKW6wGMJz6nuksLvh', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_104825/train.jsonl: file-0sf6FnlsKW6wGMJz6nuksLvh
Ran train size 50 and got MAE 2.16328, GPR baseline 1.9669827946305245
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.41449 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  0.59011 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.92288 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/130k [00:00<?, ?it/s]Upload progress: 100%|██████████| 130k/130k [00:00<00:00, 248Mit/s]
2023-01-31 12:10:57.873 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675163457,
  "events": [
    {
      "created_at": 1675163457,
      "level": "info",
      "message": "Created fine-tune: ft-2mr7yh5bcNG8pNxL65DrBXbY",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-2mr7yh5bcNG8pNxL65DrBXbY",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 130362,
      "created_at": 1675163457,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_121056/train.jsonl",
      "id": "file-KnM7AwvDZo8alwz2RTImDWwe",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675163457,
  "validation_files": []
}
2023-01-31 12:10:58.054 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:12:58.575 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:14:59.072 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:16:59.605 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:19:00.138 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:21:00.684 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:23:01.188 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:25:01.644 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:27:02.188 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:29:02.698 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:31:03.228 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:33:03.701 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:35:04.232 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:37:04.745 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:39:05.279 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:41:05.791 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:43:06.258 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:45:06.807 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:47:07.292 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:49:07.790 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:51:08.319 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:53:08.855 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:55:09.394 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:57:09.940 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 12:59:10.462 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:01:10.980 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:03:11.407 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:05:11.915 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:07:12.464 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:09:13.179 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:11:13.631 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:13:14.158 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:15:14.718 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:17:15.266 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:19:15.805 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:21:16.351 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:23:16.841 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:25:17.338 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:27:17.895 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:29:18.446 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:31:18.975 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:33:19.533 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:35:20.085 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:37:20.613 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:39:21.162 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:41:21.700 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:43:22.347 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:45:23.520 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:47:24.064 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:49:24.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:51:25.107 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:53:25.600 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:55:26.071 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:57:26.602 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 13:59:27.146 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:01:27.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:03:28.221 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:05:28.757 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:07:29.277 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:09:29.841 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:11:30.358 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:13:30.829 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:15:31.309 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:17:31.861 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:19:32.425 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:21:32.979 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:23:33.493 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:25:34.055 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:27:34.509 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:29:34.989 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:31:35.552 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:33:36.134 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:35:36.684 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:37:37.165 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:39:37.736 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:41:38.256 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:43:38.818 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:45:39.353 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:47:39.895 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:49:40.454 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:51:40.961 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:53:41.453 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:55:41.919 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:57:42.447 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 14:59:42.949 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:01:43.482 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:03:43.960 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:05:44.449 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:07:44.917 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:09:45.468 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:11:45.950 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:13:46.448 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:15:46.978 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:17:47.516 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:19:48.056 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:21:48.600 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 15:23:49.061 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 15:23:49.061 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_121056', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_121056/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-14-22-50', 'ft_id': 'ft-2mr7yh5bcNG8pNxL65DrBXbY', 'date': '20230131_152349', 'train_file_id': 'file-KnM7AwvDZo8alwz2RTImDWwe', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_121056/train.jsonl: file-KnM7AwvDZo8alwz2RTImDWwe
Ran train size 100 and got MAE 2.3174, GPR baseline 1.9898550848837868
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.33711 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 40.3554  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.74177 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/262k [00:00<?, ?it/s]Upload progress: 100%|██████████| 262k/262k [00:00<00:00, 494Mit/s]
2023-01-31 15:23:59.423 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675175039,
  "events": [
    {
      "created_at": 1675175039,
      "level": "info",
      "message": "Created fine-tune: ft-p8XhP6JR9DT6xgNN6zdH5aFN",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-p8XhP6JR9DT6xgNN6zdH5aFN",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 261954,
      "created_at": 1675175039,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_152357/train.jsonl",
      "id": "file-9zSXOj4uqsBlINaBEJ2rygHx",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675175039,
  "validation_files": []
}
2023-01-31 15:23:59.596 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:26:00.143 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:28:00.634 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:30:01.166 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:32:01.722 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:34:02.269 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:36:02.805 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:38:03.340 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:40:03.787 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:42:04.303 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:44:04.771 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:46:05.226 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:48:05.753 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:50:06.263 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:52:06.815 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:54:07.351 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:56:07.879 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 15:58:08.423 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:00:08.965 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:02:09.493 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:04:09.982 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:06:10.526 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:08:11.289 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:10:11.819 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:12:12.349 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:14:12.876 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 16:16:13.419 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 16:18:13.952 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 16:18:13.952 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_152357', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_152357/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-15-17-13', 'ft_id': 'ft-p8XhP6JR9DT6xgNN6zdH5aFN', 'date': '20230131_161813', 'train_file_id': 'file-9zSXOj4uqsBlINaBEJ2rygHx', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_152357/train.jsonl: file-9zSXOj4uqsBlINaBEJ2rygHx
Ran train size 200 and got MAE 2.23704, GPR baseline 1.8006358875598976
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │     value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  -1.55906 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 223.893   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   0.49773 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════╛
Upload progress:   0%|          | 0.00/665k [00:00<?, ?it/s]Upload progress: 100%|██████████| 665k/665k [00:00<00:00, 1.20Git/s]
2023-01-31 16:18:27.173 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675178307,
  "events": [
    {
      "created_at": 1675178307,
      "level": "info",
      "message": "Created fine-tune: ft-LRUdqEQN7JXoip0FupErtNZO",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-LRUdqEQN7JXoip0FupErtNZO",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 665110,
      "created_at": 1675178306,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_161824/train.jsonl",
      "id": "file-9aFev7SWIlUdOukNINtuDyzs",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675178307,
  "validation_files": []
}
2023-01-31 16:18:27.349 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:20:27.849 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:22:28.386 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:24:28.883 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:26:29.401 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:28:29.925 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:30:30.423 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:32:30.973 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:34:31.534 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:36:32.074 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:38:32.620 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:40:33.176 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:42:33.673 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:44:34.176 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:46:34.647 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:48:35.107 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:50:35.567 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:52:36.104 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:54:36.604 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:56:37.146 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 16:58:37.641 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:00:38.173 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:02:38.684 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:04:39.226 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:06:39.726 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:08:40.287 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:10:40.808 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:12:41.273 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:14:41.808 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:16:42.344 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:18:42.883 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:20:43.443 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:22:44.010 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:24:44.537 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:26:45.107 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:28:45.625 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:30:46.171 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:32:46.690 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:34:47.181 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:36:47.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:38:48.086 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:40:48.541 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:42:49.061 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-01-31 17:44:49.604 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 17:44:49.605 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_161824', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_161824/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-16-43-49', 'ft_id': 'ft-LRUdqEQN7JXoip0FupErtNZO', 'date': '20230131_174449', 'train_file_id': 'file-9aFev7SWIlUdOukNINtuDyzs', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_161824/train.jsonl: file-9aFev7SWIlUdOukNINtuDyzs
Ran train size 500 and got MAE 2.17688, GPR baseline 1.5955436161904135
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -1e-05   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  0       │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  1.00003 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/6.17k [00:00<?, ?it/s]Upload progress: 100%|██████████| 6.17k/6.17k [00:00<00:00, 9.33Mit/s]
2023-01-31 17:44:58.314 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675183498,
  "events": [
    {
      "created_at": 1675183498,
      "level": "info",
      "message": "Created fine-tune: ft-EsXojURNCjaa0WYts2iZnNPS",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-EsXojURNCjaa0WYts2iZnNPS",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 6170,
      "created_at": 1675183497,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_174457/train.jsonl",
      "id": "file-tNFgQ1yFRbrKCU1eFkmedh15",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675183498,
  "validation_files": []
}
2023-01-31 17:44:58.482 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:46:59.033 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:48:59.592 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:51:00.130 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:53:00.625 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:55:01.105 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:57:01.679 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 17:59:02.232 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:01:02.767 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:03:03.266 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:05:03.798 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:07:04.333 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:09:04.877 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:11:05.384 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:13:05.925 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:15:06.484 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:17:07.063 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:19:07.545 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:21:08.095 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:23:08.637 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:25:09.170 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:27:09.710 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:29:10.265 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:31:10.801 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:33:11.301 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:35:11.814 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:37:12.292 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:39:12.831 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:41:13.294 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:43:13.772 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:45:14.305 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:47:14.770 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:49:15.318 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:51:15.865 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:53:16.374 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:55:16.925 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:57:17.501 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 18:59:17.989 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:01:18.501 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:03:19.036 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:05:19.588 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:07:20.049 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:09:20.613 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:11:21.199 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:13:21.744 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:15:22.209 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:17:22.785 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:19:23.322 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:21:23.883 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:23:24.461 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:25:24.974 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:27:25.519 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:29:26.059 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:31:26.552 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:33:27.069 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:35:27.637 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:37:28.184 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:39:28.731 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:41:29.257 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:43:29.834 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:45:30.370 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:47:30.901 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:49:31.428 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:51:31.982 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:53:32.523 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:55:33.096 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:57:33.651 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 19:59:34.192 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:01:34.753 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:03:35.261 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:05:35.761 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:07:36.305 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:09:36.828 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:11:37.370 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:13:37.859 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:15:38.298 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:17:38.757 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:19:39.281 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:21:39.777 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:23:40.310 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:25:40.836 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:27:41.365 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:29:41.800 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:31:42.274 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:33:42.756 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:35:43.250 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:37:43.794 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:39:44.287 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:41:44.815 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:43:45.282 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:45:45.805 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:47:46.258 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:49:46.736 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:51:47.211 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:53:47.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:55:48.211 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:57:48.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 20:59:49.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:01:49.729 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:03:50.259 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:05:50.769 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:07:51.298 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:09:51.773 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:11:52.295 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:13:52.826 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:15:53.307 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:17:53.864 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:19:54.388 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 21:19:54.388 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_174457', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_174457/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-20-18-58', 'ft_id': 'ft-EsXojURNCjaa0WYts2iZnNPS', 'date': '20230131_211954', 'train_file_id': 'file-tNFgQ1yFRbrKCU1eFkmedh15', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_174457/train.jsonl: file-tNFgQ1yFRbrKCU1eFkmedh15
Ran train size 10 and got MAE 2.1617599999999997, GPR baseline 2.1152533738828447
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/36.3k [00:00<?, ?it/s]Upload progress: 100%|██████████| 36.3k/36.3k [00:00<00:00, 72.3Mit/s]
2023-01-31 21:20:02.713 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675196402,
  "events": [
    {
      "created_at": 1675196402,
      "level": "info",
      "message": "Created fine-tune: ft-RXesTBZwkX2KWiR0hmFRxA3u",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-RXesTBZwkX2KWiR0hmFRxA3u",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 36262,
      "created_at": 1675196402,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_212001/train.jsonl",
      "id": "file-k5TkVPVkzOyjBoeGdZ0G22gL",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675196402,
  "validation_files": []
}
2023-01-31 21:20:02.885 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:22:03.349 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:24:03.869 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:26:04.380 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:28:04.897 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:30:05.372 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:32:05.935 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:34:06.413 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:36:06.936 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:38:07.478 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:40:07.985 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:42:08.521 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:44:09.057 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:46:09.586 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:48:10.131 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:50:10.638 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:52:11.116 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:54:11.658 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:56:12.178 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 21:58:12.701 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:00:13.219 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:02:13.745 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:04:14.179 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:06:14.699 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:08:15.186 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:10:15.700 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:12:16.163 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:14:16.639 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:16:17.166 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:18:17.624 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:20:18.077 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:22:18.595 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:24:19.154 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:26:19.705 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:28:20.253 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:30:20.786 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:32:21.343 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:34:21.843 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:36:22.421 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:38:22.972 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:40:23.461 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:42:23.985 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:44:24.489 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:46:25.021 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:48:25.565 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-01-31 22:48:25.566 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_212001', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_212001/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-21-48-00', 'ft_id': 'ft-RXesTBZwkX2KWiR0hmFRxA3u', 'date': '20230131_224825', 'train_file_id': 'file-k5TkVPVkzOyjBoeGdZ0G22gL', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_212001/train.jsonl: file-k5TkVPVkzOyjBoeGdZ0G22gL
Ran train size 50 and got MAE 2.10944, GPR baseline 1.9669827946305245
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.41449 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  0.59011 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.92288 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/69.8k [00:00<?, ?it/s]Upload progress: 100%|██████████| 69.8k/69.8k [00:00<00:00, 96.8Mit/s]
2023-01-31 22:48:34.119 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675201714,
  "events": [
    {
      "created_at": 1675201714,
      "level": "info",
      "message": "Created fine-tune: ft-Fsd3RynW1kaQjdErtDyK1nx7",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-Fsd3RynW1kaQjdErtDyK1nx7",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 69772,
      "created_at": 1675201713,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_224832/train.jsonl",
      "id": "file-yHklT5DzWH9wNflVsiY6KL3w",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675201714,
  "validation_files": []
}
2023-01-31 22:48:34.291 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:50:34.801 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:52:35.353 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:54:35.820 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:56:36.356 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 22:58:36.896 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:00:37.373 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:02:37.836 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:04:38.362 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:06:38.871 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:08:39.407 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:10:39.949 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:12:40.506 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:14:41.035 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:16:41.564 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:18:42.095 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:20:42.669 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:22:43.244 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:24:43.784 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:26:44.344 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:28:44.872 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:30:45.354 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:32:45.879 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:34:46.385 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:36:46.862 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:38:47.414 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:40:47.853 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:42:48.378 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:44:48.823 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:46:49.304 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:48:49.832 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:50:50.358 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:52:50.893 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:54:51.397 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:56:51.916 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-01-31 23:58:52.428 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:00:52.903 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:02:53.359 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:04:53.884 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:06:54.412 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:08:54.929 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:10:55.484 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:12:56.032 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:14:56.554 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:16:57.043 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:18:57.525 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:20:58.050 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:22:58.502 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:24:59.021 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:26:59.453 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:28:59.924 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:31:00.457 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:33:00.953 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:35:01.436 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:37:01.936 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:39:02.372 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:41:02.897 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:43:03.522 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:45:03.997 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:47:04.474 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:49:05.001 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:51:05.490 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:53:05.950 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:55:06.477 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:57:06.998 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 00:59:07.517 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:01:08.026 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:03:08.534 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:05:09.046 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:07:09.558 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:09:10.091 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:11:10.633 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:13:11.071 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:15:11.555 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:17:12.073 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:19:12.577 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:21:13.013 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:23:13.479 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 01:25:13.931 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 01:25:13.931 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_224832', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_224832/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-00-24-53', 'ft_id': 'ft-Fsd3RynW1kaQjdErtDyK1nx7', 'date': '20230201_012513', 'train_file_id': 'file-yHklT5DzWH9wNflVsiY6KL3w', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230131_224832/train.jsonl: file-yHklT5DzWH9wNflVsiY6KL3w
Ran train size 100 and got MAE 2.2664400000000002, GPR baseline 1.9898550848837868
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0.33711 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 40.3554  │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.74177 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/141k [00:00<?, ?it/s]Upload progress: 100%|██████████| 141k/141k [00:00<00:00, 348Mit/s]
2023-02-01 01:25:23.092 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675211122,
  "events": [
    {
      "created_at": 1675211122,
      "level": "info",
      "message": "Created fine-tune: ft-RljkqNy1CradrqJFdpdQeUZb",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-RljkqNy1CradrqJFdpdQeUZb",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 141454,
      "created_at": 1675211122,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_012520/train.jsonl",
      "id": "file-pIrQRnrklyOesgVpPsRRYt6f",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675211122,
  "validation_files": []
}
2023-02-01 01:25:23.266 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:27:23.755 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:29:24.296 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:31:24.743 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:33:25.267 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:35:25.802 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:37:26.259 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:39:26.715 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:41:27.219 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:43:27.749 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:45:28.236 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:47:28.751 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:49:29.226 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:51:29.662 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:53:30.180 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:55:30.727 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:57:31.164 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 01:59:31.693 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:01:32.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:03:32.658 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:05:33.189 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:07:33.679 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:09:34.206 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:11:34.663 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:13:35.214 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:15:35.754 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:17:36.240 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:19:36.765 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:21:37.270 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:23:37.746 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:25:38.232 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:27:38.703 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:29:39.242 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:31:39.691 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:33:40.214 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:35:40.761 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:37:41.217 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:39:41.701 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:41:42.218 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:43:42.733 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:45:43.223 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:47:43.771 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:49:44.246 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:51:44.781 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:53:45.318 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:55:45.773 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:57:46.239 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 02:59:46.767 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:01:47.248 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:03:47.755 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:05:48.295 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:07:48.781 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:09:49.220 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:11:49.734 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:13:50.269 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:15:50.697 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:17:51.193 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:19:51.714 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:21:52.250 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:23:52.757 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:25:53.266 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:27:53.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:29:54.130 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:31:54.638 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:33:55.159 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:35:55.681 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:37:56.188 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:39:56.688 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:41:57.225 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:43:57.710 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:45:58.150 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:47:58.667 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:49:59.241 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:51:59.751 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:54:00.256 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:56:00.795 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 03:58:01.294 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:00:01.805 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:02:02.342 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:04:02.814 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:06:03.338 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:08:03.876 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:10:04.401 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:12:04.858 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:14:05.403 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:16:05.932 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:18:06.455 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:20:06.991 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:22:07.535 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:24:08.030 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:26:08.560 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:28:09.106 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:30:09.629 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:32:10.147 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:34:10.705 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:36:11.185 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:38:11.717 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:40:12.217 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:42:12.720 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:44:13.171 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:46:13.697 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:48:14.239 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:50:14.733 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:52:15.253 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:54:15.767 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:56:16.231 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 04:58:16.753 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:00:17.290 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:02:17.815 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:04:18.293 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:06:18.733 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:08:19.260 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:10:19.748 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:12:20.264 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:14:20.758 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:16:21.293 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:18:21.827 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:20:22.360 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 05:22:22.902 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 05:24:23.473 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 05:24:23.473 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_012520', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_012520/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-04-23-03', 'ft_id': 'ft-RljkqNy1CradrqJFdpdQeUZb', 'date': '20230201_052423', 'train_file_id': 'file-pIrQRnrklyOesgVpPsRRYt6f', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_012520/train.jsonl: file-pIrQRnrklyOesgVpPsRRYt6f
Ran train size 200 and got MAE 2.0548, GPR baseline 1.8006358875598976
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │     value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  -1.55906 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 223.893   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   0.49773 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════╛
Upload progress:   0%|          | 0.00/358k [00:00<?, ?it/s]Upload progress: 100%|██████████| 358k/358k [00:00<00:00, 720Mit/s]
2023-02-01 05:24:36.101 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675225475,
  "events": [
    {
      "created_at": 1675225476,
      "level": "info",
      "message": "Created fine-tune: ft-n1JWQ3QDk0G5huVcVfKCEYvB",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-n1JWQ3QDk0G5huVcVfKCEYvB",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 358162,
      "created_at": 1675225475,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_052433/train.jsonl",
      "id": "file-AKj5eVRxuiojZhy0lH13xJSz",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675225476,
  "validation_files": []
}
2023-02-01 05:24:36.279 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:26:36.805 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:28:37.315 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:30:37.840 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:32:38.382 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:34:38.916 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:36:39.441 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:38:39.966 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:40:40.447 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:42:40.995 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:44:41.506 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:46:42.044 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 05:48:42.587 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 05:50:43.135 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 05:52:43.597 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 05:54:44.237 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 05:56:44.777 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 05:58:45.264 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 05:58:45.265 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_052433', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_052433/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-04-57-15', 'ft_id': 'ft-n1JWQ3QDk0G5huVcVfKCEYvB', 'date': '20230201_055845', 'train_file_id': 'file-AKj5eVRxuiojZhy0lH13xJSz', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_052433/train.jsonl: file-AKj5eVRxuiojZhy0lH13xJSz
Ran train size 500 and got MAE 2.03276, GPR baseline 1.5955436161904135
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/3.64k [00:00<?, ?it/s]Upload progress: 100%|██████████| 3.64k/3.64k [00:00<00:00, 6.94Mit/s]
2023-02-01 05:58:52.409 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675227532,
  "events": [
    {
      "created_at": 1675227532,
      "level": "info",
      "message": "Created fine-tune: ft-EE09PHld6miFKRmqFNkBBg6f",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-EE09PHld6miFKRmqFNkBBg6f",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 3636,
      "created_at": 1675227532,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_055851/train.jsonl",
      "id": "file-8zjPkIhnhYdEiXs8aVc9dI2U",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675227532,
  "validation_files": []
}
2023-02-01 05:58:52.572 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:00:53.120 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:02:53.551 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:04:54.073 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:06:54.626 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:08:55.147 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:10:55.667 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:12:56.169 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:14:56.726 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:16:57.261 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:18:57.730 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:20:58.277 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:22:58.808 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:24:59.313 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:26:59.819 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:29:00.265 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:31:00.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:33:01.159 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:35:01.713 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:37:02.250 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:39:02.808 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:41:03.365 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:43:03.901 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:45:04.371 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 06:47:04.910 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 06:47:04.911 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_055851', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_055851/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-05-45-44', 'ft_id': 'ft-EE09PHld6miFKRmqFNkBBg6f', 'date': '20230201_064704', 'train_file_id': 'file-8zjPkIhnhYdEiXs8aVc9dI2U', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_055851/train.jsonl: file-8zjPkIhnhYdEiXs8aVc9dI2U
Ran train size 10 and got MAE 2.6272399999999996, GPR baseline 1.951759989002057
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │    value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ -0       │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │  0       │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼──────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │  0.99996 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧══════════╛
Upload progress:   0%|          | 0.00/17.3k [00:00<?, ?it/s]Upload progress: 100%|██████████| 17.3k/17.3k [00:00<00:00, 29.2Mit/s]
2023-02-01 06:47:12.262 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675230432,
  "events": [
    {
      "created_at": 1675230432,
      "level": "info",
      "message": "Created fine-tune: ft-PxPRBnhi0Pi3Yfq1zBLcWlE9",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-PxPRBnhi0Pi3Yfq1zBLcWlE9",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 17296,
      "created_at": 1675230431,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_064710/train.jsonl",
      "id": "file-NGbvWjod0FVuLR6R4jOgGEq8",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675230432,
  "validation_files": []
}
2023-02-01 06:47:12.437 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:49:12.957 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:51:13.485 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:53:14.007 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:55:14.542 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:57:14.972 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 06:59:15.506 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:01:16.040 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:03:16.513 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:05:16.966 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:07:17.474 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:09:18.028 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:11:18.580 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:13:19.121 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:15:20.512 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:17:21.053 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:19:21.601 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:21:22.063 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:23:22.565 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:25:28.368 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:27:28.898 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:29:29.413 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:31:29.956 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:33:30.412 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:35:30.985 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:37:31.568 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:39:32.108 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:41:32.652 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:43:33.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:45:33.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:47:34.220 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:49:34.699 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:51:35.176 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:53:35.706 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:55:36.224 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:57:36.755 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 07:59:37.291 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:01:37.802 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:03:38.337 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:05:38.877 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:07:39.331 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:09:39.877 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:11:40.379 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:13:40.901 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:15:41.439 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:17:41.991 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:19:42.496 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:21:43.014 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:23:43.548 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:25:44.028 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:27:44.516 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:29:45.028 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:31:45.546 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:33:46.010 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:35:46.468 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:37:46.952 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:39:47.439 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:41:47.984 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:43:48.538 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:45:49.060 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:47:49.596 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:49:50.122 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:51:50.640 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:53:51.162 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:55:51.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:57:52.154 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 08:59:52.678 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:01:53.120 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:03:53.573 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:05:54.100 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:07:54.604 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:09:55.129 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:11:55.631 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:13:56.178 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:15:56.714 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:17:57.192 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:19:57.753 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:21:58.242 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:23:58.754 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 09:25:59.312 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 09:25:59.312 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_064710', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_064710/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-08-24-20', 'ft_id': 'ft-PxPRBnhi0Pi3Yfq1zBLcWlE9', 'date': '20230201_092559', 'train_file_id': 'file-NGbvWjod0FVuLR6R4jOgGEq8', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_064710/train.jsonl: file-NGbvWjod0FVuLR6R4jOgGEq8
Ran train size 50 and got MAE nan, GPR baseline 1.9133014359939917
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │      -0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │       0 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │       1 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Upload progress:   0%|          | 0.00/35.8k [00:00<?, ?it/s]Upload progress: 100%|██████████| 35.8k/35.8k [00:00<00:00, 72.9Mit/s]
2023-02-01 09:26:07.766 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675239967,
  "events": [
    {
      "created_at": 1675239967,
      "level": "info",
      "message": "Created fine-tune: ft-9BJjWQcuZKaOQTuVtZldziB5",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-9BJjWQcuZKaOQTuVtZldziB5",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 35848,
      "created_at": 1675239967,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_092606/train.jsonl",
      "id": "file-GJqFg5S86KynpJOW18b7Xlm6",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675239967,
  "validation_files": []
}
2023-02-01 09:26:07.942 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:28:08.462 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:30:08.983 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:32:09.504 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:34:10.013 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:36:10.565 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:38:11.050 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:40:11.564 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:42:12.109 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:44:12.532 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:46:13.097 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:48:13.594 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:50:14.136 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:52:14.615 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:54:15.150 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:56:15.669 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 09:58:16.202 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:00:16.730 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:02:17.180 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:04:17.723 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:06:18.267 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:08:18.787 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:10:19.346 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:12:19.885 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:14:20.427 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:16:20.951 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:18:21.473 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:20:22.023 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:22:22.571 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:24:23.090 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:26:23.623 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:28:24.082 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:30:24.628 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:32:25.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:34:25.720 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:36:26.247 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:38:26.746 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:40:27.243 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:42:27.764 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:44:28.313 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:46:28.764 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:48:29.308 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:50:29.873 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:52:30.424 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:54:30.975 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:56:31.419 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 10:58:31.940 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:00:32.418 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:02:32.913 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:04:33.437 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:06:33.867 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:08:34.367 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:10:34.872 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:12:35.421 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:14:35.950 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:16:36.499 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:18:36.955 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:20:37.441 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:22:37.884 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:24:38.396 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:26:38.919 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:28:39.465 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:30:39.996 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:32:40.507 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:34:41.021 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:36:41.523 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:38:41.994 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:40:42.450 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:42:42.976 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:44:43.457 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:46:43.930 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:48:44.427 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:50:44.956 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:52:45.470 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:54:45.993 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:56:46.530 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 11:58:46.996 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:00:47.509 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:02:48.045 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:04:48.598 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:06:49.133 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:08:49.676 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:10:50.216 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:12:50.762 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:14:51.264 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:16:51.786 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:18:52.383 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:20:52.807 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:22:53.348 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:24:53.863 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:26:54.382 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:28:54.879 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:30:55.410 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:32:55.911 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:34:56.456 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:36:56.959 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:38:57.401 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:40:57.865 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:42:58.323 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:44:58.858 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:46:59.370 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:48:59.852 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:51:00.302 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:53:00.820 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:55:01.344 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:57:01.855 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 12:59:02.327 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:01:02.877 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:03:03.364 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:05:03.845 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:07:04.371 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:09:04.887 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:11:05.410 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:13:05.920 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:15:06.447 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:17:06.981 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:19:07.510 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:21:08.008 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:23:08.531 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:25:09.049 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:27:09.575 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:29:10.104 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:31:10.603 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:33:11.136 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:35:11.647 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:37:12.174 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:39:12.691 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:41:13.206 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:43:13.729 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:45:14.288 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:47:14.819 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:49:15.290 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:51:15.807 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 13:53:16.348 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 13:53:16.349 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_092606', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_092606/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-12-51-44', 'ft_id': 'ft-9BJjWQcuZKaOQTuVtZldziB5', 'date': '20230201_135316', 'train_file_id': 'file-GJqFg5S86KynpJOW18b7Xlm6', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_092606/train.jsonl: file-GJqFg5S86KynpJOW18b7Xlm6
Ran train size 100 and got MAE 2.4428400000000003, GPR baseline 1.9733920226484745
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │     value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  -0.13906 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 107.678   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   0.62192 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════╛
Upload progress:   0%|          | 0.00/72.6k [00:00<?, ?it/s]Upload progress: 100%|██████████| 72.6k/72.6k [00:00<00:00, 114Mit/s]
2023-02-01 13:53:25.094 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675256004,
  "events": [
    {
      "created_at": 1675256004,
      "level": "info",
      "message": "Created fine-tune: ft-80nDQt26OwgmtlHTqEgNVL3v",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-80nDQt26OwgmtlHTqEgNVL3v",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 72600,
      "created_at": 1675256004,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_135323/train.jsonl",
      "id": "file-tXFYc9wkNsjFZzAwRkGNUpOL",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675256004,
  "validation_files": []
}
2023-02-01 13:53:25.264 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:55:26.452 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:57:26.978 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 13:59:27.527 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:01:28.066 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:03:28.592 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:05:36.764 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:07:37.205 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:09:37.780 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:11:38.331 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:13:38.873 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:15:43.491 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:17:43.986 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:19:44.447 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:21:44.977 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:23:45.508 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:25:46.035 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:27:46.561 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:29:47.106 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:31:47.650 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:33:48.172 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:35:48.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:37:49.227 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:39:49.761 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:41:50.316 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:43:50.869 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:45:51.402 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:47:51.961 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:49:52.500 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:51:53.033 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:53:53.567 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:55:54.106 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:57:54.542 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 14:59:54.984 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:01:55.488 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:03:55.984 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:05:56.516 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:07:57.050 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:09:57.594 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:11:58.145 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:13:58.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:15:59.111 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:17:59.614 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:20:00.158 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:22:00.698 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:24:01.194 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:26:01.702 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:28:02.204 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:30:02.638 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:32:03.158 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:34:03.672 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:36:04.217 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:38:04.671 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:40:05.201 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:42:05.717 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:44:06.244 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:46:06.756 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:48:07.227 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:50:07.774 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:52:08.212 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:54:08.701 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 15:56:09.195 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 15:58:09.742 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 16:00:10.255 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running
2023-02-01 16:02:10.746 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded
2023-02-01 16:02:10.746 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_135323', 'train_filename': '/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_135323/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-15-00-09', 'ft_id': 'ft-80nDQt26OwgmtlHTqEgNVL3v', 'date': '20230201_160210', 'train_file_id': 'file-tXFYc9wkNsjFZzAwRkGNUpOL', 'valid_file_id': None}
Uploaded file from /home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_135323/train.jsonl: file-tXFYc9wkNsjFZzAwRkGNUpOL
Ran train size 200 and got MAE 2.02516, GPR baseline 1.6846024690146193
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │     value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │  -1.23858 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 189.832   │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │   0.52282 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════╛
Upload progress:   0%|          | 0.00/183k [00:00<?, ?it/s]Upload progress: 100%|██████████| 183k/183k [00:00<00:00, 347Mit/s]
2023-02-01 16:02:21.777 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {
  "created_at": 1675263741,
  "events": [
    {
      "created_at": 1675263741,
      "level": "info",
      "message": "Created fine-tune: ft-9sbOrXDQqW7FGrigjjqjphp7",
      "object": "fine-tune-event"
    }
  ],
  "fine_tuned_model": null,
  "hyperparams": {
    "batch_size": null,
    "learning_rate_multiplier": 0.02,
    "n_epochs": 8,
    "prompt_loss_weight": 0.01
  },
  "id": "ft-9sbOrXDQqW7FGrigjjqjphp7",
  "model": "ada",
  "object": "fine-tune",
  "organization_id": "org-TFRJXw3PPQocOWbu71eI2t9U",
  "result_files": [],
  "status": "pending",
  "training_files": [
    {
      "bytes": 183258,
      "created_at": 1675263741,
      "filename": "/home/kevin/Documents/gptchem/experiments/04_regression/opv/out/20230201_160219/train.jsonl",
      "id": "file-n38zAm2ORZHBvWMp27t8KqzG",
      "object": "file",
      "purpose": "fine-tune",
      "status": "uploaded",
      "status_details": null
    }
  ],
  "updated_at": 1675263741,
  "validation_files": []
}
2023-02-01 16:02:21.948 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:04:22.468 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:06:22.954 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:08:23.473 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:10:24.002 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:12:24.451 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:14:24.949 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:16:25.435 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:18:25.938 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:20:26.368 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:22:26.895 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:24:27.390 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:26:27.905 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:28:28.420 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:30:28.858 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:32:29.342 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:34:29.768 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:36:30.289 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:38:30.824 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:40:31.324 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:42:31.824 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:44:32.314 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:46:32.831 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:48:33.354 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:50:33.805 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:52:34.329 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:54:34.819 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:56:35.287 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 16:58:35.778 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:00:36.304 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:02:36.835 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:04:37.365 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:06:37.846 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:08:38.394 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:10:38.926 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:12:39.404 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:14:39.878 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:16:40.401 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:18:40.900 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:20:41.421 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:22:41.914 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:24:42.396 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:26:42.935 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:28:43.469 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:30:44.029 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:32:44.564 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:34:44.997 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:36:45.483 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:38:46.013 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:40:46.553 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:42:47.051 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:44:47.564 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:46:48.111 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:48:48.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:50:49.140 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:52:49.581 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:54:50.117 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:56:50.561 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 17:58:51.064 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
2023-02-01 18:00:51.595 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending
