{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matbench.bench import MatbenchBenchmark\n",
    "from matbench.constants import CLF_KEY\n",
    "from gptchem.gpt_regressor import GPTRegressor\n",
    "from gptchem.tuner import Tuner\n",
    "\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "logger.enable(\"gptchem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decorator, time\n",
    "\n",
    "def retry(howmany, *exception_types, **kwargs):\n",
    "    timeout = kwargs.get('timeout', 0.0) # seconds\n",
    "    @decorator.decorator\n",
    "    def tryIt(func, *fargs, **fkwargs):\n",
    "        for _ in range(howmany):\n",
    "            try: return func(*fargs, **fkwargs)\n",
    "            except exception_types or Exception as e:\n",
    "                print(e)\n",
    "                if timeout is not None: time.sleep(timeout)\n",
    "    return tryIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-27 14:51:36 INFO     Initialized benchmark 'matbench_v0.1' with 2 tasks: \n",
      "['matbench_expt_gap', 'matbench_steels']\n"
     ]
    }
   ],
   "source": [
    "mb = MatbenchBenchmark(\n",
    "    autoload=True,\n",
    "    subset=[\n",
    "        \"matbench_expt_gap\",\n",
    "        \"matbench_steels\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(3, timeout=5)\n",
    "def train_test_fold(task, fold):\n",
    "    regressor = GPTRegressor(task.metadata['target'], Tuner(n_epochs=8, learning_rate_multiplier=0.02, wandb_sync=False))\n",
    "    train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "    # train and validate your model\n",
    "    regressor.fit(train_inputs, train_outputs.values)\n",
    "\n",
    "    # Get testing data\n",
    "    test_inputs = task.get_test_data(fold, include_target=False)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    # Your output should be a pandas series, numpy array, or python iterable\n",
    "    # where the array elements are floats or bools\n",
    "    predictions = regressor.predict(test_inputs)\n",
    "\n",
    "    # Record your data!\n",
    "    task.record(fold, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-27 14:57:39 INFO     Dataset matbench_expt_gap already loaded; not reloading dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 393k/393k [00:00<00:00, 329Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230127_145739/train.jsonl: file-ysZj5IeDuZBXSgW0Wtzaw44z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 14:57:42.364 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {\n",
      "  \"created_at\": 1674827862,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1674827862,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-7PRf4QcKSGT0yUWChuZAS7mZ\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-7PRf4QcKSGT0yUWChuZAS7mZ\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 392808,\n",
      "      \"created_at\": 1674827861,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230127_145739/train.jsonl\",\n",
      "      \"id\": \"file-ysZj5IeDuZBXSgW0Wtzaw44z\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1674827862,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-01-27 14:57:42.565 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 14:59:49.297 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 15:01:53.567 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 15:03:58.688 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 15:05:59.261 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:08:03.183 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:10:03.787 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:12:04.367 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:14:04.825 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:16:05.344 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:18:05.927 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:20:06.515 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:22:07.027 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:24:07.568 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:26:08.117 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:28:08.714 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:30:09.282 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:32:10.083 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:36:02.616 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:38:03.969 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:40:04.750 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:42:11.438 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:44:12.027 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:46:12.601 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:48:14.969 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:50:17.247 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:52:19.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 15:54:25.066 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-01-27 15:54:25.073 | DEBUG    | gptchem.tuner:tune:202 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230127_145739', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230127_145739/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-27-14-52-16', 'ft_id': 'ft-7PRf4QcKSGT0yUWChuZAS7mZ', 'date': '20230127_155425', 'train_file_id': 'file-ysZj5IeDuZBXSgW0Wtzaw44z', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal error {\n",
      "    \"error\": {\n",
      "        \"message\": \"internal error\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": null\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'internal error', 'type': 'invalid_request_error', 'param': None, 'code': None}} {'Date': 'Fri, 27 Jan 2023 14:54:34 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '147', 'Connection': 'keep-alive', 'Vary': 'Origin', 'X-Request-Id': 'd012d34a122aed56a8ee24feec563c94', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 393k/393k [00:00<00:00, 399Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230127_155439/train.jsonl: file-myBkZRBJPLAv3i83VtpyRFcG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 15:54:42.349 | DEBUG    | gptchem.tuner:tune:186 - Requested fine tuning. {\n",
      "  \"created_at\": 1674831282,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1674831282,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-O6Nxt2mjBm0wMNI7I93B9Rnx\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-O6Nxt2mjBm0wMNI7I93B9Rnx\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 392808,\n",
      "      \"created_at\": 1674831281,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230127_155439/train.jsonl\",\n",
      "      \"id\": \"file-myBkZRBJPLAv3i83VtpyRFcG\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1674831282,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-01-27 15:54:42.874 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 15:56:43.437 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 15:58:44.358 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:00:46.629 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:02:53.419 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:04:57.665 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:07:00.228 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:09:03.781 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:11:09.768 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:13:12.140 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:15:13.357 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:17:14.079 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:19:15.379 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:21:18.923 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-27 16:26:10.999 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:28:12.108 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:30:17.285 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:35:33.963 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:37:39.477 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:39:40.086 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:41:40.634 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:45:29.254 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:47:39.286 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:57:36.032 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 16:59:36.727 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 17:01:37.450 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 17:03:38.367 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 17:05:39.278 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-27 17:07:39.890 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n"
     ]
    }
   ],
   "source": [
    "for task in mb.tasks:\n",
    "    task.load()\n",
    "    \n",
    "    for fold_ind, fold in enumerate(task.folds):\n",
    "        if task.is_recorded[fold_ind]:\n",
    "            print(f\"Skipping fold {fold_ind} of {task.dataset_name}\")\n",
    "            continue\n",
    "        train_test_fold(task, fold)\n",
    "        train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(train_outputs)\n",
    "\n",
    "    print(f\"{task.dataset_name}: MAE  {task.scores['mae']['mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb.to_file(\"gpt_bench.json.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f3b9074e5baa1438c27e2ea813f7f53b7516c83bd70840b6d64eae6820ee5df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
