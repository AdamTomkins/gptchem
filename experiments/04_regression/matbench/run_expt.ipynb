{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from loguru import logger\n",
    "from matbench.bench import MatbenchBenchmark\n",
    "from matbench.constants import CLF_KEY\n",
    "\n",
    "from gptchem.gpt_regressor import GPTRegressor\n",
    "from gptchem.tuner import Tuner\n",
    "\n",
    "logger.enable(\"gptchem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import decorator\n",
    "\n",
    "\n",
    "def retry(howmany, *exception_types, **kwargs):\n",
    "    timeout = kwargs.get(\"timeout\", 0.0)  # seconds\n",
    "\n",
    "    @decorator.decorator\n",
    "    def tryIt(func, *fargs, **fkwargs):\n",
    "        for _ in range(howmany):\n",
    "            try:\n",
    "                return func(*fargs, **fkwargs)\n",
    "            except exception_types or Exception as e:\n",
    "                print(e)\n",
    "                if timeout is not None:\n",
    "                    time.sleep(timeout)\n",
    "\n",
    "    return tryIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 17:13:53 INFO     Initialized benchmark 'matbench_v0.1' with 1 tasks: \n",
      "['matbench_expt_gap']\n"
     ]
    }
   ],
   "source": [
    "mb = MatbenchBenchmark(\n",
    "    autoload=True,\n",
    "    subset=[\n",
    "          \"matbench_expt_gap\",\n",
    "        #\"matbench_steels\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(3, timeout=5)\n",
    "def train_test_fold(task, fold):\n",
    "    regressor = GPTRegressor(\n",
    "        task.metadata[\"target\"], Tuner(n_epochs=8, learning_rate_multiplier=0.02, wandb_sync=False)\n",
    "    )\n",
    "    train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "    # train and validate your model\n",
    "    regressor.fit(train_inputs, train_outputs.values)\n",
    "\n",
    "    # Get testing data\n",
    "    test_inputs = task.get_test_data(fold, include_target=False)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    # Your output should be a pandas series, numpy array, or python iterable\n",
    "    # where the array elements are floats or bools\n",
    "    predictions = regressor.predict(test_inputs)\n",
    "\n",
    "    # Record your data!\n",
    "    task.record(fold, predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 17:13:55 INFO     Dataset matbench_expt_gap already loaded; not reloading dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 707Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_171355/train.jsonl: file-u8iPFyFXJAp6Eo3zDKB3NlPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 17:13:58.017 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675181637,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675181637,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-LDIyknMulwVBINhl3kaxpTO2\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-LDIyknMulwVBINhl3kaxpTO2\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420861,\n",
      "      \"created_at\": 1675181637,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_171355/train.jsonl\",\n",
      "      \"id\": \"file-u8iPFyFXJAp6Eo3zDKB3NlPS\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675181637,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-01-31 17:13:58.241 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:15:58.847 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:17:59.434 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:20:00.023 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:22:00.632 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:24:01.530 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:26:02.218 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:28:02.812 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:30:03.548 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:32:04.044 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:34:04.742 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:36:06.011 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:38:06.601 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:40:07.193 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:42:07.868 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:44:08.390 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:46:08.980 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:48:09.584 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:50:10.195 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:52:11.233 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:54:11.793 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:56:12.338 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 17:58:12.929 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:00:13.520 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:02:14.115 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:04:14.881 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:06:15.341 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:08:15.950 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:10:16.550 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:12:17.140 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:14:17.731 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:16:18.325 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:18:19.092 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:20:19.702 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:22:20.248 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:24:20.868 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:37:18.505 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:39:19.264 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:41:20.134 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:43:20.827 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:45:21.383 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:47:22.009 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:49:22.686 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:51:23.420 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:53:24.076 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:55:24.791 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:57:25.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 18:59:28.139 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:01:28.986 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:03:29.784 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:05:32.225 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:07:33.164 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:09:33.783 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:11:34.630 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:39:25.143 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:41:25.724 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:43:26.608 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:45:27.281 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 19:47:27.864 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 20:09:47.555 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:11:48.156 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:13:48.727 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:15:49.302 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:17:49.894 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:19:50.489 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:21:51.077 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:23:52.012 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:25:52.596 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:27:53.180 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:29:56.003 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:31:56.629 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:33:57.417 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:35:58.132 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:37:58.777 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:39:59.298 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:42:00.409 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:44:02.805 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 20:46:04.719 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-01-31 20:46:04.729 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_171355', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_171355/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-19-44-04', 'ft_id': 'ft-LDIyknMulwVBINhl3kaxpTO2', 'date': '20230131_204604', 'train_file_id': 'file-u8iPFyFXJAp6Eo3zDKB3NlPS', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 20:48:06 INFO     Recorded fold matbench_expt_gap-0 successfully.\n",
      "Input contains NaN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 291Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_204811/train.jsonl: file-gGiKQ01RGBEBTLRdJGzVXr8n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 20:48:14.212 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675194493,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675194493,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-1QMg9mMIOKUov8RWbGvpVB2m\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-1QMg9mMIOKUov8RWbGvpVB2m\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420861,\n",
      "      \"created_at\": 1675194493,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_204811/train.jsonl\",\n",
      "      \"id\": \"file-gGiKQ01RGBEBTLRdJGzVXr8n\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675194493,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-01-31 20:48:14.737 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 20:50:15.579 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 20:52:23.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 20:54:24.354 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 20:56:24.937 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 20:58:25.575 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:00:26.156 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:02:26.686 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:04:27.794 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:06:28.375 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:08:28.959 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:10:29.546 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:12:30.124 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:14:31.427 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:16:31.996 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:18:33.709 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:20:35.284 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:22:36.592 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:24:37.499 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:26:38.717 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:28:40.286 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:30:41.370 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 21:32:41.927 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:34:43.264 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:36:43.984 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:38:44.629 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:40:46.403 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:42:47.001 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:44:47.578 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:46:48.176 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:48:48.747 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:50:49.342 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:52:49.936 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:54:50.682 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:56:51.443 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 21:58:52.033 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:00:53.341 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:02:53.935 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:04:54.398 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:06:55.265 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:08:56.022 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:10:56.615 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:12:57.200 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:14:57.782 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:16:58.373 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 22:18:58.957 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-01-31 22:18:58.961 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_204811', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_204811/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-21-17-41', 'ft_id': 'ft-1QMg9mMIOKUov8RWbGvpVB2m', 'date': '20230131_221858', 'train_file_id': 'file-gGiKQ01RGBEBTLRdJGzVXr8n', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 22:21:00 ERROR    Fold number 0 already recorded! Aborting record...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 591Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_222100/train.jsonl: file-GDHF9HN7lvegtMyJoo7vhjVv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 22:21:05.544 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675200064,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675200064,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-l06DTAyEDxMB62bJ1wt4TAco\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-l06DTAyEDxMB62bJ1wt4TAco\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420507,\n",
      "      \"created_at\": 1675200063,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_222100/train.jsonl\",\n",
      "      \"id\": \"file-GDHF9HN7lvegtMyJoo7vhjVv\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675200064,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-01-31 22:21:06.634 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:23:07.148 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:25:07.874 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:27:08.650 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:29:09.237 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:31:09.826 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:33:10.413 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:35:11.003 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:37:11.589 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:39:12.176 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:41:12.764 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:43:14.176 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:45:14.697 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:47:15.764 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:49:17.206 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:51:17.791 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:53:18.374 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:55:19.380 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:57:20.240 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 22:59:20.998 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:01:21.547 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:03:22.158 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:05:22.745 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:07:23.321 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:09:23.915 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:11:24.510 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:13:25.295 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:15:26.531 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:17:27.115 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:19:27.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:21:28.283 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:23:29.042 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:25:30.157 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:27:30.740 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:29:31.195 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:31:32.238 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:33:33.650 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:35:34.131 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:37:34.582 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:39:35.142 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:41:36.268 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:43:36.785 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:45:37.369 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:47:37.948 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:49:38.532 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:51:39.115 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:53:39.699 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:55:40.280 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:57:41.055 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 23:59:41.631 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 00:01:42.206 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 00:03:42.771 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 00:05:43.383 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 00:07:43.955 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 00:09:44.563 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 00:11:45.118 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-01 00:11:45.120 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_222100', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_222100/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-23-10-50', 'ft_id': 'ft-l06DTAyEDxMB62bJ1wt4TAco', 'date': '20230201_001145', 'train_file_id': 'file-GDHF9HN7lvegtMyJoo7vhjVv', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 00:13:45 INFO     Recorded fold matbench_expt_gap-1 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 420k/420k [00:00<00:00, 519Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_001345/train.jsonl: file-SHPDCAkVBtsc63Uhha7aVStb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 00:13:48.009 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675206827,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675206827,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-stVDZXQDo1ANG1bVm4fjdQln\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-stVDZXQDo1ANG1bVm4fjdQln\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420263,\n",
      "      \"created_at\": 1675206827,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_001345/train.jsonl\",\n",
      "      \"id\": \"file-SHPDCAkVBtsc63Uhha7aVStb\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675206827,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-01 00:13:48.185 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:15:48.725 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:17:49.306 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:19:49.930 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:21:50.375 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:23:51.580 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:25:52.162 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:27:52.743 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:29:53.328 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:31:53.909 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:33:54.491 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:35:55.719 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:37:56.247 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:39:56.968 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:41:57.517 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:43:58.029 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:45:58.531 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:47:59.054 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:49:59.673 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:52:00.247 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:54:00.854 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:56:01.905 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 00:58:02.486 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:00:03.069 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:02:03.649 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:04:04.231 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:06:04.783 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:08:05.403 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:10:06.059 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:12:07.927 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:14:09.156 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:16:09.933 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:18:10.513 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:20:11.095 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:22:11.678 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:24:12.261 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:26:12.844 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:28:13.426 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:30:14.406 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:32:15.115 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:34:15.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:36:16.281 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:38:16.863 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:40:17.447 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:42:18.027 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:44:18.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:46:19.718 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:48:20.300 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:50:20.891 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:52:21.472 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:54:22.051 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 01:56:22.635 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 01:58:23.216 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:00:23.800 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:02:24.906 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:04:25.556 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:06:26.070 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:08:26.697 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:10:27.145 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:12:27.817 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:14:29.496 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:16:30.032 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:18:30.678 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:20:31.198 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:22:31.780 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:24:32.364 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:26:32.946 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:28:33.721 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:30:34.193 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:32:34.763 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:34:35.612 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:36:36.159 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:38:36.638 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:40:37.196 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:42:37.660 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-01 02:42:37.665 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_001345', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_001345/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-01-41-55', 'ft_id': 'ft-stVDZXQDo1ANG1bVm4fjdQln', 'date': '20230201_024237', 'train_file_id': 'file-SHPDCAkVBtsc63Uhha7aVStb', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 02:44:37 INFO     Recorded fold matbench_expt_gap-2 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 828Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_024437/train.jsonl: file-C2UuHkea20ADasLEyR80DifU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 02:44:39.698 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675215879,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675215879,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-ZzBExjQTwtPS2uBjjHLKhG1k\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-ZzBExjQTwtPS2uBjjHLKhG1k\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420723,\n",
      "      \"created_at\": 1675215879,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_024437/train.jsonl\",\n",
      "      \"id\": \"file-C2UuHkea20ADasLEyR80DifU\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675215879,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-01 02:44:39.866 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 02:46:40.447 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 02:48:41.030 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 02:50:41.613 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 02:52:42.235 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 02:54:42.782 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:56:43.377 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 02:58:43.943 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:00:44.875 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:02:45.507 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:04:46.034 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:06:46.514 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:08:47.197 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:10:47.780 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:12:48.362 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:14:48.913 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:16:50.161 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:18:50.697 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:20:51.278 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:22:51.859 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:24:52.443 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:26:53.024 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:28:53.606 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:30:54.190 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:32:54.964 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:34:55.527 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:36:56.128 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:38:56.689 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 03:40:57.305 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-01 03:40:57.307 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_024437', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_024437/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-02-39-06', 'ft_id': 'ft-ZzBExjQTwtPS2uBjjHLKhG1k', 'date': '20230201_034057', 'train_file_id': 'file-C2UuHkea20ADasLEyR80DifU', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 03:42:57 INFO     Recorded fold matbench_expt_gap-3 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 815Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_034257/train.jsonl: file-zX4sUwoEfc7go1XjA2jcMfSO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 03:43:01.074 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675219381,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675219381,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-Q2Eq6pbQVX4AWeL1sJdGv7y6\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-Q2Eq6pbQVX4AWeL1sJdGv7y6\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420710,\n",
      "      \"created_at\": 1675219380,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_034257/train.jsonl\",\n",
      "      \"id\": \"file-zX4sUwoEfc7go1XjA2jcMfSO\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675219381,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-01 03:43:01.269 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 03:45:01.992 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 03:47:03.252 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 03:49:03.818 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 03:51:04.425 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 03:53:05.067 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 03:55:05.815 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 03:57:06.782 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 03:59:07.276 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:01:07.864 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:03:08.956 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:05:10.408 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:07:10.990 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:09:16.354 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:11:16.869 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:13:17.451 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:15:18.131 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:17:18.842 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:19:27.169 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:21:27.846 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:23:28.506 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 04:25:29.692 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:27:30.445 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:29:31.225 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:31:32.132 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:33:33.054 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:35:34.640 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:37:35.219 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:39:35.706 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:41:36.309 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:43:36.888 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:45:37.468 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:47:38.536 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:49:39.136 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:51:39.734 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:53:40.316 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:55:40.875 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:57:41.462 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 04:59:42.066 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 05:01:42.633 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 05:03:44.090 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 05:05:44.671 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 05:07:45.254 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 05:09:45.839 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 05:11:46.420 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-01 05:11:46.422 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_034257', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_034257/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-04-10-15', 'ft_id': 'ft-Q2Eq6pbQVX4AWeL1sJdGv7y6', 'date': '20230201_051146', 'train_file_id': 'file-zX4sUwoEfc7go1XjA2jcMfSO', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 05:12:54 INFO     Recorded fold matbench_expt_gap-4 successfully.\n"
     ]
    }
   ],
   "source": [
    "predictions = defaultdict(list)\n",
    "\n",
    "for task in mb.tasks:\n",
    "    task.load()\n",
    "\n",
    "    for fold_ind, fold in enumerate(task.folds):\n",
    "        if task.is_recorded[fold_ind]:\n",
    "            print(f\"Skipping fold {fold_ind} of {task.dataset_name}\")\n",
    "            continue\n",
    "        pred = train_test_fold(task, fold)\n",
    "        predictions[task.dataset_name].append(pred)\n",
    "        train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "    # print(f\"{task.dataset_name}: MAE  {task.scores['mae']['mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid __array_struct__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m task\u001b[39m.\u001b[39;49mscores\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/matbench/task.py:650\u001b[0m, in \u001b[0;36mMatbenchTask.scores\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m     raw_metrics_on_folds \u001b[39m=\u001b[39m [\n\u001b[1;32m    646\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults[fk][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_SCORES_KEY][mk]\n\u001b[1;32m    647\u001b[0m         \u001b[39mfor\u001b[39;00m fk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfolds_map\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    648\u001b[0m     ]\n\u001b[1;32m    649\u001b[0m     \u001b[39mfor\u001b[39;00m op \u001b[39min\u001b[39;00m FOLD_DIST_METRICS:\n\u001b[0;32m--> 650\u001b[0m         metric[op] \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(np, op)(raw_metrics_on_folds)\n\u001b[1;32m    651\u001b[0m     scores[mk] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m RecursiveDotDict(scores)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3429\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3430\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3432\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3433\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/numpy/core/_methods.py:164\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mean\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 164\u001b[0m     arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[1;32m    166\u001b[0m     is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39mkeepdims, where\u001b[39m=\u001b[39mwhere)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid __array_struct__"
     ]
    }
   ],
   "source": [
    "task.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-30 18:53:04 INFO     Successfully wrote MatbenchBenchmark to file 'gpt_steel_bench.json.gz'.\n"
     ]
    }
   ],
   "source": [
    "mb.to_file(\"gpt_expt_gap_bench.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f3b9074e5baa1438c27e2ea813f7f53b7516c83bd70840b6d64eae6820ee5df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
