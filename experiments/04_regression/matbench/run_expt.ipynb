{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from loguru import logger\n",
    "from matbench.bench import MatbenchBenchmark\n",
    "from matbench.constants import CLF_KEY\n",
    "\n",
    "from gptchem.gpt_regressor import GPTRegressor\n",
    "from gptchem.tuner import Tuner\n",
    "\n",
    "logger.enable(\"gptchem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import decorator\n",
    "\n",
    "\n",
    "def retry(howmany, *exception_types, **kwargs):\n",
    "    timeout = kwargs.get(\"timeout\", 0.0)  # seconds\n",
    "\n",
    "    @decorator.decorator\n",
    "    def tryIt(func, *fargs, **fkwargs):\n",
    "        for _ in range(howmany):\n",
    "            try:\n",
    "                return func(*fargs, **fkwargs)\n",
    "            except exception_types or Exception as e:\n",
    "                print(e)\n",
    "                if timeout is not None:\n",
    "                    time.sleep(timeout)\n",
    "\n",
    "    return tryIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 00:22:47 INFO     Initialized benchmark 'matbench_v0.1' with 1 tasks: \n",
      "['matbench_expt_gap']\n"
     ]
    }
   ],
   "source": [
    "mb = MatbenchBenchmark(\n",
    "    autoload=True,\n",
    "    subset=[\n",
    "          \"matbench_expt_gap\",\n",
    "        #\"matbench_steels\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(3, timeout=5)\n",
    "def train_test_fold(task, fold):\n",
    "    regressor = GPTRegressor(\n",
    "        task.metadata[\"target\"], Tuner(n_epochs=8, learning_rate_multiplier=0.02, wandb_sync=False)\n",
    "    )\n",
    "    train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "    # train and validate your model\n",
    "    regressor.fit(train_inputs, train_outputs.values)\n",
    "\n",
    "    # Get testing data\n",
    "    test_inputs = task.get_test_data(fold, include_target=False)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    # Your output should be a pandas series, numpy array, or python iterable\n",
    "    # where the array elements are floats or bools\n",
    "    predictions = regressor.predict(test_inputs)\n",
    "\n",
    "    # Record your data!\n",
    "    task.record(fold, predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 08:45:57 INFO     Dataset matbench_expt_gap already loaded; not reloading dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 829Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_084557/train.jsonl: file-b04uhvQekRJtbj22xWLfv85a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 08:46:00.762 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675151160,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675151160,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-uwfH6Ym2t6tZpl1M0nJZzCqt\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-uwfH6Ym2t6tZpl1M0nJZzCqt\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420861,\n",
      "      \"created_at\": 1675151160,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_084557/train.jsonl\",\n",
      "      \"id\": \"file-b04uhvQekRJtbj22xWLfv85a\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675151160,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-01-31 08:46:04.441 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 08:48:04.926 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 08:50:06.247 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 08:52:06.732 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 08:54:07.185 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 08:56:07.646 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 08:58:08.105 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 09:00:08.584 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 09:02:09.230 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 09:04:09.691 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 09:06:10.153 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 09:08:10.585 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 09:10:11.029 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 09:12:11.482 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:14:11.946 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:16:12.425 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:18:12.891 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:20:13.358 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:22:13.832 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:24:14.294 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:26:14.760 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:28:15.205 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:30:15.750 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:32:16.206 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:34:16.652 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:36:17.211 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:38:17.669 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:40:18.117 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:42:18.561 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:44:19.018 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:46:19.458 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:48:19.912 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:50:20.381 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:52:20.826 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:54:21.277 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:56:21.727 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 09:58:22.195 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-01-31 09:58:22.198 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_084557', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_084557/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-08-58-04', 'ft_id': 'ft-uwfH6Ym2t6tZpl1M0nJZzCqt', 'date': '20230131_095822', 'train_file_id': 'file-b04uhvQekRJtbj22xWLfv85a', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 10:00:22 INFO     Recorded fold matbench_expt_gap-0 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 825Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_100022/train.jsonl: file-8lzGg4tzapz7REUriT4vidQB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 10:00:24.663 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675155624,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675155624,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-t5kGx4hUEderRD6VpjBbGE11\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-t5kGx4hUEderRD6VpjBbGE11\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420507,\n",
      "      \"created_at\": 1675155624,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_100022/train.jsonl\",\n",
      "      \"id\": \"file-8lzGg4tzapz7REUriT4vidQB\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675155624,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-01-31 10:00:24.851 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:02:25.306 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:04:25.774 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:06:26.227 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:08:26.670 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:10:27.132 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:12:27.594 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:14:28.075 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:16:28.534 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:18:28.994 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:20:29.474 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:22:29.936 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:24:30.421 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:26:30.880 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:28:31.336 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:30:31.788 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:32:32.246 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:34:32.705 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:36:33.186 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:38:33.622 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:40:34.074 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:42:34.535 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:44:35.184 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:46:35.648 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:48:36.161 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:50:36.623 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:52:37.081 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:54:37.553 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 10:56:37.988 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 10:58:38.423 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:00:38.869 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:02:39.312 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:04:39.794 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:06:40.251 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:08:40.716 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:10:41.159 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:12:41.596 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:14:42.090 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:16:42.552 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:18:43.012 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:20:43.490 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:22:43.988 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:24:44.465 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:26:44.950 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:28:45.378 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:30:45.832 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:32:46.286 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:34:46.756 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:36:47.265 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:38:47.717 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:40:48.173 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 11:42:48.636 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-01-31 11:42:48.640 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_100022', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_100022/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-10-40-50', 'ft_id': 'ft-t5kGx4hUEderRD6VpjBbGE11', 'date': '20230131_114248', 'train_file_id': 'file-8lzGg4tzapz7REUriT4vidQB', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:44:48 INFO     Recorded fold matbench_expt_gap-1 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 420k/420k [00:00<00:00, 426Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_114448/train.jsonl: file-1rDCDOkxfyCBM41ioCcCjyNE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:44:50.719 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675161890,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675161890,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-Pg7oLij2cnq8XpNuNi6Q33ss\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-Pg7oLij2cnq8XpNuNi6Q33ss\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420263,\n",
      "      \"created_at\": 1675161890,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_114448/train.jsonl\",\n",
      "      \"id\": \"file-1rDCDOkxfyCBM41ioCcCjyNE\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675161890,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-01-31 11:44:50.900 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 11:46:51.368 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 11:48:51.821 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 11:50:52.297 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 11:52:52.764 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 11:54:53.218 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 11:56:53.704 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 11:58:54.263 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 12:00:54.720 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 12:02:55.197 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 12:04:55.667 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 12:06:56.166 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 12:08:56.636 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 12:10:57.144 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 12:12:57.645 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 12:14:58.096 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 12:16:58.585 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:18:59.063 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:30:41.726 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:32:42.199 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:34:42.674 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:36:43.150 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:38:43.830 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:40:44.303 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:42:44.756 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:44:45.305 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:46:45.745 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:48:46.192 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:50:46.646 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:52:47.108 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:54:47.588 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:56:48.041 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 12:58:48.507 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:00:48.976 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:02:49.455 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-01-31 13:02:49.462 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_114448', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_114448/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-01-31-12-02-16', 'ft_id': 'ft-Pg7oLij2cnq8XpNuNi6Q33ss', 'date': '20230131_130249', 'train_file_id': 'file-1rDCDOkxfyCBM41ioCcCjyNE', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 13:04:49 INFO     Recorded fold matbench_expt_gap-2 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 806Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_130449/train.jsonl: file-TYjf71XwA4LJNGgSrJCpK38H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 13:04:52.043 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675166691,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675166691,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-uv7RQGIVJXe6Ho7is8fsaGgY\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-uv7RQGIVJXe6Ho7is8fsaGgY\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420723,\n",
      "      \"created_at\": 1675166691,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230131_130449/train.jsonl\",\n",
      "      \"id\": \"file-TYjf71XwA4LJNGgSrJCpK38H\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675166691,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-01-31 13:04:52.232 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 13:06:52.695 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-01-31 13:08:53.179 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:10:53.665 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:12:54.149 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:14:54.630 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:16:55.101 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:18:55.612 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:20:56.082 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:22:56.531 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:24:56.993 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:26:57.465 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:28:57.951 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:30:58.398 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-01-31 13:32:58.864 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n"
     ]
    }
   ],
   "source": [
    "predictions = defaultdict(list)\n",
    "\n",
    "for task in mb.tasks:\n",
    "    task.load()\n",
    "\n",
    "    for fold_ind, fold in enumerate(task.folds):\n",
    "        if task.is_recorded[fold_ind]:\n",
    "            print(f\"Skipping fold {fold_ind} of {task.dataset_name}\")\n",
    "            continue\n",
    "        pred = train_test_fold(task, fold)\n",
    "        predictions[task.dataset_name].append(pred)\n",
    "        train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "    # print(f\"{task.dataset_name}: MAE  {task.scores['mae']['mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot score unless all folds are recorded!; folds [0, 1, 2, 3, 4] not recorded!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m task\u001b[39m.\u001b[39;49mscores\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/matbench/task.py:640\u001b[0m, in \u001b[0;36mMatbenchTask.scores\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m metric_keys \u001b[39m=\u001b[39m (\n\u001b[1;32m    637\u001b[0m     REG_METRICS \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mtask_type \u001b[39m==\u001b[39m REG_KEY \u001b[39melse\u001b[39;00m CLF_METRICS\n\u001b[1;32m    638\u001b[0m )\n\u001b[1;32m    639\u001b[0m scores \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 640\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_all_folds_recorded(\u001b[39m\"\u001b[39;49m\u001b[39mCannot score unless all folds are recorded!\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    641\u001b[0m \u001b[39mfor\u001b[39;00m mk \u001b[39min\u001b[39;00m metric_keys:\n\u001b[1;32m    642\u001b[0m     metric \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/matbench/task.py:174\u001b[0m, in \u001b[0;36mMatbenchTask._check_all_folds_recorded\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m\"\"\"Private method to check if all folds have been recorded.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[39mThrows error if all folds have not been recorded.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m    None\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_folds_recorded:\n\u001b[0;32m--> 174\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    175\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m; folds \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m[f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_recorded \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_recorded[f]]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnot recorded!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot score unless all folds are recorded!; folds [0, 1, 2, 3, 4] not recorded!"
     ]
    }
   ],
   "source": [
    "task.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-30 18:53:04 INFO     Successfully wrote MatbenchBenchmark to file 'gpt_steel_bench.json.gz'.\n"
     ]
    }
   ],
   "source": [
    "mb.to_file(\"gpt_expt_gap_bench.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f3b9074e5baa1438c27e2ea813f7f53b7516c83bd70840b6d64eae6820ee5df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
