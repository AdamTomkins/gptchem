{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from loguru import logger\n",
    "from matbench.bench import MatbenchBenchmark\n",
    "from matbench.constants import CLF_KEY\n",
    "\n",
    "from gptchem.gpt_regressor import GPTRegressor\n",
    "from gptchem.tuner import Tuner\n",
    "\n",
    "logger.enable(\"gptchem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import decorator\n",
    "\n",
    "\n",
    "def retry(howmany, *exception_types, **kwargs):\n",
    "    timeout = kwargs.get(\"timeout\", 0.0)  # seconds\n",
    "\n",
    "    @decorator.decorator\n",
    "    def tryIt(func, *fargs, **fkwargs):\n",
    "        for _ in range(howmany):\n",
    "            try:\n",
    "                return func(*fargs, **fkwargs)\n",
    "            except exception_types or Exception as e:\n",
    "                print(e)\n",
    "                if timeout is not None:\n",
    "                    time.sleep(timeout)\n",
    "\n",
    "    return tryIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 08:18:44 INFO     Initialized benchmark 'matbench_v0.1' with 1 tasks: \n",
      "['matbench_expt_gap']\n"
     ]
    }
   ],
   "source": [
    "mb = MatbenchBenchmark(\n",
    "    autoload=True,\n",
    "    subset=[\n",
    "          \"matbench_expt_gap\",\n",
    "        #\"matbench_steels\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(3, timeout=5)\n",
    "def train_test_fold(task, fold):\n",
    "    regressor = GPTRegressor(\n",
    "        task.metadata[\"target\"], Tuner(n_epochs=8, learning_rate_multiplier=0.02, wandb_sync=False),\n",
    "        querier_settings={'max_tokens': 10}\n",
    "    )\n",
    "    train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "    # train and validate your model\n",
    "    regressor.fit(train_inputs, train_outputs.values)\n",
    "\n",
    "    # Get testing data\n",
    "    test_inputs = task.get_test_data(fold, include_target=False)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    # Your output should be a pandas series, numpy array, or python iterable\n",
    "    # where the array elements are floats or bools\n",
    "    predictions = regressor.predict(test_inputs)\n",
    "\n",
    "    # Record your data!\n",
    "    task.record(fold, predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 08:18:45 INFO     Dataset matbench_expt_gap already loaded; not reloading dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 504Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_081845/train.jsonl: file-ARrYEF7Zdlfsgv10JMANU3jJ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 08:18:48.458 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675235928,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675235928,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-FEYGNp0CwMubNCiqlt3GL8sY\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-FEYGNp0CwMubNCiqlt3GL8sY\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420861,\n",
      "      \"created_at\": 1675235927,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_081845/train.jsonl\",\n",
      "      \"id\": \"file-ARrYEF7Zdlfsgv10JMANU3jJ\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675235928,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-01 08:18:48.623 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:20:49.243 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:22:49.793 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:24:50.572 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:26:51.174 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:28:51.904 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:30:52.502 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:32:53.267 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:34:53.860 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:36:54.609 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:38:55.174 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:40:55.769 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:42:56.350 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:44:56.914 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:46:57.991 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:48:58.583 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:50:59.177 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:52:59.769 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:55:00.358 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:57:01.136 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 08:59:01.695 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:01:02.279 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:03:03.302 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:05:03.806 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:07:04.925 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:09:05.514 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:11:06.100 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:13:06.688 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:15:07.205 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:17:07.729 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:19:08.223 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:21:08.742 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:23:09.403 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:25:10.010 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:27:10.500 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:29:11.058 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:31:11.658 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:33:12.794 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:35:15.846 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:37:16.579 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:39:17.148 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:41:17.685 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:43:18.439 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:45:19.016 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:47:19.605 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:49:20.172 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:51:20.735 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:53:21.363 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:55:22.634 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:57:23.242 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 09:59:23.840 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:01:24.622 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:03:25.196 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:05:26.191 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:07:26.704 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:09:27.297 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:11:27.900 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:13:28.686 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:15:29.474 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:17:30.043 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:19:30.619 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:21:31.725 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:23:32.459 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:25:34.645 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:27:35.209 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:29:36.364 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:31:36.934 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:33:37.746 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:35:38.474 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:37:39.064 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:39:39.658 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:41:40.328 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:43:41.453 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:45:42.147 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:47:42.766 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:49:43.565 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:51:44.157 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:53:45.158 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:55:45.910 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:57:46.702 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 10:59:47.360 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:01:47.943 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:03:48.534 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:05:49.190 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:07:49.783 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:09:50.438 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:11:50.959 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:13:51.548 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:15:52.140 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:17:52.736 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:19:53.323 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:21:53.909 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:23:54.566 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:25:55.168 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:27:55.759 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:29:56.495 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:31:57.080 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:33:58.127 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:35:58.717 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:37:59.497 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:40:00.021 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:42:00.602 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:44:01.105 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:46:01.981 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:48:02.627 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:51:17.608 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:53:18.189 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 11:55:19.338 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:02:27.203 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:04:27.710 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:06:28.315 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:08:29.075 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:10:29.687 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:12:30.269 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:14:30.872 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:16:31.446 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:18:32.050 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:20:32.988 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:22:34.298 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:24:34.877 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:26:35.434 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:28:36.050 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:30:36.614 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:32:37.234 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:34:37.803 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:36:38.955 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:38:39.513 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:40:39.994 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:42:40.559 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 12:44:41.110 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 12:46:41.706 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 12:48:42.389 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 12:50:43.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 12:52:43.654 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 12:54:44.143 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 12:56:44.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 12:58:45.251 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:00:45.839 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:02:46.668 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:04:47.216 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:06:47.704 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:08:48.235 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:10:48.818 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:12:49.404 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:14:49.986 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:16:50.588 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:18:51.178 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:20:51.767 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:22:52.357 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:24:52.948 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:26:53.528 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:28:54.120 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 13:30:54.727 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-01 13:30:54.762 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_081845', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_081845/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-12-28-59', 'ft_id': 'ft-FEYGNp0CwMubNCiqlt3GL8sY', 'date': '20230201_133054', 'train_file_id': 'file-ARrYEF7Zdlfsgv10JMANU3jJ', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 13:32:55 INFO     Recorded fold matbench_expt_gap-0 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 301Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_133255/train.jsonl: file-oADBU4v2BqrCMJBGhDGMgKKK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 13:32:58.245 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675254778,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675254778,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-f6OTECMmzuQ4uk0312YdZptE\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-f6OTECMmzuQ4uk0312YdZptE\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420507,\n",
      "      \"created_at\": 1675254777,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_133255/train.jsonl\",\n",
      "      \"id\": \"file-oADBU4v2BqrCMJBGhDGMgKKK\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675254778,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-01 13:32:58.534 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:34:59.349 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:37:00.166 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:40:26.153 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:42:27.087 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:44:27.825 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:46:28.391 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:48:28.916 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:50:29.501 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:52:30.306 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:54:31.071 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:56:31.784 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 13:58:32.376 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:00:32.916 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:02:33.464 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:04:34.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:06:34.831 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:08:35.958 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:10:36.545 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:12:36.998 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:14:38.264 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:16:38.820 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:18:39.890 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:20:40.446 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:22:41.022 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:24:41.537 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:26:42.268 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 14:28:42.863 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:30:43.458 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:32:44.215 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:34:44.754 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:36:45.394 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:38:45.885 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:40:46.374 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:42:47.131 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:44:47.669 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:46:48.133 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:48:49.221 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:50:49.955 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:52:50.537 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:54:51.120 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:56:51.862 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 14:58:52.561 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 15:00:53.195 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 15:02:53.745 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 15:04:54.334 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 15:06:54.924 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 15:08:55.549 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 15:10:56.107 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 15:12:56.695 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-01 15:12:56.704 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_133255', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_133255/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-14-12-41', 'ft_id': 'ft-f6OTECMmzuQ4uk0312YdZptE', 'date': '20230201_151256', 'train_file_id': 'file-oADBU4v2BqrCMJBGhDGMgKKK', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 15:14:57 INFO     Recorded fold matbench_expt_gap-1 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 420k/420k [00:00<00:00, 91.2Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_151457/train.jsonl: file-xaNVbL4q0UW039rbePFw0taS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 15:14:59.701 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675260899,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675260899,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-dqqViIfMkUYlTss2SFf89pbc\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-dqqViIfMkUYlTss2SFf89pbc\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420263,\n",
      "      \"created_at\": 1675260899,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_151457/train.jsonl\",\n",
      "      \"id\": \"file-xaNVbL4q0UW039rbePFw0taS\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675260899,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-01 15:14:59.898 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:17:00.489 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:19:01.148 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:21:01.732 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:23:02.400 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:25:03.162 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:27:03.748 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:29:04.356 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:31:04.941 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:33:06.079 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:35:06.621 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:37:07.564 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:39:08.102 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:41:08.653 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:43:09.242 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:45:09.845 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:47:10.677 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:49:11.332 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:51:11.999 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:53:12.729 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:55:13.334 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:57:14.200 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 15:59:14.719 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 16:01:27.614 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 16:03:28.315 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 16:05:33.869 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 16:07:34.482 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:09:35.095 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:11:35.677 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:13:36.266 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:15:39.665 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:17:40.213 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:19:40.784 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:21:41.387 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:23:41.970 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:25:46.041 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:27:46.702 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:29:47.314 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:31:47.910 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:33:48.505 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:35:49.123 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:37:49.703 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:40:51.435 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:42:52.039 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:44:52.829 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:46:53.685 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:48:54.175 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:50:54.744 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-01 16:52:55.639 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-01 16:52:55.641 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_151457', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_151457/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-01-15-51-47', 'ft_id': 'ft-dqqViIfMkUYlTss2SFf89pbc', 'date': '20230201_165255', 'train_file_id': 'file-xaNVbL4q0UW039rbePFw0taS', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 16:54:55 INFO     Recorded fold matbench_expt_gap-2 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 706Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_165455/train.jsonl: file-95TFqfcTknod0DTizJ4ZbXXQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 16:54:58.029 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675266897,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675266897,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-TUSbWrEwLd6dZP9CYYS9FXso\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-TUSbWrEwLd6dZP9CYYS9FXso\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420723,\n",
      "      \"created_at\": 1675266897,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230201_165455/train.jsonl\",\n",
      "      \"id\": \"file-95TFqfcTknod0DTizJ4ZbXXQ\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675266897,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-01 16:54:58.204 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 16:56:58.897 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 16:58:59.842 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:01:00.470 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:03:00.997 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:05:01.921 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:07:02.560 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:09:03.661 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:11:04.215 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:13:04.680 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:15:05.464 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:17:06.028 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:19:06.847 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:21:07.627 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:23:08.663 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:28:41.105 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:30:41.708 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:32:42.297 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:34:42.876 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:36:43.385 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:38:43.878 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:40:44.964 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:42:45.555 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:44:46.159 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:46:46.742 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:48:47.336 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-01 17:50:47.923 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n"
     ]
    }
   ],
   "source": [
    "predictions = defaultdict(list)\n",
    "\n",
    "for task in mb.tasks:\n",
    "    task.load()\n",
    "\n",
    "    for fold_ind, fold in enumerate(task.folds):\n",
    "        if task.is_recorded[fold_ind]:\n",
    "            print(f\"Skipping fold {fold_ind} of {task.dataset_name}\")\n",
    "            continue\n",
    "        pred = train_test_fold(task, fold)\n",
    "        predictions[task.dataset_name].append(pred)\n",
    "        train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "    # print(f\"{task.dataset_name}: MAE  {task.scores['mae']['mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid __array_struct__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m task\u001b[39m.\u001b[39;49mscores\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/matbench/task.py:650\u001b[0m, in \u001b[0;36mMatbenchTask.scores\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m     raw_metrics_on_folds \u001b[39m=\u001b[39m [\n\u001b[1;32m    646\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults[fk][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_SCORES_KEY][mk]\n\u001b[1;32m    647\u001b[0m         \u001b[39mfor\u001b[39;00m fk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfolds_map\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    648\u001b[0m     ]\n\u001b[1;32m    649\u001b[0m     \u001b[39mfor\u001b[39;00m op \u001b[39min\u001b[39;00m FOLD_DIST_METRICS:\n\u001b[0;32m--> 650\u001b[0m         metric[op] \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(np, op)(raw_metrics_on_folds)\n\u001b[1;32m    651\u001b[0m     scores[mk] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m RecursiveDotDict(scores)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3429\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3430\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3432\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3433\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/numpy/core/_methods.py:164\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mean\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 164\u001b[0m     arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[1;32m    166\u001b[0m     is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39mkeepdims, where\u001b[39m=\u001b[39mwhere)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid __array_struct__"
     ]
    }
   ],
   "source": [
    "task.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-30 18:53:04 INFO     Successfully wrote MatbenchBenchmark to file 'gpt_steel_bench.json.gz'.\n"
     ]
    }
   ],
   "source": [
    "mb.to_file(\"gpt_expt_gap_bench.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f3b9074e5baa1438c27e2ea813f7f53b7516c83bd70840b6d64eae6820ee5df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
