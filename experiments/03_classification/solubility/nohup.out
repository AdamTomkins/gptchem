WARNING:tensorflow:Gradients do not exist for variables ['weave_layer_1/kernel:0', 'weave_layer_1/Variable:0', 'weave_layer_1/kernel:0', 'weave_layer_1/Variable:0', 'weave_layer_1/kernel:0', 'weave_layer_1/Variable:0', 'weave_layer_1/batch_normalization_3/gamma:0', 'weave_layer_1/batch_normalization_3/beta:0', 'weave_layer_1/batch_normalization_4/gamma:0', 'weave_layer_1/batch_normalization_4/beta:0', 'weave_layer_1/batch_normalization_5/gamma:0', 'weave_layer_1/batch_normalization_5/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
WARNING:tensorflow:Gradients do not exist for variables ['weave_layer_1/kernel:0', 'weave_layer_1/Variable:0', 'weave_layer_1/kernel:0', 'weave_layer_1/Variable:0', 'weave_layer_1/kernel:0', 'weave_layer_1/Variable:0', 'weave_layer_1/batch_normalization_3/gamma:0', 'weave_layer_1/batch_normalization_3/beta:0', 'weave_layer_1/batch_normalization_4/gamma:0', 'weave_layer_1/batch_normalization_4/beta:0', 'weave_layer_1/batch_normalization_5/gamma:0', 'weave_layer_1/batch_normalization_5/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
2023-01-17 10:26:16.473302: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
WARNING:tensorflow:Gradients do not exist for variables ['weave_layer_1/kernel:0', 'weave_layer_1/Variable:0', 'weave_layer_1/kernel:0', 'weave_layer_1/Variable:0', 'weave_layer_1/kernel:0', 'weave_layer_1/Variable:0', 'weave_layer_1/batch_normalization_3/gamma:0', 'weave_layer_1/batch_normalization_3/beta:0', 'weave_layer_1/batch_normalization_4/gamma:0', 'weave_layer_1/batch_normalization_4/beta:0', 'weave_layer_1/batch_normalization_5/gamma:0', 'weave_layer_1/batch_normalization_5/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
WARNING:tensorflow:5 out of the last 34 calls to <function KerasModel._create_gradient_fn.<locals>.apply_gradient_for_batch at 0x2c3e5a940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[32m[I 2023-01-17 10:26:28,818][0m A new study created in memory with name: no-name-165e5a10-5f44-4b62-804b-1aef26646b89[0m
/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.001, 0.05),
/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py:46: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.
  "colsample_bytree": trial.suggest_loguniform("colsample_bytree", 0.2, 1),
/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py:47: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.
  "subsample": trial.suggest_loguniform("subsample", 0.00001, 1),
/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py:48: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.
  "alpha": trial.suggest_loguniform("alpha", 1e-8, 10.0),
/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.
  "lambda": trial.suggest_loguniform("lambda", 1e-8, 10.0),
/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/xgboost/sklearn.py:198: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.
  warnings.warn('The seed parameter is deprecated as of version .6.'
/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/xgboost/core.py:995: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  if isinstance(params, collections.Mapping):
/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/xgboost/sklearn.py:198: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.
  warnings.warn('The seed parameter is deprecated as of version .6.'
[33m[W 2023-01-17 10:26:30,772][0m Trial 0 failed because of the following error: ValueError('y contains previously unseen labels: [4]')[0m
Traceback (most recent call last):
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py", line 83, in <lambda>
    lambda trial: objective(
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py", line 68, in objective
    model.fit(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/xgboost/sklearn.py", line 522, in fit
    evals = list(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/xgboost/sklearn.py", line 523, in <genexpr>
    DMatrix(eval_set[i][0], label=self._le.transform(eval_set[i][1]),
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 142, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/preprocessing/_label.py", line 139, in transform
    return _encode(y, uniques=self.classes_)
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/utils/_encode.py", line 231, in _encode
    raise ValueError(f"y contains previously unseen labels: {str(diff)}")
ValueError: y contains previously unseen labels: [4]
╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═════════╕
│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │   value │
╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═════════╡
│ GPR.mean_function.c     │ Parameter │ Identity         │         │ True        │ ()      │ float64 │ 0.21801 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 2.00247 │
├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼─────────┤
│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │ 0.26671 │
╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═════════╛
Loading model that can be used for inference only
Using a Transformer with 25.82 M parameters
Traceback (most recent call last):
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/solubility/run_experiment.py", line 70, in <module>
    train_test(train_size, representation, num_class, seed=i + 425667726)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/solubility/run_experiment.py", line 32, in train_test
    baseline = train_test_solubility_classification_baseline(
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/baselines/solubility.py", line 159, in train_test_solubility_classification_baseline
    xgbclassifier.tune(X_train, y_train)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py", line 82, in tune
    study.optimize(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/optuna/study/study.py", line 419, in optimize
    _optimize(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/optuna/study/_optimize.py", line 160, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/optuna/study/_optimize.py", line 234, in _run_trial
    raise func_err
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py", line 83, in <lambda>
    lambda trial: objective(
  File "/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/src/gptchem/models/xgboost.py", line 68, in objective
    model.fit(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/xgboost/sklearn.py", line 522, in fit
    evals = list(
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/xgboost/sklearn.py", line 523, in <genexpr>
    DMatrix(eval_set[i][0], label=self._le.transform(eval_set[i][1]),
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 142, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/preprocessing/_label.py", line 139, in transform
    return _encode(y, uniques=self.classes_)
  File "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/utils/_encode.py", line 231, in _encode
    raise ValueError(f"y contains previously unseen labels: {str(diff)}")
ValueError: y contains previously unseen labels: [4]
/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/tmp3d96lqmrwandb'>
  _warnings.warn(warn_message, ResourceWarning)
/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/tmp6j2at3zswandb-artifacts'>
  _warnings.warn(warn_message, ResourceWarning)
/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/tmpizskfrdewandb-media'>
  _warnings.warn(warn_message, ResourceWarning)
/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/tempfile.py:821: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/tmpnddg4cvmwandb-media'>
  _warnings.warn(warn_message, ResourceWarning)
