{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptchem.data import get_hea_phase_data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_hea_phase_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['composition'] = data['Alloy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['composition', 'phase_binary_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/utils/data.py:506: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  self.params = pd.read_csv(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/utils/data.py:555: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_params = self.params.append(new_data, sort=True, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from automatminer import MatPipe\n",
    "\n",
    "pipe = MatPipe.from_preset(\"express\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, train_size=10, random_state=42, stratify=data['phase_binary_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:29:52 INFO     Problem type is: classification\n",
      "2023-01-18 19:29:52 INFO     Fitting MatPipe pipeline to data.\n",
      "2023-01-18 19:29:52 INFO     AutoFeaturizer: Starting fitting.\n",
      "2023-01-18 19:29:52 INFO     AutoFeaturizer: Compositions detected as strings. Attempting conversion to Composition objects...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d884bff19048ff91d4b720c8944099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StrToComposition:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:29:54 INFO     AutoFeaturizer: Guessing oxidation states of compositions, as they were not present in input.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bec90063f64a329d271358d84c285b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CompositionToOxidComposition:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:29:55 INFO     AutoFeaturizer: Featurizer type structure not in the dataframe to be fitted. Skipping...\n",
      "2023-01-18 19:29:55 INFO     AutoFeaturizer: Featurizer type bandstructure not in the dataframe to be fitted. Skipping...\n",
      "2023-01-18 19:29:55 INFO     AutoFeaturizer: Featurizer type dos not in the dataframe to be fitted. Skipping...\n",
      "2023-01-18 19:29:55 INFO     AutoFeaturizer: Finished fitting.\n",
      "2023-01-18 19:29:55 INFO     AutoFeaturizer: Starting transforming.\n",
      "2023-01-18 19:29:55 INFO     AutoFeaturizer: Featurizing with ElementProperty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c967c4798a44f19ceca3f8b055dc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElementProperty:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:29:56 INFO     AutoFeaturizer: Featurizing with OxidationStates.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c605f240e9ee45fdb698b1081db814fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "OxidationStates:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:29:57 INFO     AutoFeaturizer: Featurizing with ElectronAffinity.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516ecabd684d43418cdf73f521e6f620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElectronAffinity:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:29:58 INFO     AutoFeaturizer: Featurizing with IonProperty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69056a98b85847b8bb02eb39b3fb44d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IonProperty:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:29:59 INFO     AutoFeaturizer: Featurizing with YangSolidSolution.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489653ae9b4f4ed585d1310a3669a705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "YangSolidSolution:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:30:00 INFO     AutoFeaturizer: Featurizing with Miedema.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ea97d8d5464bf0ab9936d4c6865738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Miedema:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:30:02 INFO     AutoFeaturizer: Featurizer type structure not in the dataframe. Skipping...\n",
      "2023-01-18 19:30:02 INFO     AutoFeaturizer: Featurizer type bandstructure not in the dataframe. Skipping...\n",
      "2023-01-18 19:30:02 INFO     AutoFeaturizer: Featurizer type dos not in the dataframe. Skipping...\n",
      "2023-01-18 19:30:02 INFO     AutoFeaturizer: Finished transforming.\n",
      "2023-01-18 19:30:02 INFO     DataCleaner: Starting fitting.\n",
      "2023-01-18 19:30:02 INFO     DataCleaner: Cleaning with respect to samples with sample na_method 'drop'\n",
      "2023-01-18 19:30:02 INFO     DataCleaner: Replacing infinite values with nan for easier screening.\n",
      "2023-01-18 19:30:02 INFO     DataCleaner: Before handling na: 10 samples, 146 features\n",
      "2023-01-18 19:30:02 INFO     DataCleaner: 0 samples did not have target values. They were dropped.\n",
      "2023-01-18 19:30:02 INFO     DataCleaner: Handling feature na by max na threshold of 0.01 with method 'drop'.\n",
      "2023-01-18 19:30:02 INFO     DataCleaner: These 8 features were removed as they had more than 1.0% missing values: {'minimum oxidation state', 'std_dev oxidation state', 'avg anion electron affinity', 'max ionic char', 'avg ionic char', 'compound possible', 'range oxidation state', 'maximum oxidation state'}\n",
      "2023-01-18 19:30:02 INFO     DataCleaner: After handling na: 10 samples, 138 features\n",
      "2023-01-18 19:30:02 INFO     DataCleaner: Finished fitting.\n",
      "2023-01-18 19:30:02 INFO     FeatureReducer: Starting fitting.\n",
      "2023-01-18 19:30:02 INFO     FeatureReducer: 67 features removed due to cross correlation more than 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:704: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:704: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:30:04 INFO     TreeFeatureReducer: Finished tree-based feature reduction of 70 initial features to 40\n",
      "2023-01-18 19:30:04 INFO     FeatureReducer: Finished fitting.\n",
      "2023-01-18 19:30:04 INFO     FeatureReducer: Starting transforming.\n",
      "2023-01-18 19:30:04 INFO     FeatureReducer: Finished transforming.\n",
      "2023-01-18 19:30:04 INFO     TPOTAdaptor: Starting fitting.\n",
      "29 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9089bbb6f14b44b7a00b8b2d7218fd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-3\t0.7\tGaussianNB(RobustScaler(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.6500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.9500000000000001)))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-3\t0.9\tBernoulliNB(RBFSampler(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.8500000000000002, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.7000000000000001), RBFSampler__gamma=0.15000000000000002), BernoulliNB__alpha=1.0, BernoulliNB__fit_prior=True)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-3\t0.9\tBernoulliNB(RBFSampler(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.8500000000000002, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.7000000000000001), RBFSampler__gamma=0.15000000000000002), BernoulliNB__alpha=1.0, BernoulliNB__fit_prior=True)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as metric. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-3\t0.9\tBernoulliNB(RBFSampler(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.8500000000000002, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.7000000000000001), RBFSampler__gamma=0.15000000000000002), BernoulliNB__alpha=1.0, BernoulliNB__fit_prior=True)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as metric. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as metric. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by Nystroem..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The 'step' parameter of RFE must be an int in the range (0, inf) or a float in the range (0, 1). Got 1.0 instead..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by Nystroem..\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PCA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by PCA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by Nystroem..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by Nystroem..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(9, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(9, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-3\t1.0\tDecisionTreeClassifier(MaxAbsScaler(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesClassifier__criterion=gini, SelectFromModel__ExtraTreesClassifier__max_features=0.8500000000000002, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.05)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=1, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=2)\n",
      "2023-01-18 19:34:30 INFO     TPOTAdaptor: Finished fitting.\n",
      "2023-01-18 19:34:30 INFO     MatPipe successfully fit.\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(train, 'phase_binary_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:34:31 INFO     Beginning MatPipe prediction using fitted pipeline.\n",
      "2023-01-18 19:34:31 INFO     AutoFeaturizer: Starting transforming.\n",
      "2023-01-18 19:34:31 INFO     AutoFeaturizer: Compositions detected as strings. Attempting conversion to Composition objects...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ded3e0051314247a4a5302ab79ae671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StrToComposition:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:34:45 INFO     AutoFeaturizer: Guessing oxidation states of compositions, as they were not present in input.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116db419e4014973a209c829bdc406c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CompositionToOxidComposition:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:35:28 WARNING  AutoFeaturizer: Could not decorate oxidation states due to 'float' object has no attribute 'elements'\n",
      "TO SKIP THESE ERRORS when featurizing specific compounds, set 'ignore_errors=True' when running the batch featurize() operation (e.g., featurize_many(), featurize_dataframe(), etc.).. Some featurizers based on composition oxidstatesmay not work\n",
      "2023-01-18 19:35:28 INFO     AutoFeaturizer: Featurizing with ElementProperty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364f790df15646628e71368f8344d705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElementProperty:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:36:07 INFO     AutoFeaturizer: Featurizing with OxidationStates.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c7b85ec71f4b1c8a3583b71e75b004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "OxidationStates:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:36:44 INFO     AutoFeaturizer: Featurizing with ElectronAffinity.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e31a642ce6408d8a7c04373d02c8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElectronAffinity:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:37:22 INFO     AutoFeaturizer: Featurizing with IonProperty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cddd598d8647b283af788066415a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IonProperty:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:38:05 INFO     AutoFeaturizer: Featurizing with YangSolidSolution.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be96da94f204bc28ab521cfa38291fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "YangSolidSolution:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:38:44 INFO     AutoFeaturizer: Featurizing with Miedema.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d2f97197864f589b0fc3ecaa903f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Miedema:   0%|          | 0/1242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n",
      "/Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/featurizers/composition/alloy.py:198: RuntimeWarning: invalid value encountered in power\n",
      "  alp_a = np.multiply(1.5, np.power(v_a, 2 / 3)) / reduce(lambda x, y: 1 / x + 1 / y, np.power(n_ws, 1 / 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 19:39:25 INFO     AutoFeaturizer: Featurizer type structure not in the dataframe. Skipping...\n",
      "2023-01-18 19:39:25 INFO     AutoFeaturizer: Featurizer type bandstructure not in the dataframe. Skipping...\n",
      "2023-01-18 19:39:25 INFO     AutoFeaturizer: Featurizer type dos not in the dataframe. Skipping...\n",
      "2023-01-18 19:39:25 INFO     AutoFeaturizer: Finished transforming.\n",
      "2023-01-18 19:39:25 INFO     DataCleaner: Starting transforming.\n",
      "2023-01-18 19:39:25 INFO     DataCleaner: Cleaning with respect to samples with sample na_method 'fill'\n",
      "2023-01-18 19:39:25 INFO     DataCleaner: Replacing infinite values with nan for easier screening.\n",
      "2023-01-18 19:39:25 INFO     DataCleaner: Before handling na: 1242 samples, 146 features\n",
      "2023-01-18 19:39:25 INFO     DataCleaner: 0 samples did not have target values. They were dropped.\n",
      "2023-01-18 19:39:25 WARNING  DataCleaner: Mismatched columns found in dataframe used for fitting and argument dataframe.\n",
      "2023-01-18 19:39:25 WARNING  DataCleaner: Coercing mismatched columns...\n",
      "2023-01-18 19:39:25 WARNING  DataCleaner: Following columns are being dropped:\n",
      "['minimum oxidation state', 'maximum oxidation state', 'range oxidation state', 'std_dev oxidation state', 'avg anion electron affinity', 'compound possible', 'max ionic char', 'avg ionic char']\n",
      "2023-01-18 19:39:25 INFO     DataCleaner: After handling na: 1242 samples, 138 features\n",
      "2023-01-18 19:39:25 INFO     DataCleaner: Reordering columns...\n",
      "2023-01-18 19:39:25 INFO     DataCleaner: Finished transforming.\n",
      "2023-01-18 19:39:25 INFO     FeatureReducer: Starting transforming.\n",
      "2023-01-18 19:39:25 INFO     FeatureReducer: Finished transforming.\n",
      "2023-01-18 19:39:25 INFO     TPOTAdaptor: Starting predicting.\n",
      "2023-01-18 19:39:25 INFO     TPOTAdaptor: Prediction finished successfully.\n",
      "2023-01-18 19:39:25 INFO     TPOTAdaptor: Finished predicting.\n",
      "2023-01-18 19:39:25 INFO     MatPipe prediction completed.\n"
     ]
    }
   ],
   "source": [
    "predictions = pipe.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycm import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(actual_vector=test['phase_binary_encoded'].values, predict_vector=predictions['phase_binary_encoded predicted'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict   0         1         \n",
      "Actual\n",
      "0         272       350       \n",
      "\n",
      "1         1         619       \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overall Statistics : \n",
      "\n",
      "95% CI                                                            (0.69235,0.74243)\n",
      "ACC Macro                                                         0.71739\n",
      "ARI                                                               0.18859\n",
      "AUNP                                                              0.71784\n",
      "AUNU                                                              0.71784\n",
      "Bangdiwala B                                                      0.59324\n",
      "Bennett S                                                         0.43478\n",
      "CBA                                                               0.53805\n",
      "CSI                                                               0.53541\n",
      "Chi-Squared                                                       343.68828\n",
      "Chi-Squared DF                                                    1\n",
      "Conditional Entropy                                               0.50374\n",
      "Cramer V                                                          0.52604\n",
      "Cross Entropy                                                     1.27337\n",
      "F1 Macro                                                          0.69346\n",
      "F1 Micro                                                          0.71739\n",
      "FNR Macro                                                         0.28216\n",
      "FNR Micro                                                         0.28261\n",
      "FPR Macro                                                         0.28216\n",
      "FPR Micro                                                         0.28261\n",
      "Gwet AC1                                                          0.47571\n",
      "Hamming Loss                                                      0.28261\n",
      "Joint Entropy                                                     1.50374\n",
      "KL Divergence                                                     0.27337\n",
      "Kappa                                                             0.43529\n",
      "Kappa 95% CI                                                      (0.38525,0.48533)\n",
      "Kappa No Prevalence                                               0.43478\n",
      "Kappa Standard Error                                              0.02553\n",
      "Kappa Unbiased                                                    0.38693\n",
      "Krippendorff Alpha                                                0.38717\n",
      "Lambda A                                                          0.43387\n",
      "Lambda B                                                          0.0\n",
      "Mutual Information                                                0.25608\n",
      "NIR                                                               0.50081\n",
      "Overall ACC                                                       0.71739\n",
      "Overall CEN                                                       0.50663\n",
      "Overall J                                                         (1.07474,0.53737)\n",
      "Overall MCC                                                       0.52604\n",
      "Overall MCEN                                                      0.33216\n",
      "Overall RACC                                                      0.49955\n",
      "Overall RACCU                                                     0.53903\n",
      "P-Value                                                           None\n",
      "PPV Macro                                                         0.81757\n",
      "PPV Micro                                                         0.71739\n",
      "Pearson C                                                         0.46556\n",
      "Phi-Squared                                                       0.27672\n",
      "RCI                                                               0.25608\n",
      "RR                                                                621.0\n",
      "Reference Entropy                                                 1.0\n",
      "Response Entropy                                                  0.75981\n",
      "SOA1(Landis & Koch)                                               Moderate\n",
      "SOA2(Fleiss)                                                      Intermediate to Good\n",
      "SOA3(Altman)                                                      Moderate\n",
      "SOA4(Cicchetti)                                                   Fair\n",
      "SOA5(Cramer)                                                      Relatively Strong\n",
      "SOA6(Matthews)                                                    Moderate\n",
      "Scott PI                                                          0.38693\n",
      "Standard Error                                                    0.01278\n",
      "TNR Macro                                                         0.71784\n",
      "TNR Micro                                                         0.71739\n",
      "TPR Macro                                                         0.71784\n",
      "TPR Micro                                                         0.71739\n",
      "Zero-one Loss                                                     351\n",
      "\n",
      "Class Statistics :\n",
      "\n",
      "Classes                                                           0             1             \n",
      "ACC(Accuracy)                                                     0.71739       0.71739       \n",
      "AGF(Adjusted F-score)                                             0.58231       0.84382       \n",
      "AGM(Adjusted geometric mean)                                      0.77318       0.58619       \n",
      "AM(Difference between automatic and manual classification)        -349          349           \n",
      "AUC(Area under the ROC curve)                                     0.71784       0.71784       \n",
      "AUCI(AUC value interpretation)                                    Good          Good          \n",
      "AUPR(Area under the PR curve)                                     0.71682       0.81859       \n",
      "BB(Braun-Blanquet similarity)                                     0.4373        0.6388        \n",
      "BCD(Bray-Curtis dissimilarity)                                    0.1405        0.1405        \n",
      "BM(Informedness or bookmaker informedness)                        0.43569       0.43569       \n",
      "CEN(Confusion entropy)                                            0.54066       0.48746       \n",
      "DOR(Diagnostic odds ratio)                                        481.05143     481.05143     \n",
      "DP(Discriminant power)                                            1.47877       1.47877       \n",
      "DPI(Discriminant power interpretation)                            Limited       Limited       \n",
      "ERR(Error rate)                                                   0.28261       0.28261       \n",
      "F0.5(F0.5 score)                                                  0.79347       0.68839       \n",
      "F1(F1 score - harmonic mean of precision and sensitivity)         0.60782       0.77911       \n",
      "F2(F2 score)                                                      0.49258       0.89736       \n",
      "FDR(False discovery rate)                                         0.00366       0.3612        \n",
      "FN(False negative/miss/type 2 error)                              350           1             \n",
      "FNR(Miss rate or false negative rate)                             0.5627        0.00161       \n",
      "FOR(False omission rate)                                          0.3612        0.00366       \n",
      "FP(False positive/type 1 error/false alarm)                       1             350           \n",
      "FPR(Fall-out or false positive rate)                              0.00161       0.5627        \n",
      "G(G-measure geometric mean of precision and sensitivity)          0.66007       0.79861       \n",
      "GI(Gini index)                                                    0.43569       0.43569       \n",
      "GM(G-mean geometric mean of specificity and sensitivity)          0.66075       0.66075       \n",
      "HD(Hamming distance)                                              351           351           \n",
      "IBA(Index of balanced accuracy)                                   0.19163       0.68156       \n",
      "ICSI(Individual classification success index)                     0.43364       0.63719       \n",
      "IS(Information score)                                             0.99238       0.35577       \n",
      "J(Jaccard index)                                                  0.4366        0.63814       \n",
      "LS(Lift score)                                                    1.98947       1.27967       \n",
      "MCC(Matthews correlation coefficient)                             0.52604       0.52604       \n",
      "MCCI(Matthews correlation coefficient interpretation)             Moderate      Moderate      \n",
      "MCEN(Modified confusion entropy)                                  0.48225       0.54087       \n",
      "MK(Markedness)                                                    0.63514       0.63514       \n",
      "N(Condition negative)                                             620           622           \n",
      "NLR(Negative likelihood ratio)                                    0.56361       0.00369       \n",
      "NLRI(Negative likelihood ratio interpretation)                    Negligible    Good          \n",
      "NPV(Negative predictive value)                                    0.6388        0.99634       \n",
      "OC(Overlap coefficient)                                           0.99634       0.99839       \n",
      "OOC(Otsuka-Ochiai coefficient)                                    0.66007       0.79861       \n",
      "OP(Optimized precision)                                           0.32658       0.32658       \n",
      "P(Condition positive or support)                                  622           620           \n",
      "PLR(Positive likelihood ratio)                                    271.1254      1.77428       \n",
      "PLRI(Positive likelihood ratio interpretation)                    Good          Poor          \n",
      "POP(Population)                                                   1242          1242          \n",
      "PPV(Precision or positive predictive value)                       0.99634       0.6388        \n",
      "PRE(Prevalence)                                                   0.50081       0.49919       \n",
      "Q(Yule Q - coefficient of colligation)                            0.99585       0.99585       \n",
      "QI(Yule Q interpretation)                                         Strong        Strong        \n",
      "RACC(Random accuracy)                                             0.11008       0.38947       \n",
      "RACCU(Random accuracy unbiased)                                   0.12982       0.40921       \n",
      "TN(True negative/correct rejection)                               619           272           \n",
      "TNR(Specificity or true negative rate)                            0.99839       0.4373        \n",
      "TON(Test outcome negative)                                        969           273           \n",
      "TOP(Test outcome positive)                                        273           969           \n",
      "TP(True positive/hit)                                             272           619           \n",
      "TPR(Sensitivity, recall, hit rate, or true positive rate)         0.4373        0.99839       \n",
      "Y(Youden index)                                                   0.43569       0.43569       \n",
      "dInd(Distance index)                                              0.5627        0.5627        \n",
      "sInd(Similarity index)                                            0.60211       0.60211       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f3b9074e5baa1438c27e2ea813f7f53b7516c83bd70840b6d64eae6820ee5df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
