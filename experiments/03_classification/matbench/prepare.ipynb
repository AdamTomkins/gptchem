{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matbench.bench import MatbenchBenchmark\n",
    "from matbench.constants import CLF_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:46:34 INFO     Initialized benchmark 'matbench_v0.1' with 2 tasks: \n",
      "['matbench_glass', 'matbench_expt_is_metal']\n"
     ]
    }
   ],
   "source": [
    "mb = MatbenchBenchmark(\n",
    "    autoload=False,\n",
    "    subset=[\n",
    "        \"matbench_glass\",\n",
    "        \"matbench_expt_is_metal\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:48:08 INFO     Loading dataset 'matbench_glass'...\n",
      "Fetching matbench_glass.json.gz from https://ml.materialsproject.org/projects/matbench_glass.json.gz to /Users/kevinmaikjablonka/miniconda3/envs/gptchem/lib/python3.9/site-packages/matminer/datasets/matbench_glass.json.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching https://ml.materialsproject.org/projects/matbench_glass.json.gz in MB: 0.040959999999999996MB [00:00, 14.65MB/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:48:08 INFO     Dataset 'matbench_glass loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot score unless all folds are recorded!; folds [0, 1, 2, 3, 4] not recorded!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39m# predictions = predictions.flatten()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m# if stds is not None:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m#     stds = stds.flatten()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39m#     predictions,\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m task\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mtask_type \u001b[39m==\u001b[39m CLF_KEY:\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m.\u001b[39mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m: Accuracy score \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m.\u001b[39mscores[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m.\u001b[39mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m: ROC score \u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m.\u001b[39mscores[\u001b[39m'\u001b[39m\u001b[39mrocauc\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m task\u001b[39m.\u001b[39mdf \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/matbench/task.py:640\u001b[0m, in \u001b[0;36mMatbenchTask.scores\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m metric_keys \u001b[39m=\u001b[39m (\n\u001b[1;32m    637\u001b[0m     REG_METRICS \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mtask_type \u001b[39m==\u001b[39m REG_KEY \u001b[39melse\u001b[39;00m CLF_METRICS\n\u001b[1;32m    638\u001b[0m )\n\u001b[1;32m    639\u001b[0m scores \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 640\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_all_folds_recorded(\u001b[39m\"\u001b[39;49m\u001b[39mCannot score unless all folds are recorded!\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    641\u001b[0m \u001b[39mfor\u001b[39;00m mk \u001b[39min\u001b[39;00m metric_keys:\n\u001b[1;32m    642\u001b[0m     metric \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/matbench/task.py:174\u001b[0m, in \u001b[0;36mMatbenchTask._check_all_folds_recorded\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m\"\"\"Private method to check if all folds have been recorded.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[39mThrows error if all folds have not been recorded.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m    None\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_folds_recorded:\n\u001b[0;32m--> 174\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    175\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m; folds \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m[f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_recorded \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_recorded[f]]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnot recorded!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot score unless all folds are recorded!; folds [0, 1, 2, 3, 4] not recorded!"
     ]
    }
   ],
   "source": [
    "for task in mb.tasks:\n",
    "    task.load()\n",
    "\n",
    "    for fold_ind, fold in enumerate(task.folds):\n",
    "        print(fold)\n",
    "\n",
    "        train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "        # train and validate your model\n",
    "        my_model.train_and_validate(train_inputs, train_outputs)\n",
    "\n",
    "        # Get testing data\n",
    "        test_inputs = task.get_test_data(fold, include_target=False)\n",
    "\n",
    "        # Predict on the testing data\n",
    "        # Your output should be a pandas series, numpy array, or python iterable\n",
    "        # where the array elements are floats or bools\n",
    "        predictions = my_model.predict(test_inputs)\n",
    "\n",
    "        # Record your data!\n",
    "        task.record(fold, predictions)\n",
    "        \n",
    "    if task.metadata.task_type == CLF_KEY:\n",
    "        print(f\"{task.dataset_name}: Accuracy score {task.scores['accuracy']['mean']}\")\n",
    "        print(f\"{task.dataset_name}: ROC score {task.scores['rocauc']['mean']}\")\n",
    "\n",
    "    task.df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f3b9074e5baa1438c27e2ea813f7f53b7516c83bd70840b6d64eae6820ee5df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
