{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotx\n",
    "import pandas as pd\n",
    "from fastcore.xtras import load_pickle\n",
    "from scipy.stats import sem\n",
    "\n",
    "from gptchem.evaluator import find_learning_curve_intersection, fit_learning_curve, lc\n",
    "from gptchem.settings import ONE_COL_WIDTH_INCH, TWO_COL_GOLDEN_RATIO_HEIGHT_INCH, ONE_COL_GOLDEN_RATIO_HEIGHT_INCH\n",
    "\n",
    "from gptchem.plotsettings import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out = glob(\"out/**/*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out = [load_pickle(p) for p in all_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train_size': 50,\n",
       "  'predictions': (#119) [1,1,1,1,1,1,1,0,1,0...],\n",
       "  'xgboost': {'accuracy': 0.7983193277310925,\n",
       "   'acc_macro': 0.7983193277310925,\n",
       "   'racc': 0.6399971753407245,\n",
       "   'kappa': 0.439780306002354,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.7197802197802198,\n",
       "   'f1_micro': 0.7983193277310925,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#119) [1,1,1,1,0,1,1,1,1,0...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "          1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "          1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "          0, 1, 1, 1, 0, 1, 0, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'tabpfn': {'accuracy': 0.7563025210084033,\n",
       "   'acc_macro': 0.7563025210084033,\n",
       "   'racc': 0.7563025210084033,\n",
       "   'kappa': 0.0,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.430622009569378,\n",
       "   'f1_micro': 0.7563025210084033,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#119) [1,1,1,1,0,1,1,1,1,0...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'base_model': 'ada',\n",
       "  'batch_size': None,\n",
       "  'n_epochs': 8,\n",
       "  'learning_rate_multiplier': 0.02,\n",
       "  'run_name': None,\n",
       "  'wandb_sync': False,\n",
       "  'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_150831',\n",
       "  'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_150831/train.jsonl',\n",
       "  'valid_filename': 'None',\n",
       "  'model_name': 'ada:ft-lsmoepfl-2023-01-31-15-11-48',\n",
       "  'ft_id': 'ft-MTSJIHsq6HKtm00M87sxOdhn',\n",
       "  'date': '20230131_161210',\n",
       "  'train_file_id': 'file-ieoELakPOfCI4L2Yb8Mr8S5A',\n",
       "  'valid_file_id': None,\n",
       "  'accuracy': 0.7310924369747899,\n",
       "  'acc_macro': 0.7310924369747899,\n",
       "  'racc': 0.6744580185015182,\n",
       "  'kappa': 0.1739696312364425,\n",
       "  'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "  'f1_macro': 0.5824561403508772,\n",
       "  'f1_micro': 0.7310924369747899,\n",
       "  'frac_valid': 1.0,\n",
       "  'all_y_true': (#119) [1,1,1,1,0,1,1,1,1,0...],\n",
       "  'all_y_pred': (#119) [1,1,1,1,1,1,1,0,1,0...],\n",
       "  'valid_indices': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118],\n",
       "  'might_have_rounded_floats': False},\n",
       " {'train_size': 20,\n",
       "  'predictions': (#149) [0,1,1,1,0,0,1,1,1,1...],\n",
       "  'xgboost': {'accuracy': 0.7583892617449665,\n",
       "   'acc_macro': 0.7583892617449665,\n",
       "   'racc': 0.7583892617449665,\n",
       "   'kappa': 0.0,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.4312977099236641,\n",
       "   'f1_micro': 0.7583892617449665,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#149) [1,1,1,1,1,1,1,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'tabpfn': {'accuracy': 0.738255033557047,\n",
       "   'acc_macro': 0.738255033557047,\n",
       "   'racc': 0.7133012026485294,\n",
       "   'kappa': 0.08703849175176757,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.5237275633144824,\n",
       "   'f1_micro': 0.738255033557047,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#149) [1,1,1,1,1,1,1,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'base_model': 'ada',\n",
       "  'batch_size': None,\n",
       "  'n_epochs': 8,\n",
       "  'learning_rate_multiplier': 0.02,\n",
       "  'run_name': None,\n",
       "  'wandb_sync': False,\n",
       "  'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230201_003837',\n",
       "  'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230201_003837/train.jsonl',\n",
       "  'valid_filename': 'None',\n",
       "  'model_name': 'ada:ft-lsmoepfl-2023-02-01-03-11-34',\n",
       "  'ft_id': 'ft-C1uaRxnahU9tBRADoWv7Eg9y',\n",
       "  'date': '20230201_041153',\n",
       "  'train_file_id': 'file-HgUGHNbYSahTsm2vakF4vEkD',\n",
       "  'valid_file_id': None,\n",
       "  'accuracy': 0.6040268456375839,\n",
       "  'acc_macro': 0.6040268456375839,\n",
       "  'racc': 0.6300617089320301,\n",
       "  'kappa': -0.0703762328016558,\n",
       "  'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "  'f1_macro': 0.464779299847793,\n",
       "  'f1_micro': 0.6040268456375839,\n",
       "  'frac_valid': 1.0,\n",
       "  'all_y_true': (#149) [1,1,1,1,1,1,1,1,1,1...],\n",
       "  'all_y_pred': (#149) [0,1,1,1,0,0,1,1,1,1...],\n",
       "  'valid_indices': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148],\n",
       "  'might_have_rounded_floats': False},\n",
       " {'train_size': 10,\n",
       "  'predictions': (#159) [1,1,1,1,1,1,1,1,1,1...],\n",
       "  'xgboost': {'accuracy': 0.7547169811320755,\n",
       "   'acc_macro': 0.7547169811320755,\n",
       "   'racc': 0.7547169811320755,\n",
       "   'kappa': 0.0,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.43010752688172044,\n",
       "   'f1_micro': 0.7547169811320755,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#159) [1,1,1,1,0,1,0,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148,\n",
       "    149,\n",
       "    150,\n",
       "    151,\n",
       "    152,\n",
       "    153,\n",
       "    154,\n",
       "    155,\n",
       "    156,\n",
       "    157,\n",
       "    158],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'tabpfn': {'accuracy': 0.7672955974842768,\n",
       "   'acc_macro': 0.7672955974842768,\n",
       "   'racc': 0.7419010323958705,\n",
       "   'kappa': 0.09839080459770119,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.5024947145877379,\n",
       "   'f1_micro': 0.7672955974842768,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#159) [1,1,1,1,0,1,0,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148,\n",
       "    149,\n",
       "    150,\n",
       "    151,\n",
       "    152,\n",
       "    153,\n",
       "    154,\n",
       "    155,\n",
       "    156,\n",
       "    157,\n",
       "    158],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'base_model': 'ada',\n",
       "  'batch_size': None,\n",
       "  'n_epochs': 8,\n",
       "  'learning_rate_multiplier': 0.02,\n",
       "  'run_name': None,\n",
       "  'wandb_sync': False,\n",
       "  'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230201_093054',\n",
       "  'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230201_093054/train.jsonl',\n",
       "  'valid_filename': 'None',\n",
       "  'model_name': 'ada:ft-lsmoepfl-2023-02-01-12-53-23',\n",
       "  'ft_id': 'ft-Mz05I8tCKTnckdHJuUVVquO3',\n",
       "  'date': '20230201_135431',\n",
       "  'train_file_id': 'file-uVL5nPUysTHBUqO0ThDddN9u',\n",
       "  'valid_file_id': None,\n",
       "  'accuracy': 0.7547169811320755,\n",
       "  'acc_macro': 0.7547169811320755,\n",
       "  'racc': 0.7547169811320755,\n",
       "  'kappa': 0.0,\n",
       "  'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "  'f1_macro': 0.43010752688172044,\n",
       "  'f1_micro': 0.7547169811320755,\n",
       "  'frac_valid': 1.0,\n",
       "  'all_y_true': (#159) [1,1,1,1,0,1,0,1,1,1...],\n",
       "  'all_y_pred': (#159) [1,1,1,1,1,1,1,1,1,1...],\n",
       "  'valid_indices': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158],\n",
       "  'might_have_rounded_floats': False},\n",
       " {'train_size': 10,\n",
       "  'predictions': (#159) [1,1,0,1,1,1,1,1,1,1...],\n",
       "  'xgboost': {'accuracy': 0.7547169811320755,\n",
       "   'acc_macro': 0.7547169811320755,\n",
       "   'racc': 0.7547169811320755,\n",
       "   'kappa': 0.0,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.43010752688172044,\n",
       "   'f1_micro': 0.7547169811320755,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#159) [1,1,1,1,1,1,0,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148,\n",
       "    149,\n",
       "    150,\n",
       "    151,\n",
       "    152,\n",
       "    153,\n",
       "    154,\n",
       "    155,\n",
       "    156,\n",
       "    157,\n",
       "    158],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'tabpfn': {'accuracy': 0.7735849056603774,\n",
       "   'acc_macro': 0.7735849056603774,\n",
       "   'racc': 0.7451050195799217,\n",
       "   'kappa': 0.11173184357541911,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.5062111801242236,\n",
       "   'f1_micro': 0.7735849056603774,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#159) [1,1,1,1,1,1,0,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148,\n",
       "    149,\n",
       "    150,\n",
       "    151,\n",
       "    152,\n",
       "    153,\n",
       "    154,\n",
       "    155,\n",
       "    156,\n",
       "    157,\n",
       "    158],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'base_model': 'ada',\n",
       "  'batch_size': None,\n",
       "  'n_epochs': 8,\n",
       "  'learning_rate_multiplier': 0.02,\n",
       "  'run_name': None,\n",
       "  'wandb_sync': False,\n",
       "  'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_222923',\n",
       "  'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_222923/train.jsonl',\n",
       "  'valid_filename': 'None',\n",
       "  'model_name': 'ada:ft-lsmoepfl-2023-01-31-23-36-07',\n",
       "  'ft_id': 'ft-mXMkYScpirIOAIvZxKHBsvlU',\n",
       "  'date': '20230201_003609',\n",
       "  'train_file_id': 'file-MQkmjZoXsPcCqMox73Pfs057',\n",
       "  'valid_file_id': None,\n",
       "  'accuracy': 0.5849056603773585,\n",
       "  'acc_macro': 0.5849056603773585,\n",
       "  'racc': 0.6489854040583837,\n",
       "  'kappa': -0.18255578093306263,\n",
       "  'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "  'f1_macro': 0.40752032520325204,\n",
       "  'f1_micro': 0.5849056603773585,\n",
       "  'frac_valid': 1.0,\n",
       "  'all_y_true': (#159) [1,1,1,1,1,1,0,1,1,1...],\n",
       "  'all_y_pred': (#159) [1,1,0,1,1,1,1,1,1,1...],\n",
       "  'valid_indices': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158],\n",
       "  'might_have_rounded_floats': False},\n",
       " {'train_size': 150,\n",
       "  'predictions': (#19) [1,1,0,1,1,1,1,1,1,1...],\n",
       "  'xgboost': {'accuracy': 0.7894736842105263,\n",
       "   'acc_macro': 0.7894736842105263,\n",
       "   'racc': 0.6620498614958449,\n",
       "   'kappa': 0.3770491803278689,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.6833333333333333,\n",
       "   'f1_micro': 0.7894736842105263,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#19) [0,1,1,1,1,1,1,0,1,0...],\n",
       "   'all_y_pred': array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'tabpfn': {'accuracy': 0.7894736842105263,\n",
       "   'acc_macro': 0.7894736842105263,\n",
       "   'racc': 0.7119113573407202,\n",
       "   'kappa': 0.26923076923076933,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.6041666666666666,\n",
       "   'f1_micro': 0.7894736842105263,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#19) [0,1,1,1,1,1,1,0,1,0...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'base_model': 'ada',\n",
       "  'batch_size': None,\n",
       "  'n_epochs': 8,\n",
       "  'learning_rate_multiplier': 0.02,\n",
       "  'run_name': None,\n",
       "  'wandb_sync': False,\n",
       "  'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_194732',\n",
       "  'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_194732/train.jsonl',\n",
       "  'valid_filename': 'None',\n",
       "  'model_name': 'ada:ft-lsmoepfl-2023-01-31-21-24-50',\n",
       "  'ft_id': 'ft-irpP4zzvMnPvw9UN3GTzxKFt',\n",
       "  'date': '20230131_222502',\n",
       "  'train_file_id': 'file-qVGXDyxgspJGCXnXbHF16HZT',\n",
       "  'valid_file_id': None,\n",
       "  'accuracy': 0.7894736842105263,\n",
       "  'acc_macro': 0.7894736842105263,\n",
       "  'racc': 0.6620498614958449,\n",
       "  'kappa': 0.3770491803278689,\n",
       "  'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "  'f1_macro': 0.6833333333333333,\n",
       "  'f1_micro': 0.7894736842105263,\n",
       "  'frac_valid': 1.0,\n",
       "  'all_y_true': (#19) [0,1,1,1,1,1,1,0,1,0...],\n",
       "  'all_y_pred': (#19) [1,1,0,1,1,1,1,1,1,1...],\n",
       "  'valid_indices': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18],\n",
       "  'might_have_rounded_floats': False},\n",
       " {'train_size': 50,\n",
       "  'predictions': (#119) [1,1,1,1,1,1,0,1,1,0...],\n",
       "  'xgboost': {'accuracy': 0.8067226890756303,\n",
       "   'acc_macro': 0.8067226890756303,\n",
       "   'racc': 0.6787656238966174,\n",
       "   'kappa': 0.3983293031435482,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.6951097248524005,\n",
       "   'f1_micro': 0.8067226890756303,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#119) [1,1,1,1,1,1,1,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "          1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "          1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'tabpfn': {'accuracy': 0.7983193277310925,\n",
       "   'acc_macro': 0.7983193277310925,\n",
       "   'racc': 0.6572275969211214,\n",
       "   'kappa': 0.41161928306551304,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.7047146401985112,\n",
       "   'f1_micro': 0.7983193277310925,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#119) [1,1,1,1,1,1,1,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "          1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "          1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'base_model': 'ada',\n",
       "  'batch_size': None,\n",
       "  'n_epochs': 8,\n",
       "  'learning_rate_multiplier': 0.02,\n",
       "  'run_name': None,\n",
       "  'wandb_sync': False,\n",
       "  'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230201_041624',\n",
       "  'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230201_041624/train.jsonl',\n",
       "  'valid_filename': 'None',\n",
       "  'model_name': 'ada:ft-lsmoepfl-2023-02-01-04-58-51',\n",
       "  'ft_id': 'ft-vnJEtCNPlOKN6RRVtMiMQiog',\n",
       "  'date': '20230201_055906',\n",
       "  'train_file_id': 'file-qoS6Bt697muJeGFGjo2wChEy',\n",
       "  'valid_file_id': None,\n",
       "  'accuracy': 0.7563025210084033,\n",
       "  'acc_macro': 0.7563025210084033,\n",
       "  'racc': 0.6787656238966174,\n",
       "  'kappa': 0.2413717300505606,\n",
       "  'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "  'f1_macro': 0.6155731313356355,\n",
       "  'f1_micro': 0.7563025210084033,\n",
       "  'frac_valid': 1.0,\n",
       "  'all_y_true': (#119) [1,1,1,1,1,1,1,1,1,1...],\n",
       "  'all_y_pred': (#119) [1,1,1,1,1,1,0,1,1,0...],\n",
       "  'valid_indices': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118],\n",
       "  'might_have_rounded_floats': False},\n",
       " {'train_size': 100,\n",
       "  'predictions': (#69) [0,1,1,0,1,0,1,1,1,1...],\n",
       "  'xgboost': {'accuracy': 0.855072463768116,\n",
       "   'acc_macro': 0.855072463768116,\n",
       "   'racc': 0.6139466498634741,\n",
       "   'kappa': 0.6245919477693145,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.8120915032679739,\n",
       "   'f1_micro': 0.855072463768116,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#69) [1,1,1,1,1,0,1,1,1,1...],\n",
       "   'all_y_pred': array([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "          0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'tabpfn': {'accuracy': 0.8260869565217391,\n",
       "   'acc_macro': 0.8260869565217391,\n",
       "   'racc': 0.6727578239865575,\n",
       "   'kappa': 0.46854942233632857,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.7311688311688311,\n",
       "   'f1_micro': 0.8260869565217391,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#69) [1,1,1,1,1,0,1,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "          0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'base_model': 'ada',\n",
       "  'batch_size': None,\n",
       "  'n_epochs': 8,\n",
       "  'learning_rate_multiplier': 0.02,\n",
       "  'run_name': None,\n",
       "  'wandb_sync': False,\n",
       "  'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_163638',\n",
       "  'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_163638/train.jsonl',\n",
       "  'valid_filename': 'None',\n",
       "  'model_name': 'ada:ft-lsmoepfl-2023-01-31-18-39-59',\n",
       "  'ft_id': 'ft-n1m57ccqoZVxb0ri4bAln4zK',\n",
       "  'date': '20230131_194128',\n",
       "  'train_file_id': 'file-rh7esiZEVMBnVVNBi9kDQoj0',\n",
       "  'valid_file_id': None,\n",
       "  'accuracy': 0.782608695652174,\n",
       "  'acc_macro': 0.782608695652174,\n",
       "  'racc': 0.6360008401596303,\n",
       "  'kappa': 0.40276976341604165,\n",
       "  'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "  'f1_macro': 0.7012987012987013,\n",
       "  'f1_micro': 0.782608695652174,\n",
       "  'frac_valid': 1.0,\n",
       "  'all_y_true': (#69) [1,1,1,1,1,0,1,1,1,1...],\n",
       "  'all_y_pred': (#69) [0,1,1,0,1,0,1,1,1,1...],\n",
       "  'valid_indices': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68],\n",
       "  'might_have_rounded_floats': False},\n",
       " {'train_size': 10,\n",
       "  'predictions': (#159) [1,1,1,1,1,1,1,1,1,1...],\n",
       "  'xgboost': {'accuracy': 0.7547169811320755,\n",
       "   'acc_macro': 0.7547169811320755,\n",
       "   'racc': 0.7547169811320755,\n",
       "   'kappa': 0.0,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.43010752688172044,\n",
       "   'f1_micro': 0.7547169811320755,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#159) [1,1,1,1,0,1,0,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148,\n",
       "    149,\n",
       "    150,\n",
       "    151,\n",
       "    152,\n",
       "    153,\n",
       "    154,\n",
       "    155,\n",
       "    156,\n",
       "    157,\n",
       "    158],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'tabpfn': {'accuracy': 0.7672955974842768,\n",
       "   'acc_macro': 0.7672955974842768,\n",
       "   'racc': 0.7419010323958705,\n",
       "   'kappa': 0.09839080459770119,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.5024947145877379,\n",
       "   'f1_micro': 0.7672955974842768,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#159) [1,1,1,1,0,1,0,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148,\n",
       "    149,\n",
       "    150,\n",
       "    151,\n",
       "    152,\n",
       "    153,\n",
       "    154,\n",
       "    155,\n",
       "    156,\n",
       "    157,\n",
       "    158],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'base_model': 'ada',\n",
       "  'batch_size': None,\n",
       "  'n_epochs': 8,\n",
       "  'learning_rate_multiplier': 0.02,\n",
       "  'run_name': None,\n",
       "  'wandb_sync': False,\n",
       "  'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_111441',\n",
       "  'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_111441/train.jsonl',\n",
       "  'valid_filename': 'None',\n",
       "  'model_name': 'ada:ft-lsmoepfl-2023-01-31-10-52-40',\n",
       "  'ft_id': 'ft-GsnbqoPrnkdyjKx8ejMWaSFh',\n",
       "  'date': '20230131_115252',\n",
       "  'train_file_id': 'file-kaZYOo5rChf4xoM3vc2qp6yS',\n",
       "  'valid_file_id': None,\n",
       "  'accuracy': 0.7547169811320755,\n",
       "  'acc_macro': 0.7547169811320755,\n",
       "  'racc': 0.7547169811320755,\n",
       "  'kappa': 0.0,\n",
       "  'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "  'f1_macro': 0.43010752688172044,\n",
       "  'f1_micro': 0.7547169811320755,\n",
       "  'frac_valid': 1.0,\n",
       "  'all_y_true': (#159) [1,1,1,1,0,1,0,1,1,1...],\n",
       "  'all_y_pred': (#159) [1,1,1,1,1,1,1,1,1,1...],\n",
       "  'valid_indices': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158],\n",
       "  'might_have_rounded_floats': False},\n",
       " {'train_size': 20,\n",
       "  'predictions': (#149) [0,1,1,1,1,1,1,1,1,1...],\n",
       "  'xgboost': {'accuracy': 0.7583892617449665,\n",
       "   'acc_macro': 0.7583892617449665,\n",
       "   'racc': 0.7583892617449665,\n",
       "   'kappa': 0.0,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.4312977099236641,\n",
       "   'f1_micro': 0.7583892617449665,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#149) [1,1,1,1,1,1,1,1,1,1...],\n",
       "   'all_y_pred': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'tabpfn': {'accuracy': 0.7248322147651006,\n",
       "   'acc_macro': 0.7248322147651006,\n",
       "   'racc': 0.6647448313139048,\n",
       "   'kappa': 0.1792288055891439,\n",
       "   'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "   'f1_macro': 0.5873691320499831,\n",
       "   'f1_micro': 0.7248322147651006,\n",
       "   'frac_valid': 1.0,\n",
       "   'all_y_true': (#149) [1,1,1,1,1,1,1,1,1,1...],\n",
       "   'all_y_pred': array([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "          0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "          1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1]),\n",
       "   'valid_indices': [0,\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    11,\n",
       "    12,\n",
       "    13,\n",
       "    14,\n",
       "    15,\n",
       "    16,\n",
       "    17,\n",
       "    18,\n",
       "    19,\n",
       "    20,\n",
       "    21,\n",
       "    22,\n",
       "    23,\n",
       "    24,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    30,\n",
       "    31,\n",
       "    32,\n",
       "    33,\n",
       "    34,\n",
       "    35,\n",
       "    36,\n",
       "    37,\n",
       "    38,\n",
       "    39,\n",
       "    40,\n",
       "    41,\n",
       "    42,\n",
       "    43,\n",
       "    44,\n",
       "    45,\n",
       "    46,\n",
       "    47,\n",
       "    48,\n",
       "    49,\n",
       "    50,\n",
       "    51,\n",
       "    52,\n",
       "    53,\n",
       "    54,\n",
       "    55,\n",
       "    56,\n",
       "    57,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    64,\n",
       "    65,\n",
       "    66,\n",
       "    67,\n",
       "    68,\n",
       "    69,\n",
       "    70,\n",
       "    71,\n",
       "    72,\n",
       "    73,\n",
       "    74,\n",
       "    75,\n",
       "    76,\n",
       "    77,\n",
       "    78,\n",
       "    79,\n",
       "    80,\n",
       "    81,\n",
       "    82,\n",
       "    83,\n",
       "    84,\n",
       "    85,\n",
       "    86,\n",
       "    87,\n",
       "    88,\n",
       "    89,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    94,\n",
       "    95,\n",
       "    96,\n",
       "    97,\n",
       "    98,\n",
       "    99,\n",
       "    100,\n",
       "    101,\n",
       "    102,\n",
       "    103,\n",
       "    104,\n",
       "    105,\n",
       "    106,\n",
       "    107,\n",
       "    108,\n",
       "    109,\n",
       "    110,\n",
       "    111,\n",
       "    112,\n",
       "    113,\n",
       "    114,\n",
       "    115,\n",
       "    116,\n",
       "    117,\n",
       "    118,\n",
       "    119,\n",
       "    120,\n",
       "    121,\n",
       "    122,\n",
       "    123,\n",
       "    124,\n",
       "    125,\n",
       "    126,\n",
       "    127,\n",
       "    128,\n",
       "    129,\n",
       "    130,\n",
       "    131,\n",
       "    132,\n",
       "    133,\n",
       "    134,\n",
       "    135,\n",
       "    136,\n",
       "    137,\n",
       "    138,\n",
       "    139,\n",
       "    140,\n",
       "    141,\n",
       "    142,\n",
       "    143,\n",
       "    144,\n",
       "    145,\n",
       "    146,\n",
       "    147,\n",
       "    148],\n",
       "   'might_have_rounded_floats': False},\n",
       "  'base_model': 'ada',\n",
       "  'batch_size': None,\n",
       "  'n_epochs': 8,\n",
       "  'learning_rate_multiplier': 0.02,\n",
       "  'run_name': None,\n",
       "  'wandb_sync': False,\n",
       "  'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_115701',\n",
       "  'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/03_classification/mof_water_stability/out/20230131_115701/train.jsonl',\n",
       "  'valid_filename': 'None',\n",
       "  'model_name': 'ada:ft-lsmoepfl-2023-01-31-13-50-10',\n",
       "  'ft_id': 'ft-qcyBD7Ce0T0bEyIPTRJk5pEy',\n",
       "  'date': '20230131_150323',\n",
       "  'train_file_id': 'file-VcyhDkl1Ou59ESo3nWhXmELq',\n",
       "  'valid_file_id': None,\n",
       "  'accuracy': 0.7651006711409396,\n",
       "  'acc_macro': 0.7651006711409396,\n",
       "  'racc': 0.747984325030404,\n",
       "  'kappa': 0.0679177837354782,\n",
       "  'confusion_matrix': pycm.ConfusionMatrix(classes: [0, 1]),\n",
       "  'f1_macro': 0.4837144837144837,\n",
       "  'f1_micro': 0.7651006711409396,\n",
       "  'frac_valid': 1.0,\n",
       "  'all_y_true': (#149) [1,1,1,1,1,1,1,1,1,1...],\n",
       "  'all_y_pred': (#149) [0,1,1,1,1,1,1,1,1,1...],\n",
       "  'valid_indices': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148],\n",
       "  'might_have_rounded_floats': False}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_res = []\n",
    "xgboost_results = []\n",
    "tabpfn_results = []\n",
    "dummy_results = []\n",
    "\n",
    "for out in all_out:\n",
    "    res = {\n",
    "        \"train_size\": out[\"train_size\"],\n",
    "        \"frac_valid\": out[\"frac_valid\"],\n",
    "        \"accuracy\": out[\"accuracy\"],\n",
    "        \"f1_macro\": out[\"f1_macro\"],\n",
    "        \"f1_micro\": out[\"f1_micro\"],\n",
    "        \"kappa\": out[\"kappa\"],\n",
    "    }\n",
    "\n",
    "    xgb_res = {\n",
    "        \"train_size\": out[\"train_size\"],\n",
    "        \"accuracy\": out[\"xgboost\"][\"accuracy\"],\n",
    "        \"f1_macro\": out[\"xgboost\"][\"f1_macro\"],\n",
    "        \"f1_micro\": out[\"xgboost\"][\"f1_micro\"],\n",
    "        \"kappa\": out[\"xgboost\"][\"kappa\"],\n",
    "    }\n",
    "\n",
    "    tabpfn_res = {\n",
    "        \"train_size\": out[\"train_size\"],\n",
    "        \"accuracy\": out[\"tabpfn\"][\"accuracy\"],\n",
    "        \"f1_macro\": out[\"tabpfn\"][\"f1_macro\"],\n",
    "        \"f1_micro\": out[\"tabpfn\"][\"f1_micro\"],\n",
    "        \"kappa\": out[\"tabpfn\"][\"kappa\"],\n",
    "    }\n",
    "\n",
    "\n",
    "    extracted_res.append(res)\n",
    "    xgboost_results.append(xgb_res)\n",
    "    tabpfn_results.append(tabpfn_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(extracted_res)\n",
    "xgboost_res = pd.DataFrame(xgboost_results)\n",
    "tabpfn_res = pd.DataFrame(tabpfn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_res = res.groupby([\"train_size\"]).agg([\"mean\", \"std\", \"count\", sem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_xgboost_res = xgboost_res.groupby([\"train_size\"]).agg([\"mean\", \"std\", \"count\", sem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_tabpfn_res = tabpfn_res.groupby([\"train_size\"]).agg([\"mean\", \"std\", \"count\", sem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">frac_valid</th>\n",
       "      <th colspan=\"4\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"4\" halign=\"left\">f1_macro</th>\n",
       "      <th colspan=\"4\" halign=\"left\">f1_micro</th>\n",
       "      <th colspan=\"4\" halign=\"left\">kappa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.098041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.422578</td>\n",
       "      <td>0.013041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.098041</td>\n",
       "      <td>3</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>-0.060852</td>\n",
       "      <td>0.105399</td>\n",
       "      <td>3</td>\n",
       "      <td>0.060852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.684564</td>\n",
       "      <td>0.113896</td>\n",
       "      <td>2</td>\n",
       "      <td>0.080537</td>\n",
       "      <td>0.474247</td>\n",
       "      <td>0.013389</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009468</td>\n",
       "      <td>0.684564</td>\n",
       "      <td>0.113896</td>\n",
       "      <td>2</td>\n",
       "      <td>0.080537</td>\n",
       "      <td>-0.001229</td>\n",
       "      <td>0.097789</td>\n",
       "      <td>2</td>\n",
       "      <td>0.069147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.743697</td>\n",
       "      <td>0.017826</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012605</td>\n",
       "      <td>0.599015</td>\n",
       "      <td>0.023417</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016558</td>\n",
       "      <td>0.743697</td>\n",
       "      <td>0.017826</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012605</td>\n",
       "      <td>0.207671</td>\n",
       "      <td>0.047660</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.402770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           frac_valid                  accuracy                            \\\n",
       "                 mean  std count  sem      mean       std count       sem   \n",
       "train_size                                                                  \n",
       "10                1.0  0.0     3  0.0  0.698113  0.098041     3  0.056604   \n",
       "20                1.0  0.0     2  0.0  0.684564  0.113896     2  0.080537   \n",
       "50                1.0  0.0     2  0.0  0.743697  0.017826     2  0.012605   \n",
       "100               1.0  NaN     1  NaN  0.782609       NaN     1       NaN   \n",
       "150               1.0  NaN     1  NaN  0.789474       NaN     1       NaN   \n",
       "\n",
       "            f1_macro                            f1_micro                  \\\n",
       "                mean       std count       sem      mean       std count   \n",
       "train_size                                                                 \n",
       "10          0.422578  0.013041     3  0.007529  0.698113  0.098041     3   \n",
       "20          0.474247  0.013389     2  0.009468  0.684564  0.113896     2   \n",
       "50          0.599015  0.023417     2  0.016558  0.743697  0.017826     2   \n",
       "100         0.701299       NaN     1       NaN  0.782609       NaN     1   \n",
       "150         0.683333       NaN     1       NaN  0.789474       NaN     1   \n",
       "\n",
       "                         kappa                            \n",
       "                 sem      mean       std count       sem  \n",
       "train_size                                                \n",
       "10          0.056604 -0.060852  0.105399     3  0.060852  \n",
       "20          0.080537 -0.001229  0.097789     2  0.069147  \n",
       "50          0.012605  0.207671  0.047660     2  0.033701  \n",
       "100              NaN  0.402770       NaN     1       NaN  \n",
       "150              NaN  0.377049       NaN     1       NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_res.loc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"4\" halign=\"left\">f1_macro</th>\n",
       "      <th colspan=\"4\" halign=\"left\">f1_micro</th>\n",
       "      <th colspan=\"4\" halign=\"left\">kappa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.769392</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.503734</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.769392</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.102838</td>\n",
       "      <td>0.007702</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.731544</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.555548</td>\n",
       "      <td>0.045001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031821</td>\n",
       "      <td>0.731544</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.133134</td>\n",
       "      <td>0.065188</td>\n",
       "      <td>2</td>\n",
       "      <td>0.046095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.777311</td>\n",
       "      <td>0.029710</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021008</td>\n",
       "      <td>0.567668</td>\n",
       "      <td>0.193813</td>\n",
       "      <td>2</td>\n",
       "      <td>0.137046</td>\n",
       "      <td>0.777311</td>\n",
       "      <td>0.029710</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021008</td>\n",
       "      <td>0.205810</td>\n",
       "      <td>0.291059</td>\n",
       "      <td>2</td>\n",
       "      <td>0.205810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.826087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.731169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.468549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            accuracy                            f1_macro                  \\\n",
       "                mean       std count       sem      mean       std count   \n",
       "train_size                                                                 \n",
       "10          0.769392  0.003631     3  0.002096  0.503734  0.002146     3   \n",
       "20          0.731544  0.009491     2  0.006711  0.555548  0.045001     2   \n",
       "50          0.777311  0.029710     2  0.021008  0.567668  0.193813     2   \n",
       "100         0.826087       NaN     1       NaN  0.731169       NaN     1   \n",
       "150         0.789474       NaN     1       NaN  0.604167       NaN     1   \n",
       "\n",
       "                      f1_micro                               kappa            \\\n",
       "                 sem      mean       std count       sem      mean       std   \n",
       "train_size                                                                     \n",
       "10          0.001239  0.769392  0.003631     3  0.002096  0.102838  0.007702   \n",
       "20          0.031821  0.731544  0.009491     2  0.006711  0.133134  0.065188   \n",
       "50          0.137046  0.777311  0.029710     2  0.021008  0.205810  0.291059   \n",
       "100              NaN  0.826087       NaN     1       NaN  0.468549       NaN   \n",
       "150              NaN  0.789474       NaN     1       NaN  0.269231       NaN   \n",
       "\n",
       "                            \n",
       "           count       sem  \n",
       "train_size                  \n",
       "10             3  0.004447  \n",
       "20             2  0.046095  \n",
       "50             2  0.205810  \n",
       "100            1       NaN  \n",
       "150            1       NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_tabpfn_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">accuracy</th>\n",
       "      <th colspan=\"4\" halign=\"left\">f1_macro</th>\n",
       "      <th colspan=\"4\" halign=\"left\">f1_micro</th>\n",
       "      <th colspan=\"4\" halign=\"left\">kappa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>7.850462e-17</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>7.850462e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.758389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.431298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.802521</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>2</td>\n",
       "      <td>4.201681e-03</td>\n",
       "      <td>0.707445</td>\n",
       "      <td>0.017445</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012335</td>\n",
       "      <td>0.802521</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>2</td>\n",
       "      <td>4.201681e-03</td>\n",
       "      <td>0.419055</td>\n",
       "      <td>0.02931</td>\n",
       "      <td>2</td>\n",
       "      <td>0.020726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.855072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.812092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.624592</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            accuracy                                f1_macro                  \\\n",
       "                mean       std count           sem      mean       std count   \n",
       "train_size                                                                     \n",
       "10          0.754717  0.000000     3  7.850462e-17  0.430108  0.000000     3   \n",
       "20          0.758389  0.000000     2  0.000000e+00  0.431298  0.000000     2   \n",
       "50          0.802521  0.005942     2  4.201681e-03  0.707445  0.017445     2   \n",
       "100         0.855072       NaN     1           NaN  0.812092       NaN     1   \n",
       "150         0.789474       NaN     1           NaN  0.683333       NaN     1   \n",
       "\n",
       "                      f1_micro                                   kappa  \\\n",
       "                 sem      mean       std count           sem      mean   \n",
       "train_size                                                               \n",
       "10          0.000000  0.754717  0.000000     3  7.850462e-17  0.000000   \n",
       "20          0.000000  0.758389  0.000000     2  0.000000e+00  0.000000   \n",
       "50          0.012335  0.802521  0.005942     2  4.201681e-03  0.419055   \n",
       "100              NaN  0.855072       NaN     1           NaN  0.624592   \n",
       "150              NaN  0.789474       NaN     1           NaN  0.377049   \n",
       "\n",
       "                                     \n",
       "                std count       sem  \n",
       "train_size                           \n",
       "10          0.00000     3  0.000000  \n",
       "20          0.00000     2  0.000000  \n",
       "50          0.02931     2  0.020726  \n",
       "100             NaN     1       NaN  \n",
       "150             NaN     1       NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_xgboost_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 325x444.984 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with plt.style.context(matplotx.styles.dufte):\n",
    "fig, ax = plt.subplots(\n",
    "    4, 1, figsize=(ONE_COL_WIDTH_INCH, TWO_COL_GOLDEN_RATIO_HEIGHT_INCH), sharex=\"all\"\n",
    ")\n",
    "\n",
    "# plot the binary case, i.e. num_classes = 2\n",
    "\n",
    "for i, metric in enumerate([\"accuracy\", \"f1_macro\", \"f1_micro\", \"kappa\"]):\n",
    "    ax[i].plot(\n",
    "        grouped_res.loc[ :].index,\n",
    "        grouped_res.loc[:][metric][\"mean\"],\n",
    "        marker=\"o\",\n",
    "        label=\"GPT\",\n",
    "    )\n",
    "    ax[i].fill_between(\n",
    "        grouped_res.loc[ :].index,\n",
    "        grouped_res.loc[:][metric][\"mean\"] - grouped_res.loc[ :][metric][\"sem\"],\n",
    "        grouped_res.loc[ :][metric][\"mean\"] + grouped_res.loc[ :][metric][\"sem\"],\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    range_frame(\n",
    "        ax[i],\n",
    "        grouped_res.loc[ :].index,\n",
    "        np.stack([\n",
    "            grouped_res.loc[:][metric][\"mean\"],\n",
    "              grouped_xgboost_res.loc[ :][metric][\"mean\"],\n",
    "              grouped_tabpfn_res.loc[ :][metric][\"mean\"], \n",
    "        ]).flatten()\n",
    "    )\n",
    "\n",
    "    ax[i].plot(\n",
    "        grouped_xgboost_res.loc[ :].index,\n",
    "        grouped_xgboost_res.loc[ :][metric][\"mean\"],\n",
    "        marker=\"o\",\n",
    "        label=\"XGBoost\",\n",
    "    )\n",
    "    ax[i].fill_between(\n",
    "        grouped_xgboost_res.loc[ :].index,\n",
    "        grouped_xgboost_res.loc[:][metric][\"mean\"]\n",
    "        - grouped_xgboost_res.loc[ :][metric][\"sem\"],\n",
    "        grouped_xgboost_res.loc[:][metric][\"mean\"]\n",
    "        + grouped_xgboost_res.loc[:][metric][\"sem\"],\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    ax[i].plot(\n",
    "        grouped_tabpfn_res.loc[ :].index,\n",
    "        grouped_tabpfn_res.loc[ :][metric][\"mean\"],\n",
    "        marker=\"o\",\n",
    "        label=\"TabPFN\",\n",
    "    )\n",
    "    ax[i].fill_between(\n",
    "        grouped_tabpfn_res.loc[ :].index,\n",
    "        grouped_tabpfn_res.loc[:][metric][\"mean\"]\n",
    "        - grouped_tabpfn_res.loc[ :][metric][\"sem\"],\n",
    "        grouped_tabpfn_res.loc[ :][metric][\"mean\"]\n",
    "        + grouped_tabpfn_res.loc[:][metric][\"sem\"],\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "ylabel_top(\"accuracy\", ax=ax[0], x_pad=0.05, y_pad=0.06)\n",
    "ylabel_top(r\"F$_{1}$ macro\", ax=ax[1], x_pad=0.05, y_pad=0.06)\n",
    "ylabel_top(r\"F$_{1}$ micro\", ax=ax[2], x_pad=0.05, y_pad=0.06)\n",
    "ylabel_top(r\"$\\kappa$\", ax=ax[3], x_pad=0.05, y_pad=0.06)\n",
    "ax[-1].set_xlabel(\"number of training points\", labelpad=4)\n",
    "matplotx.line_labels(ax=ax[0])\n",
    "plt.subplots_adjust(hspace=.6, top=1, bottom=.2)\n",
    "\n",
    "fig.savefig(\"mof_water_stability.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m find_learning_curve_intersection(\n\u001b[0;32m----> 2\u001b[0m     grouped_res\u001b[39m.\u001b[39;49mloc[\u001b[39m2\u001b[39;49m, :][\u001b[39m\"\u001b[39m\u001b[39mf1_macro\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m     fit_learning_curve(\n\u001b[1;32m      4\u001b[0m         grouped_xgboost_res\u001b[39m.\u001b[39mloc[\u001b[39m2\u001b[39m, :]\u001b[39m.\u001b[39mindex,\n\u001b[1;32m      5\u001b[0m         grouped_xgboost_res\u001b[39m.\u001b[39mloc[\u001b[39m2\u001b[39m, :][\u001b[39m\"\u001b[39m\u001b[39mf1_macro\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     )[\u001b[39m0\u001b[39m],\n\u001b[1;32m      7\u001b[0m ) \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1247\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1246\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1247\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_lowerdim(tup)\n\u001b[1;32m   1249\u001b[0m \u001b[39m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:941\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[39m# we may have a nested tuples indexer here\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_nested_tuple_indexer(tup):\n\u001b[0;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_nested_tuple(tup)\n\u001b[1;32m    943\u001b[0m \u001b[39m# we maybe be using a tuple to represent multiple dimensions here\u001b[39;00m\n\u001b[1;32m    944\u001b[0m ax0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1047\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_nested_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     axis \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1045\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m-> 1047\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(obj, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1048\u001b[0m axis \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# if we have a scalar, we are done\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1312\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1312\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1260\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: \u001b[39mint\u001b[39m):\n\u001b[1;32m   1259\u001b[0m     \u001b[39m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1260\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mxs(label, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/generic.py:4056\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4054\u001b[0m             new_index \u001b[39m=\u001b[39m index[loc]\n\u001b[1;32m   4055\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4056\u001b[0m     loc \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4058\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m   4059\u001b[0m         \u001b[39mif\u001b[39;00m loc\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mbool_:\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "find_learning_curve_intersection(\n",
    "    grouped_res.loc[2, :][\"f1_macro\"][\"mean\"].values[0],\n",
    "    fit_learning_curve(\n",
    "        grouped_xgboost_res.loc[2, :].index,\n",
    "        grouped_xgboost_res.loc[2, :][\"f1_macro\"][\"mean\"],\n",
    "    )[0],\n",
    ") / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m find_learning_curve_intersection(\n\u001b[0;32m----> 2\u001b[0m     grouped_res\u001b[39m.\u001b[39;49mloc[\u001b[39m2\u001b[39;49m, :][\u001b[39m\"\u001b[39m\u001b[39mf1_macro\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m     fit_learning_curve(\n\u001b[1;32m      4\u001b[0m         grouped_tabpfn_res\u001b[39m.\u001b[39mloc[\u001b[39m2\u001b[39m, :]\u001b[39m.\u001b[39mindex,\n\u001b[1;32m      5\u001b[0m         grouped_tabpfn_res\u001b[39m.\u001b[39mloc[\u001b[39m2\u001b[39m, :][\u001b[39m\"\u001b[39m\u001b[39mf1_macro\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     )[\u001b[39m0\u001b[39m],\n\u001b[1;32m      7\u001b[0m ) \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1247\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1246\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1247\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_lowerdim(tup)\n\u001b[1;32m   1249\u001b[0m \u001b[39m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:941\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[39m# we may have a nested tuples indexer here\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_nested_tuple_indexer(tup):\n\u001b[0;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_nested_tuple(tup)\n\u001b[1;32m    943\u001b[0m \u001b[39m# we maybe be using a tuple to represent multiple dimensions here\u001b[39;00m\n\u001b[1;32m    944\u001b[0m ax0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1047\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_nested_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     axis \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1045\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m-> 1047\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(obj, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1048\u001b[0m axis \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# if we have a scalar, we are done\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1312\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1312\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexing.py:1260\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: \u001b[39mint\u001b[39m):\n\u001b[1;32m   1259\u001b[0m     \u001b[39m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1260\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mxs(label, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/generic.py:4056\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4054\u001b[0m             new_index \u001b[39m=\u001b[39m index[loc]\n\u001b[1;32m   4055\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4056\u001b[0m     loc \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4058\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m   4059\u001b[0m         \u001b[39mif\u001b[39;00m loc\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mbool_:\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "find_learning_curve_intersection(\n",
    "    grouped_res.loc[2, :][\"f1_macro\"][\"mean\"].values[0],\n",
    "    fit_learning_curve(\n",
    "        grouped_tabpfn_res.loc[2, :].index,\n",
    "        grouped_tabpfn_res.loc[2, :][\"f1_macro\"][\"mean\"],\n",
    "    )[0],\n",
    ") / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f3b9074e5baa1438c27e2ea813f7f53b7516c83bd70840b6d64eae6820ee5df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
